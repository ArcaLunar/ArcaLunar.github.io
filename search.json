[{"title":"Linux 下用 HKU 账号使用 HK eduroam WiFi","path":"/connect-to-eduroam-hku/","content":"eduroam 我是 Linux Mint 系统，所以 Ubuntu/Mint 应该比较通用。 Option Value Security WPA &amp; WPA2 Enterprise Authentication Protected EAP (PEAP) Anonymous Identity 留空 Domain hku.hk CA Certificate 选 (None)，勾选 No CA Certificate is required PEAP version Automatic Inner Authentication MSCHAPv2 Username &lt;UID&gt;@hku.hk，&lt;UID&gt; 是邮箱 @ 前的部分，这里后缀必须是 @hku.hk Password Portal 的登录密码"},{"title":"Docker Introduction","path":"/docker-intro/","content":"Dockerfile Docker Image 编写完 Dockerfile 后，我们可以用命令创建一个 docker 镜像 1docker build -t [镜像名称] [Dockerfile 所在目录] 启动 container 还需要一个 container 才能运行起来 1docker run -p [映射到主机的哪个端口]:[容器内的哪个端口] -d [镜像名称] -p：指定端口 -d：后台运行。想要查看终端输出的话需要到 docker desktop 里查看 用 Volume 保存 container 的数据 利用 -v 参数，把本地文件夹 ~/A 挂载到 container 里的 .../B，这样在 container 里读写 .../B 其实等于读写 ~/A. 我们先创建一个数据卷 1docker volume create [数据卷位置] 然后在 docker run 的时候进行挂载 1docker run -v [数据卷在本地的位置]:[数据卷在容器里的位置] -d [镜像名称] Docker Compose 多个容器共同协作：例如数据库和前端分离。 在 docker-compose.yml 里用 services 进行定义 12345678910111213# docker-compose.ymlservices: # 定义前端和数据库 front-end: # 前端 build: . # 镜像——从文件夹构建 ports: # 端口映射 - &quot;80:5000&quot; database: # 数据库 image: &quot;mysql&quot; # 镜像——从其他地方拉取 environment: # 可以定义环境变量 OPENAI_API_KEY: &quot;...&quot; OPENAI_API_BASE: &quot;&quot; volumes: # 数据卷，等同于 -v 参数 - &quot;~/A:.../B&quot; 定义完毕后，使用 1docker compose up -d 来运行所有的 container 1docker compose down 来停止并删除所有的 container"},{"title":"本地部署 llama.cpp 大模型服务器并连接","path":"/local-deploy/","content":"llama.cpp 的安装、编译 alternative: llama-cpp-python 提供了 llama.cpp 的 Python 接口。通过 llama-cpp-python 也可以启动一个 LLM Server 启动一个 LLM server 直接在命令行里输入启动服务器 1llama-server -m [模型路径] --port 8080 模型要保证必须是 .gguf 格式，可以使用 llama.cpp 项目根目录下的 convert_hf_to_gguf.py 进行转换。 convert_hf_to_gguf&nbsp;食用方法 配置好虚拟环境后，命令行里输入 1python convert_hf_to_gguf.py [模型.bin文件所在的目录] 这个目录末尾应该是哈希码，例如 ~/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562 连接 如果用 LangChain 进行连接，必须注意要输入 http://localhost:8080 的 http://（被坑了）"},{"title":"About Me","path":"/about/index.html","content":"About Me"},{"title":"强化学习：Markov Chain 与贝尔曼方程","path":"/wiki/rl/rl-bellman-equation.html","content":"我们定义： 机器人所处于的状态为 state s∈Ss \\in \\mathbb Ss∈S。S\\mathbb SS 表示机器人所有可能的状态 机器人采取的行动为 action a∈Aa \\in\\mathbb Aa∈A。A\\mathbb AA 表示机器人所有可能采取的行动 Markov Chain 给定当前状态，未来和过去互相独立，且采取的行动只和当前状态有关 st+1∈St+1∼Distribution(at,st)s_{t+1}\\in\\mathbb S_{t+1}\\sim\\text{Distribution}( a_t,s_t) st+1​∈St+1​∼Distribution(at​,st​) 状态空间中的每一个状态都有一定概率被转移到，因此我们用概率进行建模，即 T(s,a,s′)T(s,a,s&#x27;) T(s,a,s′) Markov Decision Process 因此进一步定义： 世界（环境）模型 T(s,a,s′)T(s,a,s&#x27;)T(s,a,s′)，表示++机器人在状态 sss 时，如果采取 aaa 行动，那么有 T(s,a,s′)T(s,a,s&#x27;)T(s,a,s′) 的概率进入状态 s′s&#x27;s′。++这个模型也就是机器人与环境交互的入口 奖励函数 R(s,a,s′)R(s,a,s&#x27;)R(s,a,s′)，表示如果机器人在状态 sss 时采取 aaa 行动并进入状态 s′s&#x27;s′，就能获得 R(s,a,s′)R(s,a,s&#x27;)R(s,a,s′) 的奖励。 我们希望，机器人在世界模型和奖励函数（这两个是事先给定的）中，学习到在某个环境下该采取何种行动这一个 objective，这样一个 objective 的数学本质是 π:S↦A\\pi:\\mathbb S\\mapsto\\mathbb Aπ:S↦A，即 π(s)=a\\pi(s)=aπ(s)=a，函数输入状态，输出该采取什么行动。我们把这个 π(s)\\pi(s)π(s) 称为 Policy How to Learn a Policy Evaluation 很显然，我们需要一个 criterion 才能评判一个 Policy 到底好不好。 当我们的机器人根据 πi()\\pi_i()πi​() 运行了一段时间后，会得到一连串的 Reward 和一个 Accumulative Reward，而由于世界模型是概率模型，因此同一个 πi()\\pi_i()πi​() 可能会产生不同的 Reward Sequence 和不同的 Accumulative Reward。 我们也要考虑步数的影响（不然机器人来回踱步刷分数），因此引入 Discount，在每一步的 Reward 上乘的衰减系数 γ\\gammaγ，表明 Reward 随着步数的增长而减少。 我们在此基础上定义，即为 Policy 的 Utility 为 Reward 的期望值。 State 的 Utility 为从这个 State sss 出发的 Expected Utility，即为 V(s)V(s)V(s)，用上标 ∗\\ast∗ 表示最优策略 MDP Search Tree MDP Search Tree Value of State 定义： Q-State 为机器人选择完行动后，但还没有执行（还没有转移到 s′s&#x27;s′）的中间状态。 根据这棵树的结构可以推导出 V∗(s)=max⁡aQ∗(s,a)Q∗(s,a)=∑s′T(s,a,s′)[R(s,a,s′)+γV∗(s′)]so we get…⟹V∗(s)=max⁡a∑s′T(s,a,s′)[R(s,a,s′)+γV∗(s′)]\\begin{aligned} V^\\ast(s)&amp;=\\max_a Q^\\ast(s,a)\\\\ Q^\\ast(s,a)&amp;=\\sum_{s&#x27;} T(s,a,s&#x27;)\\Big[R(s,a,s&#x27;)+\\gamma V^\\ast(s&#x27;)\\Big]\\\\ &amp;\\textbf{so we get}\\dots\\\\ \\Longrightarrow V^\\ast(s)&amp;=\\max_a \\sum_{s&#x27;}T(s,a,s&#x27;)\\Big[R(s,a,s&#x27;)+\\gamma V^\\ast(s&#x27;)\\Big] \\end{aligned} V∗(s)Q∗(s,a)⟹V∗(s)​=amax​Q∗(s,a)=s′∑​T(s,a,s′)[R(s,a,s′)+γV∗(s′)]so we get…=amax​s′∑​T(s,a,s′)[R(s,a,s′)+γV∗(s′)]​ 求解 Policy 从 V(s) 求解 policy 数值迭代算法 从 V0(s)=0V_0(s)=0V0​(s)=0 开始 用上一次的 Vt(s)V_t(s)Vt​(s) 更小当次的 Vt+1(s)V_{t+1}(s)Vt+1​(s) Vt+1(s)←max⁡a∑s′T(s,a,s′)[R(s,a,s′)+γVt(s′)]V_{t+1}(s)\\gets \\max_a\\sum_{s&#x27;} T(s,a,s&#x27;)\\Big[R(s,a,s&#x27;)+\\gamma V_t(s&#x27;) \\Big] Vt+1​(s)←amax​s′∑​T(s,a,s′)[R(s,a,s′)+γVt​(s′)] 这里的 γ\\gammaγ 表示步数的 Discount 直到收敛 V∗(s)V^\\ast(s)V∗(s)，时间复杂度 O(S2A)O(S^2A)O(S2A) 从 V∗(s)V^\\ast(s)V∗(s) 提取 Policy π∗(s)=arg max⁡a∑s′T(s,a,s′)[R(s,a,s′)+γV∗(s′)]\\pi^\\ast(s)=\\argmax_a\\sum_{s&#x27;} T(s,a,s&#x27;)\\Big[R(s,a,s&#x27;)+\\gamma V^\\ast(s&#x27;) \\Big] π∗(s)=aargmax​s′∑​T(s,a,s′)[R(s,a,s′)+γV∗(s′)] 为每一个 state 选择一个 action 策略迭代算法 当 Policy 固定为 πi()\\pi_i()πi​() 时，此时不用考虑最优策略，等同于不需要取 max⁡a\\max_amaxa​，因此从 state sss 出发的 expected utility 就只有单纯的求和了，为 Vπi(s)=∑s′T(s,πi(s),s′)[R(s,πi(s),s′)+γVπi(s′)]V^{\\pi_i}(s)=\\sum_{s&#x27;} T(s,\\pi_i(s),s&#x27;)\\Big[R(s,\\pi_i(s),s&#x27;)+\\gamma V^{\\pi_i}(s&#x27;)\\Big] Vπi​(s)=s′∑​T(s,πi​(s),s′)[R(s,πi​(s),s′)+γVπi​(s′)] Policy Evaluation. 为选定的 Policy 计算 Utility（非 Optimal Utility） Vt+1πi(s)←∑s′T(s,πi(s),s′)[R(s,πi(s),s′)+γVtπi(s′)]V_{t+1}^{\\pi_i}(s)\\gets \\sum_{s&#x27;}T(s,\\pi_i(s), s&#x27;)\\Big[R(s,\\pi_i(s),s&#x27;)+\\gamma V^{\\pi_i}_{t}(s&#x27;) \\Big] Vt+1πi​​(s)←s′∑​T(s,πi​(s),s′)[R(s,πi​(s),s′)+γVtπi​​(s′)] Policy Improvement. 优化 Policy πt+1(s)←arg max⁡a∑s′T(s,a,s′)[R(s,a,s′)+γVπi(s′)]\\pi_{t+1}(s)\\gets \\argmax_a \\sum_{s&#x27;} T(s,a,s&#x27;)\\Big[R(s,a,s&#x27;)+\\gamma V^{\\pi_i}(s&#x27;) \\Big] πt+1​(s)←aargmax​s′∑​T(s,a,s′)[R(s,a,s′)+γVπi​(s′)] 直到 Policy 收敛 从 Q∗(s,a)Q^\\ast(s,a)Q∗(s,a) 提取 Policy π∗(s)=arg max⁡aQ∗(s,a)\\pi^\\ast(s)=\\argmax_a Q^\\ast(s,a) π∗(s)=aargmax​Q∗(s,a) 从 Q-State 求解 Policy Q∗(s,a)=∑s′T(s,a,s′)[R(s,a,s′)+γmax⁡a′Q∗(s′,a′)]Q^\\ast(s,a)=\\sum_{s&#x27;} T(s,a,s&#x27;)\\Big[R(s,a,s&#x27;)+\\gamma\\max_{a&#x27;}Q^\\ast(s&#x27;,a&#x27;) \\Big] Q∗(s,a)=s′∑​T(s,a,s′)[R(s,a,s′)+γa′max​Q∗(s′,a′)] 提取 Policy： π∗(s)=arg max⁡aQ∗(s,a)\\pi^\\ast(s)=\\argmax_a Q^\\ast(s,a) π∗(s)=aargmax​Q∗(s,a) Summary","categories":[null]},{"title":"Valuation of Stocks","path":"/wiki/fina/valuation-of-stocks.html","content":"stock 债券 Dividend Discount Model (DDM 模型) stock 会产生分红现金流， PV of Dividend Stream=∑i=1+∞Di(1+R)i\\text{PV of Dividend Stream}=\\sum_{i=1}^{+\\infin}\\frac{D_i}{(1+R)^i} PV of Dividend Stream=i=1∑+∞​(1+R)iDi​​ 也被称为 funcdamental value of stock. 在市场完全竞争的情况下，价格 P0=PV of Dividend StreamP_0=\\text{PV of Dividend Stream}P0​=PV of Dividend Stream. 这个模型被称为 Dividend Discount Model. Special Case 1: 恒常分红现金流 当 Di=DjD_i=D_jDi​=Dj​ 时，通过等比数列求和可以简便地计算 P0P_0P0​ P0=D1RP_0=\\frac{D_1}{R} P0​=RD1​​ Special Case 2: 现金流等比增长 也被称为 Gordon’s Growth Model，设分红以每年 ggg 的速度增长，即 Di=(1+g)Di−1D_i=(1+g)D_{i-1}Di​=(1+g)Di−1​，则有 P0=D1R−gP_0=\\frac{D_1}{R-g} P0​=R−gD1​​ 一些指标 Dividend Yield 计算产生的分红 Dividend Yield=D1P0\\text{Dividend Yield}=\\frac{D_1}{P_0} Dividend Yield=P0​D1​​ Capital Gains Yield 如果在 ttt 时刻卖出 stock，可以计算获得的资本 Capital Gains Yield=Pt−P0P0\\text{Capital Gains Yield}=\\frac{P_t-P_0}{P_0} Capital Gains Yield=P0​Pt​−P0​​ Common vs. Preferred Common stock 有投票权 Preferred stock 享有优先分红的权利 由于 Preferred stock 的期限是无穷的，我们可以直接按 Perpetual Bond 来看待。如果每年分红 DpD_pDp​，利率 RpR_pRp​，初始股价 P0P_0P0​，根据 DDM 模型，有 P0=DpRpP_0=\\frac{D_p}{R_p} P0​=Rp​Dp​​"},{"title":"Q Learning","path":"/wiki/rl/q-learning.html","content":"Q-Learning 我们的 Agent 每次从环境接收 transition=(s,a,r,s′)\\texttt{transition}=(s,a,r,s&#x27;)transition=(s,a,r,s′) 的反馈，以此进行学习。由于无法建模出转移概率 T(s,a,s′)T(s,a,s&#x27;)T(s,a,s′)，我们用采样的方式（蒙特卡洛）来训练 Qt+1(s,a)←(1−α)Qt(s,a)+α[R(s,a,s′)+γmax⁡a′Qt(s′,a′)]Q_{t+1}(s,a) \\gets (1-\\alpha)Q_{t}(s,a)+\\alpha\\Big[ R(s,a,s&#x27;)+\\gamma \\max_{a&#x27;} Q_t(s&#x27;,a&#x27;) \\Big] Qt+1​(s,a)←(1−α)Qt​(s,a)+α[R(s,a,s′)+γa′max​Qt​(s′,a′)] 这个式子也可以等价地写作 Qt+1(s,a)←Qt(s,a)+α[r+γmax⁡a′Qt(s′,a′)−Qt(s,a)]Q_{t+1}(s,a)\\gets Q_t(s,a)+\\alpha\\Big[ r+\\gamma \\max_{a&#x27;}Q_t(s&#x27;,a&#x27;)-Q_t(s,a) \\Big] Qt+1​(s,a)←Qt​(s,a)+α[r+γa′max​Qt​(s′,a′)−Qt​(s,a)] Approx Q-Learning 在 Approx Q-Learning 算法里，我们把 Q(s,a)Q(s,a)Q(s,a) 分解为多个关于 state sss 和行动 aaa 的 feature\\tt featurefeature 之线性组合（feature\\tt featurefeature 不一定需要和 s,as,as,a 成线性） Q(s,a)=∑iwi×fi(s,a)\\boxed {Q(s,a)=\\sum olimits_i w_i \\times f_i(s,a)} Q(s,a)=∑i​wi​×fi​(s,a)​ 令当次从环境的采样为 (s,a,s′,r)(s,a,s&#x27;,r)(s,a,s′,r)，表示从状态 sss 执行动作 aaa 转移到状态 s′s&#x27;s′ 得到奖励 rrr，定义 Sample Difference 为 Δ=r+γmax⁡a′Q(s′,a′)−Q(s,a)\\Delta=r+\\gamma\\max_{a&#x27;}Q(s&#x27;,a&#x27;)-Q(s,a) Δ=r+γa′max​Q(s′,a′)−Q(s,a) feature weights\\texttt{feature weights}feature weights 的更新则为 wi←wi+α×Δ×fi(s,a)w_i\\gets w_i+\\alpha\\times\\Delta\\times f_i(s,a) wi​←wi​+α×Δ×fi​(s,a) 推导 核心公式 GetAction()π(s)=arg max⁡aQ(s,a)UpdateWeights()wi←wi+α×[Δ]×fi(s,a)\\begin{array}{|r|cl|} \\hline \\text{GetAction()}&amp;\\pi(s)&amp;=\\argmax_{a}Q(s,a)\\\\ \\hline \\text{UpdateWeights()}&amp;w_i&amp;\\gets w_i+\\alpha\\times[\\Delta]\\times f_i(s,a)\\\\ \\hline \\end{array} GetAction()UpdateWeights()​π(s)wi​​=argmaxa​Q(s,a)←wi​+α×[Δ]×fi​(s,a)​​ 实现代码 12345678910111213141516171819class ApproxQLearningAgent(): def __init__(self): self.gamma = # reward discount rate self.alpha = # weight update factor self.epsilon = # learning rate self.weights = &#123; &quot;f1&quot;: 0.0, # correspond to w1*f1(s,a) &quot;f2&quot;: 0.0, # correspond to w2*f2(s,a) # ... &#125; def get_legal_actions(self): &#x27;&#x27;&#x27; 获取 &#x27;&#x27;&#x27; pass def"}]