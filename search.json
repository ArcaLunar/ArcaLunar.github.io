[{"title":"llama.cpp server 端的 API","path":"/llama-cpp-server-api/","content":"llama-server 参数"},{"title":"OpenAI 流式传输与 StreamLit","path":"/openai-stream-mode-w-streamlit/","content":"流式传输 大模型的 API 通常都支持流式传输。所谓流式传输，就是指将大模型生成的文字拆分成一小块一小块发送过来，比如说每隔 555 秒就发送一次生成的文字，而不是等文字全部生成完毕才一次性全部发送。 这样做的好处在于 langchain langchain 的 ChatOpenAI 已经包装的十分完善了。"},{"title":"Python 并行库 joblib","path":"/python-joblib/","content":"joblib joblib 提供两个最核心的功能：caching 和 parallel computing. 使用 joblib.Parallel 进行并行计算 joblib.Parallel 的基础用法是通过 n_jobs 指定进程数（指定 n_jobs=-1 则表示能用多少用多少），初始化类后，用 joblib.delayed(&lt;function&gt;)(args) 指定每一个进程的工作 并行读取图片，保存为 NumPy Array 123456789101112131415import matplotlib.pyplot as plt # 读取图片from joblib import Parallel, delayed # 并行计算from rich.progress import track # 可视化进度条def read_img(path): return plt.imread(path)imgs = Parallel(n_jobs=-1)( delayed(read_img)(path) for path in track(csv[&quot;im_name&quot;], description=&quot;Loading images ... &quot;, transient=True) # transient=True 指定进度条在完成后隐藏) # parallel() 结束之后，imgs 是 List[np.ndarray]imgs = np.asarray(imgs) # 转化为 np.ndarray"},{"title":"使用 Socat 创建虚拟串口并指定名称","path":"/socat-usage/","content":"Preface 起因主要是社团……没有车的时候调试个 serial port 十分费劲，甚至根本调试不了写的对不对 所以只能用 socat 开虚拟串口模拟通讯了 socat 安装 安装比较容易，可以直接通过 apt 包管理器安装 1sudo apt install socat socat 指定串口名称 指定名称时，用 link= 表示指定的串口位置，pty,raw,echo=0 表示串口的配置参数 1sudo socat -d -d pty,raw,echo=0,link=/dev/ttyACM0 pty,raw,echo=0,link=/dev/ttyACM1 然后还需要给 /dev/ttyACM0, /dev/ttyACM1 这两个串口权限，方便起见，这里直接全部设为 rwx 12sudo chmod 777 /dev/ttyACM0sudo chmod 777 /dev/ttyACM1"},{"title":"roboverse","path":"/roboverse/","content":"Robotics Dataset Scene Concerns real world complex, uncontrollable synthesis quality, diversity, standardization"},{"title":"使用 llama.cpp 在本地部署大模型","path":"/llama-cpp-locally-deploy/","content":"安装 llama.cpp 可以通过 brew 安装，一条命令行搞定，省心省力。 1brew install llama.cpp"},{"title":"Python 通过 URL 获取 Embedding","path":"/python-get-embedding-through-urls/","content":"requests 需求类似于不希望内部数据上传到其他网页，于是希望在本地同时部署 Embedding Model 和 LLM. 于是，我用 llama-server 同时 serve 了 BGE-m3 和 Deepseek-R1-Distill-Llama-8B，前者作为 Embedding 模型暴露在 http://localhost:8081，后者作为 LLM 暴露在 http://localhost:8080 然后就遇到了一个小问题，怎么通过 Python 去获取 Embedding 呢？我这里的解决方案是直接用 requests 库发送请求了。好在 llama.cpp 提供的 llama-server 能够兼容 OpenAI 的 API 接口。 12345678910111213141516171819202122import requestsembedding_url = &quot;http://localhost:8081/v1/embeddings&quot;# OpenAI compatible embeddingapi_key = &quot;not_used&quot;# 因为是本地部署，所有干脆没有设置 API Keydata = &#123; &quot;input&quot;: &quot;要嵌入的文字&quot;, &quot;model&quot;: &quot;BGE-m3&quot;, # 这里就填本地部署的模型名称&#125;headers = &#123; &quot;Authorization&quot;: f&quot;Bearer &#123;api_key&#125;&quot;, &quot;content-type&quot;: &quot;application/json&quot;,&#125;result = requests.post( embedding_url, data=str(data), # 这里必须将字典以字符串的格式传入 headers=headers, # 这个 headers 其实也可以不用)embedding = result.json()[&quot;data&quot;][0][&quot;embedding&quot;]"},{"title":"Linux Mint 22.1 升级 6.11 内核和升级英伟达 560 驱动","path":"/upgrade-to-linux-kernel611-and-nvidia/","content":"升级到 Linux Kernel 6.11.0-21.21 事情的原委很简单，Update Manager 疯狂地提醒我该升级 Kernel 了，正巧想玩玩 CUDA 升级一下 nvcc，于是想顺便升级一下 nvidia-driver. 但是很快就初见端倪，升级 Kernel 提示 1installed linux-image-6.11.0-21-generic package post-installation script subprocess returned error exit status 11 啊？报错了？又往前翻了翻，发现 1nvidia-fs/2.22.3 autoinstall failed due to missing dependencies: nvidia OK 破案了，原来又是英伟达驱动搞的鬼，那么先处理 nvidia-drvier 吧[1] nvidia-driver-560 我现在已经在用 nvidia-driver-550 了，但是为什么会报错缺少 nvidia 呢？这里没有多想，就顺着上面的帖子，重装驱动了。 我是直接在 Driver Manager 里的 GUI 操作，但是报错安装失败。 man! 怎么个事？重启一下系统，虽然可以开机，但是默认切换到 Intel 核显了，并且 nvidia-smi 也提示无法链接 GPU. 值得一提的是，系统居然是 6.11 内核的……我还以为没安装成功呢 hhhh 那看来只能进 recovery-mode 了 Recovery Mode 进入 Recovery Mode 后，尝试用命令行删除驱动，再重新安装驱动。先移动到 network 打开网络，然后移动到 root 回车进入命令行。先删除所有英伟达的驱动 1apt purge ~nnvidia 删除倒是挺简单的，然后安装驱动，我直接选择了 nvidia-driver-560-open（这个版本是英伟达官方推荐 Ubuntu 24.04 系统使用的驱动版本，而我使用的 Linux Mint 也是基于 Ubuntu 24.04 制作的） 1apt install nvidia-driver-560-open 回车等待结果，然而，在 Building for Linux-kernel-6.11.0-21.21-generic 的时候，却出现了报错 12nvidia-dkms-560 configuration failed# 大致差不多长这样，其实就是提示你有两个组件构建失败 不过在构建失败后，也给出了一个日志文件让我们去查看，日志目录是 /var/lib/dkms/nvidia/&lt;版本号&gt;/build/make.log，我们用 vim 进行查看 12cc: unrecognized command line option &#x27;-ftrivial-auto-var-init=zero&#x27;# 大致是这么个意思 然后又检查一下 cc -v 版本信息，发现是 11.4.0，而 nvidia-driver-560-open 是用 gcc-13 构建的 所以问题很明晰了，nvidia-driver-560-open 用 gcc-13 进行构建，但是由于编译时，可执行文件用的是 cc，而在我的机器上，我的 cc 版本为 gcc-11，所以不支持这个命令行参数（即 ftrivial-auto-var-init=zero），因此构建失败，也导致了后续一系列的问题。 好消息是，我本机已经安装过 gcc-13，因此，我先删除了 gcc-11, g++-11, gcc-11-base 等包，然后用 symlink 将 cc 直接映射为 gcc-13 1ln -s /usr/bin/gcc-13 /usr/bin/cc 再次安装 nvidia-driver-560-open，成功！安装 Linux Kernel 6.11，也是成功！ 还好没有心急让电脑 remake TAT https://answers.launchpad.net/ubuntu/+question/820141 ↩︎"},{"title":"Matplotlib 快速入门指南","path":"/matplotlib-fast-tutorial/","content":"网格布局 plt.subplots() 例如，我想将 121212 张 MNIST 图片排列成 333 行 444 列的样子。 使用 fig, axes = plt.subplot() 新建图片，并划分成网格 可以搭配 axes = axes.flatten() 进一步方便处理（将二维网格拍成一维，方便循环处理） 代码 1234567fig, axes = plt.subplot(3, 4) # 可以额外指定 fig_size 指定图片大小axes = axes.flatten()for i in range(12): axes[i].imshow(......) axes[i].set_title(......) # 每一张小图像的标题 axes[i].axis(&quot;off&quot;) 如果想要给整张图添加总标题的话，是 plt.suptitle(&quot;......&quot;) 散点图 plt.scatter()"},{"title":"conda 与 pip 配置代理服务器","path":"/conda-pip-proxy/","content":"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 Password is needed."},{"title":"海康威视 (HikVision) 相机食用指南","path":"/hikvision-camera/","content":"打开相机、关闭相机的流程 首先要 MV_CC_Initialize() 初始化相机 SDK 需要实现 enum_device() 找到相机设备 通过 MV_CC_CreateHandle() 创建 handle MV_CC_OpenDevice() 打开相机"},{"title":"Python LangChain 将图像当作 URL 传递","path":"/raw-image-pass-as-url/","content":"Pass Image as if URL 一个小 trick 可以将本地图片 encode 成 byte string 之后，放在 URL 栏里传递给多模态大模型。 123456789import base64with open(&quot;path/to/image.png&quot;, &quot;rb&quot;) as image_file: b64_image = base64.b64encode(image_file.read()).decode(&quot;utf-8&quot;)def encode(path): with open(path, &quot;rb&quot;) as image_file: code = base64.b64encode(image_file.read()).decode(&quot;utf-8&quot;) return f&quot;data:image;base64,&#123;code&#125;&quot; 然后就可以正常放在 URL 栏里了。"},{"title":"使用 Python 和 Flask 库快速构建网页后端","path":"/python-flask-framework/","content":"启动一个后端 导入库后，用 app = Flask(__name__) 初始化一个 App。 一个 function 对应一个子网页的服务，用 @app.route() 指明，最后 app.run() 启动后端。 一个后端子网页 123@app.route(&quot;/upload&quot;, methods=[&quot;POST&quot;])def upload(): # ...... 启动后端 12if __name__ == &quot;__main__&quot;: app.run(debug=True, host=&quot;0.0.0.0&quot;, port=5000) 当 host 为 0.0.0.0 的话，Flask 会多设置一个 IP 地址，供本机的其他程序访问后端。"},{"title":"Django 快速开始","path":"/django-kickstart/","content":"Django 组织结构 Django 大体架构是一个 Project 管理若干个小 Application，每一个 Application 负责一个功能，跟 Application 平行的还有一个用于部署网站的 Config Folder（默认和 Project 同名）. 1234567891011121314151617181920212223242526272829UH├── CedarsCenter│ ├── admin.py│ ├── apps.py│ ├── __init__.py│ ├── migrations│ │ └── __init__.py│ ├── models.py│ ├── tests.py│ └── views.py├── manage.py├── StudentCenter│ ├── admin.py│ ├── apps.py│ ├── __init__.py│ ├── migrations│ │ └── __init__.py│ ├── models.py│ ├── tests.py│ └── views.py└── UH ├── asgi.py ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-311.pyc │ └── settings.cpython-311.pyc ├── settings.py ├── urls.py └── wsgi.py 管理某一个 Project 的时候，通过 manage.py 运行相应的指令。例如在当前 Project 下新增一个 Application poll，则运行 1python manage.py startapp poll Django 中 Model 的作用 Model 的作用只是用来查询数据用的"},{"title":"Python 使用 C/C++ 接口","path":"/python-c-integration/","content":"Python 调用 C/C++ 代码 如何编写 C/C++ 代码？ 首先导入 Python.h 头文件，包含了必要的结构体、方法。（需要通过 sudo apt install python3-dev 提前安装好） 12#define PY_SSIZE_T_CLEAN#include &lt;python3.12/Python.h&gt; // 我这里需要额外指定一下路径 编译为动态库 1g++ -fPIC [file_name] -shared -o [module_name].so 在 Python 里使用 直接通过这个 Module 的名字导入 123import [module_name]# ......"},{"title":"MinerU Examples","path":"/MinerU-examples/","content":"uv 包管理器安装 MinerU 先用 uv 安装 setuptools wheel torch 1uv pip install setuptools wheel torch 然后再安装 detectron2 1uv pip install --no-build-isolation git+https://github.com/facebookresearch/detectron2.git 最后安装 magic-pdf[full] 1uv pip install &#x27;magic-pdf[full]&#x27; --extra-index-url https://wheels.myhloli.com --prerelease=allow 最后检查 magic-pdf 的版本 &gt;=0.7.0，而不是 0.6.1 如果像使用 GPU 进行 PaddlePaddle OCR 的推理，继续安装 paddlepaddle-gpu 1uv pip install paddlepaddle-gpu MinerU Command Line MinerU API 使用指南 MinerU 的使用流程基本上是 将 PDF 加载为 magic_pdf.data.dataset.Dataset 执行 OCR 和 Layout Inference 这里还想更详细地记录一下 API，感觉 Documentation 里写的不是很全，得从 demo.py 里找。"},{"title":"image-text-database","path":"/image-text-database/","content":"372f7a9688aae3bb4ec6e6afab19ca439bb95e6d91164a50dda5c3f9a4f813ff Password is needed."},{"title":"软件工程：UML","path":"/swe-uml/","content":"Summary of “UML in Object-Oriented Analysis” 1. Purpose of UML in Analysis Modeling Goals: Understand the problem domain and provide a basis for design. Communicate design ideas efficiently among developers. Identify defects and omissions in specifications and designs before coding. Speed and Clarity: UML diagrams are faster to sketch than code and serve as a common language for developers. 2. UML Diagram Types Structure Diagrams: Class Diagrams: Model static structure (classes, attributes, relationships). Object Diagrams: Represent instances of classes at a specific point in time. Composite Structure Diagrams: Show internal structure of objects and collaborations. Behavior Diagrams: Activity Diagrams: Model workflows or business processes. State Machine Diagrams: Describe state transitions of objects. Interaction Diagrams: Sequence Diagrams: Depict object interactions over time. Communication Diagrams: Show message flows between objects. 3. Class Diagrams: Core of Domain Modeling Components: Classes: Named boxes with attributes (data) and operations (methods). Associations: Relationships between classes (e.g., Customer ↔ Order). Multiplicity: Defines cardinality (e.g., 1..* = “one or more”). Navigability: Direction of association (arrows). Domain Model: Represents problem-domain concepts (e.g., Product, Transaction, User). Focuses on conceptual classes (not implementation details). Steps to Build: Identify Classes: Use noun phrases from requirements and category lists (e.g., “Transaction,” “Record”). Define Attributes: Simple values (e.g., price, date). Avoid complex data as attributes (e.g., Address should be a class). Model Associations: Only meaningful relationships (e.g., Order contains OrderLineItems). 4. Best Practices for Class Diagrams Keep It Simple: Omit trivial associations to avoid clutter. Prioritize “need-to-know” relationships. Multiplicity Constraints: Use 1, 0..1, * (unbounded), or n (specific number) to define valid instances. Association Classes: Resolve many-to-many relationships by adding a class (e.g., Reservation between Customer and Flight). Example: SkillLevel between GameCharacter and Skill to track proficiency levels. 5. Domain Modeling Techniques Techniques for Identifying Classes: Noun/Phrase Extraction: Extract nouns from requirements (e.g., “Product Catalog” from “view products”). Category List: Use predefined categories (e.g., “Transaction,” “Container,” “Role”). Avoid Overcomplication: Do not model every relationship (e.g., ignore transient interactions like “Developer estimated PBI”). Focus on persistent relationships (e.g., PBI belongs to a Product Backlog). 6. Example: Domain Model for a POS System Key Classes: Customer, Cashier, Product, Sale, CashPayment. Associations: Sale contains SalesLineItems (multiplicity 1..*). Product is described by ProductDescription. Attributes: Sale has dateTime, total, and CashPayment (optional). 7. UML in Agile Development Iterative Approach: Build domain models incrementally, focusing on current user stories. Alignment with Implementation: Domain models inform REST API endpoints (e.g., shuttlebuses endpoint from ShuttleBus class). 8. Common Pitfalls Over-Simplification: Modeling complex data (e.g., Address) as attributes instead of classes. Over-Modeling: Including transient or unimportant relationships (e.g., Developer ↔ PBI for estimation). 9. Conclusion UML is a foundational tool for analyzing and designing software systems. Class diagrams are central to modeling problem domains and translating them into solutions. Proper use of UML reduces defects, clarifies requirements, and supports agile development practices. This summary captures the document’s focus on UML’s role in analysis, key diagrams, domain modeling techniques, and practical best practices."},{"title":"软件工程：Software Requirement Specification","path":"/swe-srs/","content":"Summary of “Software Requirements: Introduction” 1. Core Challenges in Requirements Engineering Key Issue: Understanding the problem the system must solve, often derived from customer/end-user needs. Challenges: Developers may misunderstand the business problem, leading to incorrect solutions. Customers/users may not clearly articulate their needs (especially problematic in predictive models like Waterfall). Developers must align software with end-users’ workflows to ensure practicality. 2. Drivers of Requirements Custom Software (Project-Based): Requirements driven by specific clients (e.g., government, enterprise tools). Contractual obligations dictate functionality and delivery timelines. Ownership transfers to the client post-delivery. Software Products: Requirements originate from the company’s vision/opportunity (e.g., apps, productivity tools). Developers prioritize features and constraints without a specific client’s contractual demands. Continuous evolution: Features can be added/removed post-release. 3. Types of Requirements Information Business Requirements: High-level organizational objectives (e.g., “Increase customer satisfaction”). Business Rules: Policies/constraints (e.g., pricing policies). Stored in a Business Rules Catalogue, not part of the SRS itself. User Requirements: Tasks users must perform or desired attributes (e.g., “Users want to check in online”). Functional Requirements: Specific behaviors the system must perform (e.g., “The system shall calculate fuel quantity”). Features: Bundles of related functional requirements (e.g., “User Authentication Feature”). System Requirements: Top-level/system-wide requirements (e.g., integration with hardware). Non-Functional Requirements: Properties like performance, security, usability (e.g., “Authorization must take ≤2 seconds”). External Interface Requirements: Connections to users, systems, or hardware (e.g., “JSON format for data exchange”). Constraints: Restrictions on development (e.g., “Use open-source libraries only”). 4. Business Rules Management Documentation: Rules are recorded in tables with: Rule ID, Rule Description, Volatility (how often they change), and Source (e.g., company policy). Example: Delivery pricing rules for an e-commerce system. Impact: Influence functional/non-functional requirements (e.g., permissions, UX design). 5. Software Requirements Specification (SRS) Purpose: Formal document outlining all functional and non-functional requirements. Structure: Introduction: Purpose, scope, conventions. Overall Description: Product context, user classes, constraints. System Features: Detailed functional requirements (e.g., “The system shall…”). Data Requirements: Logical models, reports, data retention policies. External Interfaces: User, software, hardware, and communication interfaces. Quality Attributes: Usability, performance, security, safety. Appendices: Glossary, analysis models. Formality: Varies by system criticality (e.g., strict for avionics vs. informal for web apps). Example: Flight Management System (FMS) requirements trace from system to low-level software specs (e.g., fuel quantity display in pounds/kilograms). 6. Requirements Traceability Critical in Safety-Critical Systems (e.g., aviation): Requirements must be bi-directionally traceable (e.g., from system to code). Example: FMS fuel quantity requirements linked to sensor polling intervals and data validity checks. DO-178C Compliance: Mandates traceability for airborne software. 7. Agile Requirements Management Alternatives to Traditional SRS: Vision and Scope Document: High-level goals and constraints. Product Backlog: Prioritized user stories/use cases with acceptance criteria. Dynamic Updates: Requirements evolve iteratively (e.g., user stories refined during sprints). Working Software: Focus on delivering increments over exhaustive documentation. Example: User stories for a flight check-in system (e.g., “As a passenger, I want to print boarding passes after check-in”). 8. Key Takeaways Requirement Types: Differentiate between business rules, user needs, functional/non-functional requirements. Documentation: Formal SRS for critical systems; agile backlogs for product development. Traceability: Essential for compliance and defect prevention in safety-critical domains. Agile Adaptation: Prioritize collaboration, iterative delivery, and flexibility over rigid planning. This summary captures the document’s focus on understanding requirements, their classification, documentation practices, and the shift toward agile methodologies for modern software development."},{"title":"软件工程：Software Process Model","path":"/swe-software-process-model/","content":"Summary of “Software Process and Activities: Process Models” 1. Introduction to Software Development Lifecycle (SDLC) SDLC: A structured sequence of development activities and tasks, organized into phases. Companies may adapt these models, but they generally fall into predictive or adaptive categories. Predictive Models: Plan all phases upfront (e.g., Waterfall). Emphasize control, documentation, and fixed requirements. Adaptive Models: Respond to change iteratively (e.g., Agile). Focus on flexibility, customer collaboration, and empirical process control. 2. Predictive Models: The Waterfall Model Structure: Linear, sequential phases: Requirements → 2. Design → 3. Implementation → 4. Testing → 5. Deployment → 6. Maintenance Key Features: Heavyweight Documentation: Formal plans and deliverables (e.g., requirements specs, design docs, test reports). Phase Dependency: A phase cannot start until the prior phase is completed and accepted. No re-entry once finalized. Milestones: Major deliverables reviewed at each phase to ensure completion. Advantages: Clear visibility, management control, and contractual clarity for mission-critical systems (e.g., aerospace, healthcare). Aligns with regulatory standards (e.g., DO-178C for airborne software). Disadvantages: Rigid: Cannot easily accommodate requirement changes or design flaws discovered late. Late Validation: Customers see a working product only at the end, risking undetected defects (cost to fix defects rises exponentially as development progresses). Inflexibility: Feedback loops require costly rework, often leading to frozen deliverables. 3. Adaptive Models: Agile and Empirical Process Control Core Principles: Transparency: Shared understanding of progress and goals. Inspection: Regular reviews of work and processes. Adaptation: Adjustments based on new knowledge. Iterative &amp; Incremental Development: Iterations (Sprints): Short, time-boxed cycles (e.g., 1–2 weeks in Scrum) delivering potentially releasable increments. Phases per Iteration: Each cycle includes planning, design, implementation, integration, testing, and review. Flexibility: Goals set at iteration start; requirements evolve over time. Benefits: Early and frequent customer feedback reduces late-stage defects. Handles uncertainty and complexity in innovative projects. Prioritizes working software over exhaustive documentation. Drawbacks: Overhead may outweigh benefits for simple, low-risk projects with stable requirements. 4. Product-Based vs. Project-Based Development Custom Software (Project-Based): Developed for specific clients (e.g., government systems, enterprise tools). Follows predictive models (e.g., Waterfall) for contractual clarity and stability. Ownership transfers to the client post-delivery. Software Products: Mass-market solutions (e.g., apps, productivity tools). Prioritizes time-to-market over rigid planning. Agile frameworks dominate due to rapid iteration and competition. Self-Managed Teams: No traditional project manager; roles like Product Owner (prioritizes backlog) and Scrum Master (facilitates process) are key. Continuous Development: Treated as ongoing processes, not discrete projects. 5. CHAOS Report Insights on Project Management Non-Agile Projects: Success rates vary with project manager skill, but bureaucracy and slow decision-making hinder outcomes. Agile Projects: Traditional project managers reduce success rates due to process overhead. Conclusion: Agile thrives without hierarchical project managers. Avoid tools like EPPM (Enterprise Project Portfolio Management) that introduce bureaucracy. Key Takeaway: “Software is infinite, while projects are finite.” Modern development should avoid artificial project boundaries and focus on continuous delivery. 6. When to Use Each Model Predictive Models (Waterfall): Appropriate for: Safety-critical systems, stable requirements, long-term contracts, and regulatory compliance. Risks: Fails in dynamic environments or when requirements are unclear. Adaptive Models (Agile): Appropriate for: Product development, innovative projects, evolving requirements, and complex problem-solving. Risks: Overhead for simple projects; requires disciplined team collaboration. 7. Conclusion Modern Trends: Shift toward product-based, Agile methodologies due to faster iteration, customer-centricity, and adaptability. Process Selection: Choose models based on project complexity, risk, and requirements stability. Predictive models remain viable for specific domains (e.g., aerospace), while Agile dominates in competitive, evolving markets. This summary encapsulates the document’s contrast between predictive and adaptive models, their strengths/weaknesses, and the industry’s move toward Agile for product development."},{"title":"multi-thread","path":"/multi-thread/","content":"372f7a9688aae3bb4ec6e6afab19ca439bb95e6d91164a50dda5c3f9a4f813ff Password is needed."},{"title":"大模型权重文件 .gguf 分析以及转换","path":"/gguf/","content":".gguf .gguf 是 Hugging Face 自己开发的大模型权重格式，适合快速载入模型以及推理。最初在 llama.cpp 项目里使用。 GGUF"},{"title":"Linux 下用 HKU 账号使用 HK eduroam WiFi","path":"/connect-to-eduroam-hku/","content":"eduroam 我是 Linux Mint 系统，所以 Ubuntu/Mint 应该比较通用。 Option Value Security WPA &amp; WPA2 Enterprise Authentication Protected EAP (PEAP) Anonymous Identity 留空 Domain hku.hk CA Certificate 选 (None)，勾选 No CA Certificate is required PEAP version Automatic Inner Authentication MSCHAPv2 Username &lt;UID&gt;@hku.hk，&lt;UID&gt; 是邮箱 @ 前的部分，这里后缀必须是 @hku.hk Password Portal 的登录密码"},{"title":"Docker Introduction","path":"/docker-intro/","content":"Dockerfile Docker Image 编写完 Dockerfile 后，我们可以用命令创建一个 docker 镜像 1docker build -t [镜像名称] [Dockerfile 所在目录] 启动 container 还需要一个 container 才能运行起来 1docker run -p [映射到主机的哪个端口]:[容器内的哪个端口] -d [镜像名称] -p：指定端口 -d：后台运行。想要查看终端输出的话需要到 docker desktop 里查看 用 Volume 保存 container 的数据 利用 -v 参数，把本地文件夹 ~/A 挂载到 container 里的 .../B，这样在 container 里读写 .../B 其实等于读写 ~/A. 我们先创建一个数据卷 1docker volume create [数据卷位置] 然后在 docker run 的时候进行挂载 1docker run -v [数据卷在本地的位置]:[数据卷在容器里的位置] -d [镜像名称] Docker Compose 多个容器共同协作：例如数据库和前端分离。 在 docker-compose.yml 里用 services 进行定义 12345678910111213# docker-compose.ymlservices: # 定义前端和数据库 front-end: # 前端 build: . # 镜像——从文件夹构建 ports: # 端口映射 - &quot;80:5000&quot; database: # 数据库 image: &quot;mysql&quot; # 镜像——从其他地方拉取 environment: # 可以定义环境变量 OPENAI_API_KEY: &quot;...&quot; OPENAI_API_BASE: &quot;&quot; volumes: # 数据卷，等同于 -v 参数 - &quot;~/A:.../B&quot; 定义完毕后，使用 1docker compose up -d 来运行所有的 container 1docker compose down 来停止并删除所有的 container"},{"title":"本地部署 llama.cpp 大模型服务器并连接","path":"/local-deploy/","content":"llama.cpp 的安装、编译 alternative: llama-cpp-python 提供了 llama.cpp 的 Python 接口。通过 llama-cpp-python 也可以启动一个 LLM Server 启动一个 LLM server 直接在命令行里输入启动服务器 1llama-server -m [模型路径] --port 8080 模型要保证必须是 .gguf 格式，可以使用 llama.cpp 项目根目录下的 convert_hf_to_gguf.py 进行转换。 convert_hf_to_gguf 食用方法 配置好虚拟环境后，命令行里输入 1python convert_hf_to_gguf.py [模型.bin文件所在的目录] 这个目录末尾应该是哈希码，例如 ~/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562 连接 如果用 LangChain 进行连接，必须注意要输入 http://localhost:8080 的 http://（被坑了）"},{"title":"friends","path":"/friends/index.html","content":"Friends XXZ’s blog"},{"title":"About Me","path":"/about/index.html","content":"About Me"},{"title":"OpenVLA 代码解析 (1)","path":"/wiki/agentai/OpenVLA-code-p1.html","content":"5d4944576327e6e1f59f35f07afdbc7917ffe61195e5ff6f7af12e15300ba733 Password is needed."},{"title":"OpenVLA-dataproc-pipeline","path":"/wiki/agentai/OpenVLA-dataproc-pipeline.html","content":"OpenVLA 数据处理过程 normalization diff embodyment 在环境下评测 model, performance, accuracy"},{"title":"OpenVLA 论文","path":"/wiki/agentai/OpenVLA.html","content":"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 Password is needed."},{"title":"Google Gemini Robotics","path":"/wiki/agentai/google-gemini-robotics.html","content":"AI Summary Background • Core Problem: The paper addresses the challenge of bridging the gap between state‐of‐the‐art multimodal AI models and practical robotics. Although digital AI models have achieved impressive capabilities in processing text, images, and other modalities, these models traditionally lack the embodied reasoning (i.e., the ability to understand and physically interact with the real world) required for autonomous robot control. • Current Limitations and Challenges: Existing vision–language models excel at perception and symbolic tasks, yet they fall short when it comes to physical grounding. The authors point out that current research struggles with robust spatial understanding, dexterous manipulation, safe human–robot interactions, and adapting to novel object configurations and environments. Moreover, there is an absence of standardized benchmarks that cover the nuanced set of reasoning skills needed for embodied tasks. • Real-world Needs and Theoretical Gaps: The paper underscores the necessity for general-purpose robots capable of smooth, safe, and reactive movements. There is a critical need to extend digital reasoning to handle real-world cues such as 3D geometry, object affordances, and trajectory planning. The authors identify a theoretical gap in unifying multimodal understanding (visual and language) with physical action planning. Motivation • Driving Factors Behind the New Method: The motivation comes from the desire to leverage the advanced multimodal reasoning of models like Gemini 2.0 and extend these capabilities to control physical robots. The authors aim to overcome the limitations of existing systems which are typically confined to digital domains by integrating a comprehensive embodied reasoning framework into robotics. • Shortcomings of Existing Approaches: Current practices often suffer from poor generalization when faced with unseen object types, variable settings, and complex manipulation tasks. Many existing methods lack the ability to plan sequential actions under safety constraints and cannot efficiently adapt to new robot embodiments or environments. Issues such as limited computational efficiency in planning and a narrow focus on perception without actionable outputs further drive the need for an improved, unified approach. Methodology • Core Innovation: The paper introduces the Gemini Robotics family of models––a suite of vision–language–action (VLA) systems that fuse the powerful multimodal understanding of Gemini 2.0 with a dedicated robotics layer. Two principal variants are proposed: Gemini Robotics-ER (Embodied Reasoning): Enhances the base model’s spatial and temporal reasoning to perform tasks such as object detection, 2D pointing, trajectory prediction, and top-down grasp estimation. Gemini Robotics: Extends these capabilities by incorporating robot action data to achieve high-frequency, dexterous control over physical robots. • Technical Steps and Key Components: Multimodal Input Fusion: Integrates visual inputs (images) and textual instructions, using open‐vocabulary queries to detect and localize objects in both 2D and 3D (e.g., 3D bounding boxes and multi-view correspondence). Embodied Reasoning Pipeline: Employs a chain-of-thought (CoT) prompting technique, which encourages the model to generate intermediate reasoning steps, thereby ensuring precise spatial grounding and sequential planning. (Chain-of-thought prompting is a method where the model is instructed to “think out loud” through intermediate steps before giving the final answer.) Robot Control Integration: A dedicated pipeline converts high-level planning into low-level actuation commands via a robot API, managing safe and reactive motions while adhering to physical constraints. Specialization and Adaptation: An optional fine-tuning stage enables the system to adapt to long-horizon tasks, enhance dexterity (e.g., folding or playing cards), and adjust to variations in robot embodiments such as bi-arm platforms or humanoids. • Fundamental Differences from Baseline Approaches: Unlike conventional methods that may treat perception and control as separate tasks, the Gemini Robotics models offer an end-to-end, integrated solution. By combining embodied reasoning with action data, these models generalize better to novel tasks, improve adaptation with few demonstrations, and facilitate safe control in dynamic environments. Experiments &amp; Results • Experimental Setup: The models are evaluated on several benchmarks including the newly introduced Embodied Reasoning Question Answering (ERQA) benchmark, RealworldQA, and BLINK. The experimental design encompasses tasks ranging from simple object detection to complex, dexterous manipulation tasks in both simulated and real-world scenarios. Datasets include images sourced from in-house collections and publicly available datasets like OXE, UMI Data, and MECCANO. Baseline comparisons are made against existing vision–language and diffusion-based robotics models, using performance metrics such as accuracy, success rate, and continuous progress scores. • Key Findings: Gemini 2.0 variants (Flash and Pro Experimental) achieve state-of-the-art performance across multiple benchmark evaluations, often with notable improvements when chain-of-thought prompting is used. Quantitative results show improvements in tasks like 2D pointing accuracy (measured by how well predicted points fall within ground-truth regions) and successful grasp predictions in real-world robot control. In dexterous tasks such as folding an origami fox or playing a card game, Gemini Robotics outperforms baseline models in both binary success metrics and nuanced progress scores. • Ablation and Sensitivity Analyses: The paper reports ablation studies that highlight the benefits of each component—from embodied reasoning enhancements to fine-tuning stages. Sensitivity analyses reveal that prompts such as chain-of-thought can significantly affect overall performance, and that the integrated approach provides robust improvements even under distribution shifts (e.g., novel visual variations or rephrased instructions). Effectiveness Analysis • Underlying Reasons for Success: The method’s effectiveness stems from its dual focus on robust multimodal understanding and practical action integration. By augmenting a strong foundation model with specialized embodied reasoning and robot-specific training, the system manages to bridge the digital–physical divide. In particular, the use of multi-view spatial understanding and sequential reasoning ensures continuity between perception and physical manipulation. • Critical Success Factors: Model Architecture and Training: Leveraging Gemini 2.0’s large-scale, pre-trained capabilities provides a strong foundation for embodied reasoning. Effective Data Fusion: Integrating diverse data types—including images, text, and robot sensor readings—improves generalization and adaptability. Prompting Strategies: Techniques like chain-of-thought prompting contribute to clear intermediate reasoning, enhancing the decision-making process. Safety and Adaptability Protocols: The incorporation of robot API interfaces and safety guidelines enables smooth interaction in regulated physical environments. • Potential Limitations and Applicability Boundaries: While the proposed models demonstrate impressive performance, their success is tightly coupled with the quality and diversity of training data. The system may face constraints when encountering drastically different physical environments or unanticipated object configurations. Furthermore, the reliance on comprehensive safety protocols suggests that additional work is required to ensure deployment in less structured or more variable real-world settings. Contributions • Theoretical Contributions: The paper provides a unified framework that connects high-level multimodal reasoning directly with low-level robot control. It introduces new benchmark tasks (e.g., ERQA) to evaluate embodied reasoning, setting a new standard for measuring the full spectrum of capabilities required for physical interaction. • Practical Contributions: The development of the Gemini Robotics family offers an end-to-end solution for controlling robots with diverse manipulation tasks, from dexterous object handling to adaptive task execution. Detailed model cards and open-source evaluation protocols are provided, facilitating reproducibility and further research in embodied AI. The work also contributes guidelines and best practices for ensuring safety and responsible development in robotics applications. • Implications for Future Research: This research marks a significant step toward general-purpose embodied AI. Future work can build on these findings to further enhance generalization, reduce reliance on extensive fine-tuning, and tackle more complex physical interactions. The integration of multimodal reasoning with action control paves the way for robots that can adapt to diverse, unstructured real-world environments while ensuring safe and reliable operation."},{"title":"Pi 0, A Vision-Language-Action Flow Model for General Robot Control","path":"/wiki/agentai/pi-zero.html","content":"AI Summary Background The paper addresses the challenge of developing a unified robot foundation model that can handle a wide range of dexterous tasks across different robot platforms. Current robot learning approaches often suffer from limited generalization, data scarcity, and brittle performance when adapting to complex practical scenarios. Existing methods typically focus on narrow tasks or use discretized models that struggle with continuous, high-frequency control. The authors identify a real-world need for models that can integrate rich semantic understanding (gained from large-scale vision-language pre-training) with fine-grained physical control to perform intricate multi-stage tasks such as laundry folding or box assembly. Motivation The direct drive behind this work is the gap between the robust, versatile behavior seen in large language and vision-language models and the limitations of current robot control systems. Existing approaches fail to meet the demands of real-world robotic manipulation due to: Limited Generalization: Models are often trained on task-specific data and do not transfer well to diverse environments or novel tasks. Discrete Outputs: Many prior methods rely on autoregressive and token-based approaches, which are not well-suited for high-frequency continuous control. Data and Recovery Challenges: Training solely on curated, high-quality data results in brittle systems that cannot efficiently recover from errors or unexpected perturbations. This motivates the integration of internet-scale semantic pre-training with a continuous action generation method, thereby combining robust, flexible understanding with high-precision control. Methodology The core innovation is the design of a vision-language-action model (π0) that unifies pre-trained semantic knowledge with a novel mechanism for continuous action prediction. Key elements include: Pre-trained VLM Backbone: The model starts with a vision-language model (VLM) pre-trained on vast internet data (e.g., PaliGemma). This allows the system to inherit rich semantic representations and language understanding. Brief explanation: A VLM (Vision-Language Model) is trained on image-text pairs to extract visual and linguistic features simultaneously. Action Expert with Flow Matching: A separate module—termed the action expert—is added to predict continuous action sequences. Instead of relying on discrete token outputs, it uses conditional flow matching. Brief explanation: Flow matching is a variant of diffusion methods where a denoising vector field is learned to progressively transform noise into data, enabling precise continuous outputs. The model predicts action chunks (e.g., 50 future steps) at once, accommodating high-frequency control (up to 50 Hz). The design employs multi-stage training where the model is first exposed to diverse, lower-quality pre-training data and later fine-tuned on high-quality, task-specific data. Architectural Separation: The transformer backbone routes standard inputs (images and language) through the pre-trained VLM segment, while robot-specific proprioceptive inputs and continuous actions are handled by an additional expert. This mixture of experts approach allows leveraging pre-existing knowledge while adapting to robotics-specific demands. Two-Phase Training Recipe: Emphasizing the importance of both data diversity and quality, the training is split into a large-scale pre-training phase (covering 10,000 hours of data across 68 tasks and 7 robot configurations) and a post-training phase that refines the model for complex downstream tasks. Experiments &amp; Results The authors evaluate their model on a wide range of dexterous manipulation tasks spanning different robots and complexities: Experimental Setup: Tasks include shirt folding, table bussing (easy and hard versions), grocery bagging, removing toast from a toaster, laundry folding, box assembly, packing eggs, and more. Diverse robot configurations such as single-arm, dual-arm, and mobile manipulators are used to test cross-embodiment performance. Evaluation metrics involve task-specific score rubrics that measure progress and success (e.g., percentage of task completion or discrete scores per subtasks). Key Findings: The full π0 model, trained with both pre-training and fine-tuning, outperforms baseline approaches like OpenVLA and smaller models (π0-small) on both direct prompting (out-of-box performance) and after fine-tuning. Ablation studies show that incorporating continuous action generation via flow matching results in significant improvements in dexterity and robustness. The approach demonstrates the capability to perform complex, multi-stage tasks that were previously challenging or unsolvable using traditional methods. Effectiveness Analysis The method works due to several critical factors: Integration of Semantic and Physical Knowledge: The use of a pre-trained VLM imparts broad semantic understanding, enabling the model to interpret language commands effectively. This, when combined with flow matching, allows the system to translate high-level instructions into accurate continuous motions. Flow Matching for Continuous Control: By modeling the entire distribution of actions as a continuous process, the approach ensures high precision and the ability to recover from errors—a necessity in real-world dexterous tasks. Robust Training Recipe: The dual-phase training method (diverse pre-training followed by targeted fine-tuning) allows the model to learn both general competencies and task-specialized behaviors while mitigating issues of overfitting and brittleness. Critical Success Factors: The modular architecture separating vision-language and robotics-specific pathways facilitates efficient processing and speed. The ability to predict action chunks supports high-frequency control, crucial for tasks that require fine motor skills. Potential limitations include the reliance on massive amounts of pre-training data and the current lack of comprehensive understanding about optimal data composition. Moreover, while promising for robot manipulation, it remains to be seen how well the approach transfers to other domains such as autonomous driving or legged locomotion. Contributions The paper makes several novel contributions: Theoretical Contributions: It introduces a novel integration of a pre-trained vision-language model with flow matching to generate continuous action distributions—bridging the gap between discrete output methods and the requirements of dexterous robot control. The work advances our conceptual understanding of how multi-modal pre-training can be leveraged to build versatile and general-purpose robot foundation models. Practical Contributions: The π0 model is demonstrated across multiple robot embodiments and a wide array of tasks, showcasing its practicality and robustness in real-world settings. The proposed multi-stage training recipe—including cross-embodiment data integration and action chunking—sets a new benchmark for large-scale robot learning experiments (using approximately 10,000 hours of diverse data). The empirical results establish a state-of-the-art performance in complex, temporally extended tasks such as laundry folding and box assembly. Implications for Future Research: This work lays the groundwork for future exploration of universal robot foundation models that might extend beyond manipulation to domains like navigation and legged locomotion. The proposed framework encourages further studies on optimal data weighting, integration techniques, and extending these methods to even broader real-world scenarios. Background Large Scale Right Model Architecture Right Trainging Recipe"},{"title":"内核性能分析工具","path":"/wiki/ai-infra/performance-analysis-tools.html","content":"内核优化的常见步骤 分析 Kernel 的执行时间 统计、查看各个 Kernel 的执行时间 定位性能瓶颈，确定需要优化的 Kernel 检查 GPU 的利用率等信息 例如检查是否占用过多的寄存器 （更细粒度）确定 Kernel 的性能瓶颈 优化 Kernel 性能 通过各种技术手段（软硬件、调度、内存优化）优化 Kernel PyTorch 内置 torch.profiler TensorBoard Holistic Trace Analysis NVIDIA Nsight 工具 Nsight System Nsight Compute Triton Proton"},{"title":"Triton Introduction","path":"/wiki/ai-infra/triton-intro.html","content":"PyTorch 关键组件 TorchDynamo 将所有复杂算子简化到 PrimTorch 中的 250 个算子 移除未使用的算子 确实需要存储、写入内存的中间算子，以及可融合的算子，从而减少开销 PrimTorch 定义了两个算子集合：Aten ops 和 Prim ops 将 PyTorch 程序的各种计算用这些算子集里的算子表示 简化后端需要编写的算子数量 AOTAutograd 提前获取反向传播 基于完整的 forward/backward 根据算子的依赖关系进行算子调度，对算子和层进行融合 TorchInductor 进行算子融合 自动生成低级 GPU 上的 Triton 代码（或者 CPU 上的 C++/OpenMP） 编译流程 我们用下面的例子介绍一下大致的编译流程，在运行时加入调试参数 TORCH_LOGS=&quot;...&quot; python example.py 查看中间的日志输出 12345678910import torch@torch.compiledef toy_example(x: torch.Tensor) -&gt; torch.Tensor: y = x.sin() z = y.cos() return zif __name__ == &quot;__main__&quot;: x = torch.randn(1000, device=&quot;cuda&quot;, requires_grad=True) # 开启反向传播 Step 1. TorchDynamo 运行 TORCH_LOGS=&quot;dynamo&quot; uv run example.py，我们先来看第一步 TorchDynamo 的输出。 日志输出12345678910111213141516171819202122232425262728293031323334[torch/_dynamo/symbolic_convert.py:2706] [0/0] Step 1: torchdynamo start tracing toy_example [很长的路径]/example.py:5[torch/_dynamo/symbolic_convert.py:3028] [0/0] Step 1: torchdynamo done tracing toy_example[torch/_dynamo/output_graph.py:1458] [0/0] Step 2: calling compiler function inductor[torch/_dynamo/output_graph.py:1463] [0/0] Step 2: done compiler function inductor[torch/fx/experimental/symbolic_shapes.py:4547] [0/0] produce_guards[torch/_dynamo/pgo.py:636] [0/0] put_code_state: no cache key, skipping[torch/_dynamo/eval_frame.py:398] TorchDynamo attempted to trace the following frames [ toy_example [很长的路径]/example.py:5 ][torch/_dynamo/utils.py:446] TorchDynamo compilation metrics: Function Runtimes (s) ------------------------------------ -------------- _compile.compile_inner 0.5482 OutputGraph.call_user_compiler 0.4845 _recursive_pre_grad_passes 0.0018 create_aot_dispatcher_function 0.4817 _recursive_joint_graph_passes 0.0684 compile_fx.&lt;locals&gt;.fw_compiler_base 0.3442 compile_fx_inner 0.3437 inductor_codecache_torch_key 0.0523 TritonBundler.read_and_emit 0.0002 PyCodeCache.load_by_key_path 0.0122 async_compile.precompile 0.007 async_compile.wait 0.0001 从日志中可以看到，TorchDynamo 的框架流程就是 对要编译的模型进行追踪，然后编译并生成中间表示 (FX Graph IR) 调用 compiler.inductor 对模型进行化简 1.1 Dynamo 图捕获 Dynamo 首先进行图捕获。这里，__graph_code 将原始代码的 Dataflow 进行捕获，并输出捕获的 DAG，即 FX Graph IR. FX Graph IR1234567891011121314151617181920212223# [torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code] def forward(self, L_x_: &quot;f32[1000]&quot;): l_x_ = L_x_ # File: example.py:7 in toy_example, code: y = x.sin() y: &quot;f32[1000]&quot; = l_x_.sin(); l_x_ = None # File: example.py:8 in toy_example, code: z = y.cos() z: &quot;f32[1000]&quot; = y.cos(); y = None return (z,)[torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code] def forward(self, L_x_: &quot;f32[1000][1]cuda:0&quot;): l_x_ = L_x_ # File: example.py:7 in toy_example, code: y = x.sin() y: &quot;f32[1000][1]cuda:0&quot; = l_x_.sin(); l_x_ = None # File: example.py:8 in toy_example, code: z = y.cos() z: &quot;f32[1000][1]cuda:0&quot; = y.cos(); y = None return (z,) 1.2 AOTAutograd Dynamo 的 AOTAutograd 阶段 生成正向传播图和反向传播图（也是表示为 FX Graph IR 的形式） 会将 FX Graph IR 中的算子替换为 ATen 算子库里的算子 基于完整的正向、反向传播图的视角，根据依赖关系，进行算子调度、对算子和层进行融合 将复杂的算子根据字典进一步分解为更底层的 Core ATen IR 算子或者 Prim IR 算子 AOTAutograd IR 生成的正向图与反向图12345678910111213141516171819202122232425262728293031# 这个是正向图# ===== Forward graph 0 =====# torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module): def forward(self, primals_1: &quot;f32[1000][1]cuda:0&quot;): ## File: example.py:7 in toy_example, code: y = x.sin() sin: &quot;f32[1000][1]cuda:0&quot; = torch.ops.aten.sin.default(primals_1) ## File: example.py:8 in toy_example, code: z = y.cos() cos: &quot;f32[1000][1]cuda:0&quot; = torch.ops.aten.cos.default(sin); sin = None return (cos, primals_1)# 这个是反向图# [torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:603]# [0/0] [__aot_graphs]# # TRACED GRAPH# ===== Backward graph 0 =====&lt;eval_with_key&gt;.1 class GraphModule(torch.nn.Module): def forward(self, primals_1: &quot;f32[1000][1]cuda:0&quot;, tangents_1: &quot;f32[1000][1]cuda:0&quot;): # File: example.py:7 in toy_example, code: y = x.sin() sin: &quot;f32[1000][1]cuda:0&quot; = torch.ops.aten.sin.default(primals_1) # File: example.py:8 in toy_example, code: z = y.cos() sin_1: &quot;f32[1000][1]cuda:0&quot; = torch.ops.aten.sin.default(sin); sin = None neg: &quot;f32[1000][1]cuda:0&quot; = torch.ops.aten.neg.default(sin_1); sin_1 = None mul: &quot;f32[1000][1]cuda:0&quot; = torch.ops.aten.mul.Tensor(tangents_1, neg); tangents_1 = neg = None # File: example.py:7 in toy_example, code: y = x.sin() cos_1: &quot;f32[1000][1]cuda:0&quot; = torch.ops.aten.cos.default(primals_1); primals_1 = None mul_1: &quot;f32[1000][1]cuda:0&quot; = torch.ops.aten.mul.Tensor(mul, cos_1); mul = cos_1 = None return (mul_1,) 2. Inductor Triton 的核心： compile() model fullgraph dynamic"},{"title":"RMSNorm","path":"/wiki/aitactics/RMSNorm.html","content":"372f7a9688aae3bb4ec6e6afab19ca439bb95e6d91164a50dda5c3f9a4f813ff Password is needed."},{"title":"LayerNorm","path":"/wiki/aitactics/layernorm.html","content":"LayerNorm 假设是在 CNN 的语境下，我们对一个 N×H×W×CN\\times H\\times W\\times CN×H×W×C 的 tensor 做 LayerNorm。 LayerNorm 做的事实际上是对于每一个 H×W×CH\\times W\\times CH×W×C 做 Normalization."},{"title":"Boruvka 算法：特殊限制下的最小生成树算法","path":"/wiki/algo/boruvka.html","content":"算法流程 初始化每一个点占据一个独立的连通块 遍历每一条边 (u,v)(u,v)(u,v)，如果其不在同一个连通块内，"},{"title":"无/有源汇 上下界 可行/最大/最小流","path":"/wiki/algo/bounded-flow.html","content":"无源汇 上下界可行流 每条边都存在下界 b(u,v),c(u,v)b(u,v),c(u,v)b(u,v),c(u,v) 分别表示这条边的流量至少、至多为多少。 我们先直接假设每条边已经流了 b(u,v)b(u,v)b(u,v) 的流量，设为初始流量 然后构造新图 HHH，其中的每一条边 eH(u,v)e_H(u,v)eH​(u,v) 满足其容量为 c(u,v)−b(u,v)c(u,v)-b(u,v)c(u,v)−b(u,v) 然后对 HHH 中的节点 iii 进行调整，假设 HHH 中两个额外的点 S,TS,TS,T 分别作为 HHH 中的源汇点 如果初始流量中 iii 的收支平衡，则不用添加边 如果 iii 的入流多于出流，差值为 ddd，则 SSS 向 iii 连边，容量为 ddd 如果 iii 的出流多于入流，差值为 ddd，则 iii 向 TTT 连边，容量为 ddd 然后以 SSS 为源点，TTT 为汇点跑最大流。 如果 SSS 出发的边都满流，则存在可行流；否则不存在 正确性证明 有源汇 上下界可行流 设源点为 SSS，汇点为 TTT，则我们连 T→ST\\to ST→S 的边，其下界为 000，上界为 ∞\\infin∞。于是问题转化为无源汇上下界可行流。 此时若有解，S→TS\\to TS→T 的可行流的流量就等于 T→ST\\to ST→S 的附加边的流量。 上下界最大流 上下界最小流"},{"title":"计算几何：凸包","path":"/wiki/algo/convex-hull.html","content":"求解凸包"},{"title":"网络流：EK 算法","path":"/wiki/algo/edmonds-karp.html","content":"EK 算法 EK 算法在 Residual Graph 里找增广路的时候，使用 BFS 算法求解出的增广路一定是 shortest (in terms of fewest edges). 算法证明 Distance Lemma 令 EF 算法第 iii 次在 GfG_fGf​ 上找到的增广路为 fif_ifi​，并且这些 fif_ifi​ 是最短的（经过最少的边）。记 GiG_{i}Gi​ 为 fif_ifi​ 对应的 residual graph，di(u,v)d_i(u,v)di​(u,v) 为 GiG_{i}Gi​ 上两点之间的最短距离（最小边数），那么有 di+1(s,v)≥di(s,v)\\boxed{d_{i+1}(s,v)\\ge d_i(s,v)} di+1​(s,v)≥di​(s,v)​ 证明 令 s,vs,vs,v 最短路径上的点按 distance 递增排列。我们用数学归纳法证明。 当 di+1(s,v)=0d_{i+1}(s,v)=0di+1​(s,v)=0 时，说明 s=vs=vs=v 两者是同一个点，因此在 fif_ifi​ 对应的图里，di(s,v)=0≤di+1(s,v)d_i(s,v)=0\\le d_{i+1}(s,v)di​(s,v)=0≤di+1​(s,v). 考虑任意长度 L&gt;0L\\gt 0L&gt;0，假设引理对 ∀v,di+1(s,v)&lt;L\\forall v, d_{i+1}(s,v)\\lt L∀v,di+1​(s,v)&lt;L 成立，考察 ∀v,di+1(s,v)=L\\forall v,d_{i+1}(s,v)=L∀v,di+1​(s,v)=L： 在 residual graph Gi+1G_{i+1}Gi+1​ 上找到 s,vs,vs,v 的最短路，令 xxx 为 vvv 的上一个节点，那么有 di+1(s,x)=L−1d_{i+1}(s,x)=L-1di+1​(s,x)=L−1 所以根据假设 di(s,x)≤L−1d_{i}(s,x)\\le L-1di​(s,x)≤L−1，我们再根据这个信息，推理 di(s,v)d_i(s,v)di​(s,v)。在 GiG_iGi​ 上有两种情况 GiG_iGi​ 中存在 (x,v)(x,v)(x,v) 这条边 这种情况下，我们只需要走 s→x→vs\\to x\\to vs→x→v，就一定可以保证 di(s,v)≤Ld_i(s,v)\\le Ldi​(s,v)≤L，所以 di(s,v)≤di+1(s,v)d_i(s,v)\\le d_{i+1}(s,v)di​(s,v)≤di+1​(s,v) GiG_iGi​ 中不存在 (x,v)(x,v)(x,v) 这条边 如果 GiG_iGi​ 不存在这条边，但 Gi+1G_{i+1}Gi+1​ 存在这条边，这就说明 (v,x)∈Gi(v,x)\\in G_i(v,x)∈Gi​ EK 算法找到了一条增广路，s→v→x→ts\\to v\\to x\\to ts→v→x→t 此时，由于 EF 算法找到的总是最短路，而 di(s,x)≤L−1d_i(s,x)\\le L-1di​(s,x)≤L−1 且经过 (v,x)(v,x)(v,x)，因此我们可以推导出 di(s,v)=di(s,x)−1≤L−2d_i(s,v)=d_i(s,x)-1\\le L-2 di​(s,v)=di​(s,x)−1≤L−2 所以 di(s,v)≤di+1(s,v)d_i(s,v)\\le d_{i+1}(s,v)di​(s,v)≤di+1​(s,v) 便自动成立了 Critical Edge Lemma"},{"title":"网络流：Ford-Fulkerson 增广路算法","path":"/wiki/algo/ford-fulkerson.html","content":"Ford Fulkerson 算法 对于边 (u,v)(u,v)(u,v)，我们定义 Residual Capacity cf(u,v)=c(u,v)−f(u,v)c_f(u,v)=c(u,v)-f(u,v)cf​(u,v)=c(u,v)−f(u,v)。把所有剩余容量 &gt;0\\gt 0&gt;0 的边构成的子图定义为 Residual Graph GfG_fGf​。 Residual Graph 能够 work 的核心在于对 Backward Edge 的理解。Forward Edge 上的增广和 Backward Edge 上的增广可以抵消。 退流 Ford-Fulkerson 增广算法的时间复杂度是 O(nmf)O(nmf)O(nmf) 的，受容量最大的边的影响。 正确性证明"},{"title":"图论、网络流：最小割树 (Gomory-Hu Tree)","path":"/wiki/algo/gomory-hu-tree.html","content":"Gomory-Hu Tree 以下分析约定： 符号 含义 valSval_SvalS​ 令 SSS 是边集，那么 c(S)c(S)c(S) 就是其边权和 cutu,v={U,V−U}cut_{u,v}=\\{U,V-U\\}cutu,v​={U,V−U} 分割 u,vu,vu,v 的最小割，其中 u∈U,v∈V−Uu\\in U, v\\in V-Uu∈U,v∈V−U edgeUedge_UedgeU​ 对于一个割 U,V−UU,V-UU,V−U，其为割下的所有边，即 {(u,v):u∈U,v∈V−U}\\{(u,v):u\\in U,v\\in V-U\\}{(u,v):u∈U,v∈V−U} mincutu,vmincut_{u,v}mincutu,v​ (u,v)(u,v)(u,v) 的最小割（权值最小） minvalu,vminval_{u,v}minvalu,v​ =val(mincut(u,v))=val(mincut(u,v))=val(mincut(u,v)) 最小割树 T=(V,ET)T=(V,E_T)T=(V,ET​) 是这样一种树，对于所有的边 (s,t)∈ET(s,t)\\in E_T(s,t)∈ET​，从树上去掉这两条边之后剩下的两个连通块 S,TS,TS,T，恰好就是 s,ts,ts,t 的最小割 mincuts,tmincut_{s,t}mincuts,t​ 代码实现 Code 1// pass 例题"},{"title":"贪心算法","path":"/wiki/algo/greedy.html","content":"如何证明贪心算法的正确性？ 证明最佳的 solution 可以通过贪心算法在不增加 cost 的情况下求出来 证明每一步选择时，贪心算法的答案都不劣于其他算法 贪心模型 任务规划"},{"title":"网络流模型","path":"/wiki/algo/network-flow-models.html","content":"二分图匹配模型"},{"title":"网络流：最大流、最小割","path":"/wiki/algo/network-flow.html","content":"Flow Network 网络流图 G=(V,E,c)G=(V,E,c)G=(V,E,c) 是一张有向图，其中每一条有向边 e=(u,v)e=(u,v)e=(u,v) 有容量 (capacity) c(u,v)≥0c(u,v)\\ge 0c(u,v)≥0. 除此之外，GGG 中还有两个特殊节点 source sss 和 sink ttt. Flow 在此基础上，我们定义流 (Flow). GGG 上的流 fff 给每一条边 e=(u,v)e=(u,v)e=(u,v) 都赋上一个实数 f(u,v)f(u,v)f(u,v) 且满足： 每一条边的流量都不会超过其容量 capacity.f(u,v)≤c(u,v)f(u,v)\\le c(u,v) f(u,v)≤c(u,v) 除了源点与汇点，其余每一点都满足：流入的流量和流出的流量相等。∀v∈V−{s,t},∑(x,v)∈Ef(x,v)=∑(v,y)∈Ef(v,y)\\forall v\\in V-\\{s,t\\}, \\sum_{(x,v)\\in E} f(x,v)=\\sum_{(v,y)\\in E} f(v,y) ∀v∈V−{s,t},(x,v)∈E∑​f(x,v)=(v,y)∈E∑​f(v,y) 最大流求解算法 Ford-Fulkerson 算法 Cut 什么是割？ 图的一个“割” (Cut) 是指将图分成两个点集 A,BA,BA,B 且源点 s∈As\\in As∈A，汇点 t∈Bt\\in Bt∈B. 定义 Capacity of Cut cap(A,B)=∑e out of Ac(e)cap(A,B)=\\sum_{e\\text{ out of }A}c(e)cap(A,B)=∑e out of A​c(e) Flow Value Lemma 令 fff 为 GGG 上任意的流，(A,B)(A,B)(A,B) 为 GGG 上任意的割（s∈A,t∈Bs\\in A,t\\in Bs∈A,t∈B），则必有 value(f)=∑e out of Af(e)−∑e into Af(e)\\text{value}(f)=\\sum_{e \\text{ out of }A}f(e)-\\sum_{e\\text{ into }A} f(e) value(f)=e out of A∑​f(e)−e into A∑​f(e) 证明 考虑 AAA 中的每一个节点，除了源点 sss 以外，其余所有点都满足 outflow(v)=inflow(v)\\text{outflow}(v)=\\text{inflow}(v)outflow(v)=inflow(v)，而 sss 的 inflow(s)=0\\text{inflow}(s)=0inflow(s)=0，所以 value(f)=∑outflow(s)=∑v∈Aoutflow(v)−inflow(v)\\mathrm{value}(f)=\\sum \\mathrm{outflow}(s)=\\sum_{v\\in A} \\mathrm{outflow}(v)-\\mathrm{inflow}(v) value(f)=∑outflow(s)=v∈A∑​outflow(v)−inflow(v) 我们再来考察 ∑v∈Aoutflow(v)\\sum_{v\\in A}\\mathrm{outflow}(v)∑v∈A​outflow(v)，检查所有边，可得 ∑v∈Aoutflow(v)=∑e inside Af(e)+∑e out of Af(e)\\sum_{v\\in A}\\mathrm{outflow}(v)=\\sum_{e\\text{ inside }A}f(e)+\\sum_{e\\text{ out of }A}f(e) v∈A∑​outflow(v)=e inside A∑​f(e)+e out of A∑​f(e) 同理对 inflow 有 ∑v∈Ainflow(v)=∑e inside Af(e)+∑e into Af(e)\\sum_{v\\in A}\\mathrm{inflow}(v)=\\sum_{e\\text{ inside }A}f(e)+\\sum_{e\\text{ into }A}f(e) v∈A∑​inflow(v)=e inside A∑​f(e)+e into A∑​f(e) 两式相减可得 value(f)=∑e out of Af(e)−∑e into Af(e)\\mathrm{value}(f)=\\sum_{e\\text{ out of }A}f(e)-\\sum_{e\\text{ into }A}f(e) value(f)=e out of A∑​f(e)−e into A∑​f(e) 由此也可以推得几个推论 Corollary 1 value(f)≤cap(A,B)\\mathrm{value}(f)\\le \\mathrm{cap}(A,B) value(f)≤cap(A,B) 证明 value(f)=∑e out of Af(e)−∑e into Af(e)≤∑e out of Af(e)≤∑e out of Acap(e)=cap(A,B)\\begin{aligned} \\mathrm{value}(f)&amp;=\\sum_{e\\text{ out of }A} f(e)-\\sum_{e\\text{ into }A} f(e)\\\\ &amp;\\le \\sum_{e\\text{ out of }A} f(e)\\\\ &amp;\\le\\sum_{e\\text{ out of }A} \\mathrm{cap}(e)\\\\ &amp;=\\mathrm{cap}(A,B) \\end{aligned} value(f)​=e out of A∑​f(e)−e into A∑​f(e)≤e out of A∑​f(e)≤e out of A∑​cap(e)=cap(A,B)​ Theorem 3 令 fff 为图上的流且使得 GfG_fGf​ 不存在增广路，那么存在一种 cut (A,B)(A,B)(A,B) 使得 value(f)=cap(A,B)\\mathrm{value}(f)=cap(A,B)value(f)=cap(A,B). 证明 既然 GfG_fGf​ 上已经不存在增广路，那么 GfG_fGf​ 天然的可以被划分为两个集合 A,BA,BA,B，其中 A=\\set{v:s\\to v},B=V-A 这也就是说，原图 GGG 中，AAA 到 BBB 的有向边的 residual capacity 均为 000，根据 Flow Value Lemma，cut(A,B)\\mathrm{cut}(A,B)cut(A,B) 就是一种符合条件的割。 最大流最小割定理 Max Flow=Min Cut\\text{Max Flow}=\\text{Min Cut} Max Flow=Min Cut"},{"title":"【博弈论】Nim 游戏","path":"/wiki/algo/nim-game.html","content":"有向图游戏（博弈图） 博弈图是一张有向图，每一个节点表示游戏的状态（例如每个堆里石子的个数），有向边表示行动（取石子导致了状态的变化）。根据定义，博弈图是有向无环图 Nim Game Graph Nim 定理"},{"title":"网络流：预流推进算法","path":"/wiki/algo/push-relabel.html","content":"代码参考 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138template &lt;typename T&gt;class FlowGraph &#123; public: struct Edge &#123; std::size_t to; T cap; // flow, capacity &#125;; std::vector&lt;int&gt; dist; std::vector&lt;std::size_t&gt; cur; std::vector&lt;std::vector&lt;std::size_t&gt;&gt; adj; std::vector&lt;Edge&gt; edges; std::size_t s, t; std::size_t vtot, etot; // ! ===== functionality ===== FlowGraph() : vtot(0), etot(0) &#123;&#125; void init(std::size_t s, std::size_t t, std::size_t vtot) &#123; this-&gt;s = s, this-&gt;t = t, this-&gt;vtot = vtot; adj.resize(vtot), dist.resize(vtot), cur.resize(vtot); &#125; void add(std::size_t u, std::size_t v, T f) &#123; adj.at(u).push_back(edges.size()); edges.emplace_back(v, f); adj.at(v).push_back(edges.size()); edges.emplace_back(u, 0); &#125;&#125;;template &lt;typename T&gt;class PushRelabel : public FlowGraph&lt;T&gt; &#123; public: std::vector&lt;T&gt; height; std::vector&lt;T&gt; excess; std::vector&lt;std::size_t&gt; gap; std::vector&lt;std::vector&lt;std::size_t&gt;&gt; bucket; T level&#123;0&#125;; static constexpr T inf = std::numeric_limits&lt;T&gt;::max(); bool push(std::size_t u) &#123; bool init = u == this-&gt;s; for (auto e : this-&gt;adj.at(u)) &#123; auto [v, cap] = this-&gt;edges.at(e); if (cap == 0 or this-&gt;height.at(v) == inf) continue; if (!init and this-&gt;height.at(u) != this-&gt;height.at(v) + 1) continue; T k = init ? cap : std::min(cap, this-&gt;excess.at(u)); if (v != this-&gt;s and v != this-&gt;t and this-&gt;excess.at(v) == 0) &#123; this-&gt;bucket.at(this-&gt;height.at(v)).push_back(v); this-&gt;level = std::max(this-&gt;level, this-&gt;height.at(v)); &#125; // push this-&gt;excess.at(u) -= k; this-&gt;excess.at(v) += k; this-&gt;edges.at(e).cap -= k; this-&gt;edges.at(e ^ 1).cap += k; if (this-&gt;excess.at(u) == 0) return false; // finish pushing &#125; return true; &#125; void relabel(std::size_t u) &#123; this-&gt;height.at(u) = inf; for (auto e : this-&gt;adj.at(u)) &#123; auto [v, cap] = this-&gt;edges.at(e); if (cap &gt; 0) this-&gt;height.at(u) = std::min(this-&gt;height.at(u), this-&gt;height.at(v)); &#125; this-&gt;height.at(u)++; if (this-&gt;height.at(u) &lt; static_cast&lt;T&gt;(this-&gt;vtot)) &#123; this-&gt;bucket.at(this-&gt;height.at(u)).push_back(u); level = std::max(level, this-&gt;height.at(u)); ++this-&gt;gap.at(this-&gt;height.at(u)); &#125; &#125; bool bfs_init() &#123; this-&gt;height.assign(this-&gt;vtot, inf); std::queue&lt;std::size_t&gt; q; q.push(this-&gt;t); this-&gt;height.at(this-&gt;t) = 0; while (!q.empty()) &#123; auto u = q.front(); q.pop(); for (auto e : this-&gt;adj.at(u)) &#123; auto [v, cap] = this-&gt;edges.at(e); auto [rv, rcap] = this-&gt;edges.at(e ^ 1); if (rcap &gt; 0 and this-&gt;height.at(v) &gt; this-&gt;height.at(u) + 1) &#123; this-&gt;height.at(v) = this-&gt;height.at(u) + 1; q.push(v); &#125; &#125; &#125; return this-&gt;height.at(this-&gt;s) != inf; &#125; std::size_t select() &#123; while (this-&gt;level &gt; -1 and this-&gt;bucket.at(this-&gt;level).size() == 0) this-&gt;level--; return this-&gt;level == -1 ? 114514 : this-&gt;bucket.at(this-&gt;level).back(); &#125; public: void init(std::size_t s, std::size_t t, std::size_t n) &#123; FlowGraph&lt;T&gt;::init(s, t, n); this-&gt;height.assign(n, inf); this-&gt;excess.assign(n, 0); this-&gt;gap.assign(n + 1, 0); this-&gt;bucket.assign(n + 1, &#123;&#125;); this-&gt;level = 0; &#125; T max_flow() &#123; if (not this-&gt;bfs_init()) return 0; this-&gt;gap.assign(this-&gt;vtot, 0); for (std::size_t i = 0; i &lt; this-&gt;vtot; i++) if (this-&gt;height.at(i) != inf) this-&gt;gap.at(this-&gt;height.at(i))++; this-&gt;height.at(this-&gt;s) = this-&gt;vtot; this-&gt;push(this-&gt;s); for (std::size_t u = select(); u != 114514; u = select()) &#123; this-&gt;bucket.at(this-&gt;level).pop_back(); if (this-&gt;push(u)) &#123; if (not --this-&gt;gap.at(this-&gt;height.at(u))) &#123; for (std::size_t i = 0; i &lt; this-&gt;vtot; i++) &#123; if (i == this-&gt;s) continue; if (this-&gt;height.at(i) &gt;= static_cast&lt;T&gt;(this-&gt;vtot + 1)) continue; if (this-&gt;height.at(i) &lt;= this-&gt;height.at(u)) continue; this-&gt;height.at(i) = static_cast&lt;T&gt;(this-&gt;vtot + 1); &#125; &#125; this-&gt;relabel(u); &#125; &#125; return this-&gt;excess.at(this-&gt;t); &#125;&#125;;"},{"title":"2023 ICPC World Final Luxor","path":"/wiki/algo_contests/2023-icpc-wf-luxor.html","content":"A. D. Carl’s Vacation 可以联想到将军饮马模型。我们把三维的金字塔侧面展平到二维上，那么答案的最短路径就可以表达为 tip1→foot1→foot2→top2tip_1\\to foot_1\\to foot_2\\to top_2 tip1​→foot1​→foot2​→top2​ 于是，我们可以枚举每个金字塔的四个侧面，共 4×4=164\\times 4=164×4=16 种情况，在每种情况里求最短路径即可。 接下来考虑如何求这个最短路径。我们肯定需要找到两个 footfootfoot 的坐标。考虑用向量的模长表示线段长度，以及将两个 footfootfoot 的定比分点作为变量的话，那么路径长度 f(k1,k2)f(k_1,k_2)f(k1​,k2​) 分别关于 k1,k2k_1,k_2k1​,k2​ 是单峰函数，所以可以三分套三分。 小细节：浮点数三分或者二分的话，可以指定二分次数，而非 l,rl,rl,r 相差 eps\\texttt{eps}eps，后者容易出现浮点误差。 Code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#include &quot;headers/geometry.hpp&quot;#include &lt;iostream&gt;using namespace Geo2D;using namespace std;Point p1[4], p2[4], tip1, tip2;Decimal h1, h2, d1, d2, len1, len2;Decimal phi = 0.618, cphi = -phi + 1;int main() &#123; cin &gt;&gt; p1[0] &gt;&gt; p1[1] &gt;&gt; h1; cin &gt;&gt; p2[0] &gt;&gt; p2[1] &gt;&gt; h2; for (int i = 2; i &lt; 4; i++) &#123; p1[i] = p1[i - 1] + (p1[i - 1] - p1[i - 2]).Perp(); p2[i] = p2[i - 1] + (p2[i - 1] - p2[i - 2]).Perp(); &#125; tip1 = (p1[0] + p1[2]) / 2; tip2 = (p2[0] + p2[2]) / 2; len1 = p1[0].Distance(p1[1]); len2 = p2[0].Distance(p2[1]); d1 = (len1.sqr() / 4 + h1.sqr()).sqrt(); d2 = (len2.sqr() / 4 + h2.sqr()).sqrt(); Decimal ans = 2e18; for (int i = 0; i &lt; 4; i++) &#123; Vector v1 = p1[(i + 1) % 4] - p1[i]; Point midp1 = (p1[i] + p1[(i + 1) % 4]) / 2; Point pt1 = midp1 + v1.Normal() * d1; for (int j = 0; j &lt; 4; j++) &#123; Vector v2 = p2[(j + 1) % 4] - p2[j]; Point midp2 = (p2[j] + p2[(j + 1) % 4]) / 2; Point pt2 = midp2 + v2.Normal() * d2; Decimal precent_l1 = 0; Decimal precent_r1 = 1; auto findfoot1 = [&amp;](Decimal precent_mid1) -&gt; Decimal &#123; Point foot1 = p1[i] + v1 * precent_mid1; Decimal precent_l2 = 0; Decimal precent_r2 = 1; auto findfoot2 = [&amp;](Decimal precent_mid2) -&gt; Decimal &#123; Point foot2 = p2[j] + v2 * precent_mid2; return foot1.Distance(foot2) + foot1.Distance(pt1) + foot2.Distance(pt2); &#125;; for (int __ = 1; __ &lt;= 100; __++) &#123; Decimal l2 = precent_l2 * phi + precent_r2 * cphi; Decimal r2 = precent_l2 * cphi + precent_r2 * phi; if (findfoot2(l2) &gt; findfoot2(r2)) precent_l2 = l2; else precent_r2 = r2; &#125; return findfoot2(precent_l2); &#125;; for (int _ = 1; _ &lt;= 100; _++) &#123; Decimal l1 = precent_l1 * phi + precent_r1 * cphi; Decimal r1 = precent_l1 * cphi + precent_r1 * phi; if (findfoot1(l1) &gt; findfoot1(r1)) precent_l1 = l1; else precent_r1 = r1; &#125; ans = min(ans, findfoot1(precent_l1)); &#125; &#125; std::cout &lt;&lt; ans &lt;&lt; &#x27; &#x27;;&#125;"},{"title":"2024-2025 ICPC 欧洲西北部区域赛 (NWERC 2024)","path":"/wiki/algo_contests/2024-icpc-nwerc.html","content":"A. Alphabetical Aristocrats very ez."},{"title":"2024 ICPC 区域赛：香港","path":"/wiki/algo_contests/2024-icpc-regional-hk.html","content":"E. Concave Hull 算法流程 先算一次凹包，把所有的点分成“在凸包上”和“不在凸包上”的点 SSS。 枚举 SSS 中的每一个点作为凹点 p0p_0p0​，然后对其他所有点 pip_ipi​ 计算出向量 vi=pip0→v_i=\\overrightarrow{p_ip_0}vi​=pi​p0​​ 并按极角排序。 极角排序完了之后，向量必定是 on, /, /, /, on, /, /, on, on, /, /, on ...... 这样排列（两个在凸包上的点 c1,c2c_1,c_2c1​,c2​ 中间夹着一些不在凸包上的点 djd_jdj​，记这些点的集合为 F={F0=c1,F1=d1,d2,…,Fm=dm,Fm+1=c2}F=\\{F_0=c_1,F_1=d_1,d_2,\\dots,F_m=d_m,F_{m+1}=c_2\\}F={F0​=c1​,F1​=d1​,d2​,…,Fm​=dm​,Fm+1​=c2​}）。我们尝试计算以 p0p_0p0​ 作为凹点，Fi,Fi+1F_i,F_{i+1}Fi​,Fi+1​ 作为优角的两边的凹包面积。 这里的话，如果跑暴力算法，时间复杂度会来到 O(n3)O(n^3)O(n3)。考虑到这个凹包的面积其实是凸包面积去掉一部分面积，我们可以利用这一点加速计算。 两个三角形 我们把凹包凹进去的部分分成左右两半凸壳（图中黄色和绿色部分），做两次 Andrew 凸包扫描算法（从左往右，然后从右往左）。例如 Andrew 算法从左往右扫描，只要扫描算法扫描经过这些点 FFF，那么我们就能算出由 c1→Fic_1\\to F_ic1​→Fi​ 这些点构成（且包括了 FiF_iFi​ 的）的左半凸壳的面积。同理也可以计算出右半凸壳的面积。因此以 p0p_0p0​ 为凹点、Fi,Fi+1F_i,F_{i+1}Fi​,Fi+1​ 为优角的凹包面积也就可以算出来了（大凸包面积，减掉这一个凹角对应的凸包上三角形的面积 ΔADG\\Delta ADGΔADG，再加上左右两个凸壳的面积） Code Code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177#include &lt;algorithm&gt;#include &lt;cassert&gt;#include &lt;cmath&gt;#include &lt;iostream&gt;#include &lt;ranges&gt;#include &lt;utility&gt;#include &lt;set&gt;#include &lt;vector&gt;using i64 = long long;constexpr i64 M = 1e9 + 7;struct pvec &#123; int x, y; friend std::istream &amp;operator&gt;&gt;(std::istream &amp;is, pvec &amp;a) &#123; return is &gt;&gt; a.x &gt;&gt; a.y; &#125; friend std::ostream &amp;operator&lt;&lt;(std::ostream &amp;os, const pvec &amp;a) &#123; return os &lt;&lt; a.x &lt;&lt; &#x27; &#x27; &lt;&lt; a.y; &#125; bool operator==(const pvec &amp;a) const &#123; return x == a.x &amp;&amp; y == a.y; &#125; pvec operator-(const pvec &amp;a) const &#123; return &#123;x - a.x, y - a.y&#125;; &#125; bool operator&lt;(const pvec &amp;a) const &#123; return x == a.x ? y &lt; a.y : x &lt; a.x; &#125;&#125;;int sign(i64 val) &#123; return val &lt; 0 ? -1 : (val &gt; 0 ? 1 : 0); &#125;i64 cross(const pvec &amp;a, const pvec &amp;b) &#123; return 1ll * a.x * b.y - 1ll * a.y * b.x; &#125;i64 cross(const pvec &amp;a, const pvec &amp;b, const pvec &amp;c) &#123; return cross(b - a, c - a); &#125;i64 dot(const pvec &amp;a, const pvec &amp;b) &#123; return 1ll * a.x * b.x + 1ll * a.y * b.y; &#125;int scross(const pvec &amp;a, const pvec &amp;b, const pvec &amp;c) &#123; return sign(cross(a, b, c)); &#125;i64 sqrlen(const pvec &amp;a) &#123; return dot(a, a); &#125;auto convex_hull(const std::vector&lt;pvec&gt; &amp;x) &#123; std::vector&lt;pvec&gt; v(x); std::vector&lt;pvec&gt; used, unused; std::sort(v.begin(), v.end()); int m = v.size(), tp = -1; for (int i = 0; i &lt; m; i++) &#123; while (tp &gt; 0 &amp;&amp; cross(used[tp-1], used[tp], v[i]) &lt;= 0) tp--, used.pop_back(); used.push_back(v[i]), tp++; &#125; int t = tp; for (int i = m - 1; i &gt;= 0; i--) &#123; while (tp &gt; t &amp;&amp; cross(used[tp-1], used[tp], v[i]) &lt;= 0) &#123; tp--; used.pop_back(); &#125; used.push_back(v[i]), tp++; &#125; used.pop_back(); std::set&lt;pvec&gt; s; for(auto d: used) s.insert(d); for(auto d: v) if (!s.contains(d)) unused.push_back(d); return std::pair&#123;used, unused&#125;;&#125;bool comp(const pvec &amp;a, const pvec &amp;b) &#123; bool upA = a.y &gt; 0 || (a.y == 0 &amp;&amp; a.x &gt;= 0); bool upB = b.y &gt; 0 || (b.y == 0 &amp;&amp; b.x &gt;= 0); if (upA != upB) return upA; auto val = cross(a, b); return val &gt; 0;&#125;i64 area(const std::vector&lt;pvec&gt; &amp;p) &#123; i64 res = 0; for (int i = 0, m = p.size(); i &lt; m; i++) res += cross(p[i], p[(i + 1) % m]); return res;&#125;void run() &#123; int n; std::cin &gt;&gt; n; std::vector&lt;pvec&gt; p(n); for (auto &amp;x : p) std::cin &gt;&gt; x; auto [used, un] = convex_hull(p); int sz = used.size(); i64 ans = 0; for (const auto &amp;x : un) &#123; // enum concave point. std::vector&lt;std::pair&lt;pvec, int&gt;&gt; al; std::vector&lt;pvec&gt; cur(sz); std::vector&lt;i64&gt; val(sz, 0); // area of triangle formed by on-convex points i64 sum = 0; for (const auto &amp;y : un) &#123; // compute vectors if (y == x) continue; al.push_back(&#123;y - x, -1&#125;); &#125; for (int i = 0; i &lt; sz; i++) &#123; cur[i] = used[i] - x; al.push_back(&#123;cur[i], i&#125;); &#125; // sort by angle std::sort(al.begin(), al.end(), [&amp;](const auto &amp;a, const auto &amp;b) &#123; return comp(a.first, b.first); &#125;); // rotate to satisfy pattern: // [on-convex, not, not, ..., not, on-convex, not, not .... , not, on-convex] for (int i = 0; i &lt; al.size(); i++) &#123; if (al[i].second == -1) continue; std::rotate(al.begin(), al.begin() + i, al.end()); break; &#125; // compute convex area for (int i = 0; i &lt; sz; i++) &#123; val[i] = cross(cur[i], cur[(i + 1) % sz]); sum += val[i]; &#125; // enum all points between 2 on-convex points for (int l = 0, r = 0, al_size = al.size(); l &lt; al_size; l = r) &#123; r = l + 1; while (r &lt; al_size &amp;&amp; al[r].second == -1) r++; // (l, r) is the range of not-on-convex points // l, r are on-convex points int pos = al[l].second; std::vector&lt;i64&gt; T(r - l, 0); assert((pos + 1) % sz == al[r % al_size].second); // left convex [&amp;al, &amp;T, &amp;l, &amp;r] &#123; std::vector&lt;pvec&gt; pts; int top = -1; i64 ssum = 0; for (int fix = l; fix &lt; r; fix++) &#123; const auto &amp;q = al[fix].first; while (top &gt;= 1 &amp;&amp; cross(q - pts[top - 1], pts[top] - pts[top - 1]) &gt;= 0) &#123; ssum -= cross(pts[top - 1], pts[top]); top--, pts.pop_back(); &#125; pts.push_back(q), top++; if (top &gt;= 1) ssum += cross(pts[top - 1], pts[top]); T[fix - l] += ssum; &#125; &#125;(); // right convex [&amp;al, &amp;T, &amp;l, &amp;r, &amp;al_size] &#123; std::vector&lt;pvec&gt; pts; int top = -1; i64 ssum = 0; for (int fix = r; fix &gt; l; fix--) &#123; const auto &amp;q = al[fix % al_size].first; while (top &gt;= 1 &amp;&amp; cross(pts[top - 1], pts[top], q) &gt;= 0) &#123; ssum -= cross(pts[top], pts[top - 1]); top--, pts.pop_back(); &#125; pts.push_back(q); top++; if (top &gt;= 1) ssum += cross(pts[top], pts[top - 1]); T[fix - l - 1] += ssum; &#125; &#125;(); for (int i = 0; i &lt; r - l; i++) &#123; i64 st = sum - val[pos] + T[i]; (ans += std::abs(st)) %= M; &#125; &#125; &#125; std::cout &lt;&lt; ans &lt;&lt; &#x27; &#x27;;&#125;int main() &#123; std::ios::sync_with_stdio(false); std::cin.tie(nullptr); std::cout.tie(nullptr); int T = 1; // std::cin &gt;&gt; T; while (T--) run(); return 0;&#125; H. Mah-Jong 算法流程 我们先预处理出所有可能的顺子的情况 SSS，每个顺子最多出现 222 次（否则可以视为 333 个碰），最少出现 000 次，因此最多 729729729 种情况。 于是每一个合法的区间都可以视为，SSS 中的某一个顺子搭配 sss 加上 若干个碰，因此对于区间 [l,r][l, r][l,r] 而言，令这个区间有 did_idi​ 个数字为 iii 的麻将牌，而顺子组合 sss 要求 bib_ibi​ 个数字为 iii 的牌，那么区间合法这个条件等同于 di≡bi(mod3)di≥bi\\begin{aligned} d_i&amp;\\equiv b_i \\pmod 3\\\\ d_i&amp;\\ge b_i \\end{aligned} di​di​​≡bi​(mod3)≥bi​​ 考虑如何维护 di≥bid_i\\ge b_idi​≥bi​ 这个条件： 如果我们固定右端点 rrr，那么我们只需要让 l:r→1l:r\\to 1l:r→1 扫描，直到 [l,r][l,r][l,r] 的 did_idi​ 开始满足 di≥bid_i\\ge b_idi​≥bi​，那么对于所有 p&lt;lp\\lt lp&lt;l，都一定会有 [p,r]:di≥bi[p,r]:d_i\\ge b_i[p,r]:di​≥bi​（因为 p&lt;lp\\lt lp&lt;l，因此只会往这个区间里添加新的数，因此 did_idi​ 不可能变小） 考虑如何维护同余 di≡bi(mod3)d_i\\equiv b_i\\pmod {3}di​≡bi​(mod3) 用桶维护即可。可以开 888 维数组或者用三进制表示 时间复杂度 O(36n)O(3^6n)O(36n) Code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#include &lt;iostream&gt;#include &lt;ranges&gt;#include &lt;vector&gt;using i64 = long long;void run() &#123; int n; std::cin &gt;&gt; n; std::vector cnt(n + 1, std::vector&lt;int&gt;(8, 0)); std::vector&lt;int&gt; mask(n + 1, 0); std::vector&lt;int&gt; a(n + 1, 0); auto encode = [&amp;](int d) &#123; int s = 0; for (auto i : std::views::iota(0, 8)) s = s * 3 + cnt.at(d).at(i) % 3; return s; &#125;; auto decode_chow = [&amp;](int pat) &#123; std::vector&lt;int&gt; p(8, 0); for (int i : std::views::iota(0, 6)) &#123; int u = pat % 3; pat /= 3; p.at(i) += u, p.at(i + 1) += u, p.at(i + 2) += u; &#125; return p; &#125;; for (int i = 1; i &lt;= n; i++) &#123; std::cin &gt;&gt; a.at(i); a.at(i)--; cnt.at(i) = cnt.at(i - 1); cnt.at(i).at(a.at(i))++; mask.at(i) = encode(i); &#125; i64 ans = 0; std::vector&lt;int&gt; bucket(8000, 0); for (auto pattern : std::views::iota(0, 729)) &#123; auto pat = decode_chow(pattern); for (int i = 0; i &lt;= n; i++) bucket.at(mask.at(i))++; int r = 0; for (int l = 1; l &lt;= n; l++) &#123; for (; r &lt;= l; r++) bucket.at(mask.at(r))--; // 先去除不合法的区间（即 右端点小于左端点的区间） int target = 0; for (int t : std::views::iota(0, 8)) &#123; // 用双指针去除不满足偏序关系的 while (r &lt;= n &amp;&amp; cnt.at(r).at(t) &lt; cnt.at(l - 1).at(t) + pat.at(t)) &#123; bucket.at(mask.at(r))--; r++; &#125; target = target * 3 + (cnt.at(l - 1).at(t) + pat.at(t)) % 3; &#125; if (r &gt; n) break; ans += bucket[target]; // 加上满足同余的区间的贡献 &#125; &#125; std::cout &lt;&lt; ans &lt;&lt; &#x27; &#x27;;&#125;int main() &#123; std::ios::sync_with_stdio(false); std::cin.tie(nullptr); std::cout.tie(nullptr); int T = 1; std::cin &gt;&gt; T; while (T--) run(); return 0;&#125;"},{"title":"2025 杭电多校春季赛 4","path":"/wiki/algo_contests/2025-hdu-spring-04.html","content":"持家 考虑打 aaa 折，减 bbb 元，原价 PPP 元，则有 (P−b)×a=Pa−ba&lt;Pa−b(P-b)\\times a=Pa-ba \\lt Pa-b (P−b)×a=Pa−ba&lt;Pa−b 所以我们应该总是先用打折券，再用减价券。时间复杂度为排序的 O(nlog⁡n)O(n\\log n)O(nlogn) Code 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;algorithm&gt;#include &lt;iomanip&gt;#include &lt;iostream&gt;#include &lt;vector&gt;using i64 = long long;void run() &#123; int P, n, k; std::cin &gt;&gt; P &gt;&gt; n &gt;&gt; k; std::vector&lt;double&gt; a, b; for (int i = 0, c, t; i &lt; n; i++) &#123; std::cin &gt;&gt; t &gt;&gt; c; if (t == 0) a.push_back(c * 1.0 / 10); else b.push_back(c); &#125; std::sort(a.begin(), a.end()), a.insert(a.begin(), 1); std::sort(b.begin(), b.end(), std::greater()), b.insert(b.begin(), 0); for (int i = 1; i &lt; a.size(); i++) a[i] *= a[i - 1]; for (int i = 1; i &lt; b.size(); i++) b[i] += b[i - 1]; double ans = P; for (int i = 0; i &lt;= k; i++) if (i &lt; a.size()) ans = std::min(ans, a[i] * P - b[std::min(k - i, int(b.size()) - 1)]); std::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(2) &lt;&lt; std::max(ans, 0.0) &lt;&lt; &#x27; &#x27;;&#125;int main() &#123; std::ios::sync_with_stdio(false); std::cin.tie(nullptr); std::cout.tie(nullptr); int T = 1; std::cin &gt;&gt; T; while (T--) run(); return 0;&#125;"},{"title":"香港中文大学（深圳）2025 校赛暨粤港澳国际编程比赛","path":"/wiki/algo_contests/20250419-CUHKSZ-contest.html","content":"A. Milky Loong 大致题意：给你一个字符串，提取一部分文字出来和另一部分拼一起。 实际也非常简单，用 Python 随便水一水就可以了。毕竟是照顾初学者的签到题 B. 约瑟夫问题 大致题意： 有 n≤105n\\le 10^5n≤105 个人围成一个圆，给定一个 2≤k≤92\\le k\\le 92≤k≤9，每个人轮流报数： 如果这个数是 kkk 的倍数、或者其十进制表示带有 kkk 这个数字，那么这个人就被杀死 报数的时候会跳过已经死掉的人 问最后活下来的是谁。 也毕竟简单。考虑到最多每 kkk 个数字就会干掉一个人，因此最多 nknknk 轮就会结束。 C. F. 试飞 大致题意： nnn 个人里面有 mmm 个人具有飞行经验，你的目标是选出两个有飞行经验的人试飞。 你每次可以选择任意 222 个人让其试飞，如果这两个人都具有飞行经验，则任务立刻结束；否则试飞失败。 你只能用至多 ⌊n2m⌋\\lfloor \\frac{n^2}{m}\\rfloor⌊mn2​⌋ 次试飞完成目标。 非常有意思的一道题目。考察鸽巢原理。具体做法是，把这 nnn 个人平分到 m−1m-1m−1 个组里，那么根据鸽巢原理，必然有一个组里有 222 个人具有飞行经验。 于是，我们直接对每一个组暴力枚举 pair. 由于是平分，每个组差不多 nm−1\\frac{n}{m-1}m−1n​ 人，因此一个组内的枚举次数为 12×(nm−1−1)×nm−1\\frac{1}{2}\\times(\\frac{n}{m-1}-1)\\times\\frac{n}{m-1}21​×(m−1n​−1)×m−1n​，而有 m−1m-1m−1 组，因此总枚举次数为 (m−1)×12×(nm−1−1)×nm−1=n22(m−1)−n≤n2m\\begin{aligned} &amp;(m-1)\\times\\frac{1}{2}\\times(\\frac{n}{m-1}-1)\\times\\frac{n}{m-1}\\\\ =&amp;\\frac{n^2}{2(m-1)}-n\\\\ \\le &amp;\\frac{n^2}{m} \\end{aligned} =≤​(m−1)×21​×(m−1n​−1)×m−1n​2(m−1)n2​−nmn2​​"},{"title":"使用 Dify 的“节点”完成数据预处理","path":"/wiki/dify/custom-data-preprocess.html","content":"372f7a9688aae3bb4ec6e6afab19ca439bb95e6d91164a50dda5c3f9a4f813ff Password is needed."},{"title":"Ascend C 介绍与 NPU 芯片","path":"/wiki/ascendc/intro.html","content":"d0144f89de14a1fc384914230d36d0ec0054caf00183a222885cc88f31848a18aab140cd588bb1a31357d2dfedf6c8adfb592b49da5961aaa495123a3ccb3fa9a9f6c36e1bc55e6ff161822b769668a2076f5cfab45f196e1c53f0376e4956630c8a0923bfd91ca220c8d6a51d78be71e5b15538793255c1fcf3fbc64b80ef22508f3168feccbdaffac7faff9ec41aa90bd14105244569a56fb953d126552fc91d226fb4aa73c720ffee5e27a6e6c44331325d651b1f74c98b14d272388278d75502dfd76dee11cf4ae7c17bae192f36d0fc6097e31014e778d3ce734f793cc91d6a0cd8deee525e0a2b5df1feb19690d6cb0ed574e528834c7497615774b37a7d6a622e4a391fda0f6d046ee84fabcc6866a2fa790d0da49f405d25262fe0b227284612f2bbedafb19c35fddce7dbee9ea231f1128845e2cb4d3acb14a798c9445debfe7149b4028652944b99b85e6edd553e3657d323986791286e06611026a0b53564e9d42cb8d5930b665780169dd1f70bcc60ea6a108e7508f6b4bb904766b050f2d77edb2d92da12da5e5c4e226216b6aebe1366030bb436121c7a9f88e091b50f1553e5c3fbf3e08989b7c17a3a39de4e90112e198843ee98d7250d6cd7aa6775e73d8e3f2c10e535f76a20fb37d456d87f6666f252b1711bb852ba8ebdb2569e786dc13560eadb4ae4c42e8b5d14abef39d4ba6c5316bef5bce30910aa0dbe3bd454a7fdf5dbd7c47c034aa4008e8976343eca32e87ba156dc597cb588f4606b79ed83933468023f298250221b45caa3fbc371929526c7172ba395c9ece27b8ece80706195af6c0e7022c6f17e2b0cdd6b68406ce32e43d5bd7bc28fba6d76fe865bc8d0418149ca493a51097a7de299e67e71170344738fcfd519ce277e2a286cff0c58e2a44ea50dddf92fa061b6360d6804ef591820ebe7ead7532e594b1b2328f6deb4d87aa44f3b2c206b04e743f4062a0f3e97c1b4864b8c6d7906763512c3f64396b213ab89caf45d38c0f50f1f5f93fd38816ec6bebbca3364766f2a26cda6379849f8a480114accbaeb749a9dd5b9f6cbee7c31fcb51ff6fcf80c879e9eaa5f05c522fbc29d55095152d5c8e5f7745956c602d18cc1637d57c82ea950aad8409b557346ad1245f228a4492cdfd9274376eadf8dc24f39ecd4594177677dd558ea8a857c7beb585e8d2d780bb29be3591827a4768e7f03b285aa795a4f294b5af5c6474303a17193a9be731f2d6453374ecc65ce9bbeb3cb7eccac9708994ea7be1632ecf6be3094 Password is needed."},{"title":"北京大学 2024 年《数据结构与算法A（实验班）》期末考试","path":"/wiki/algo_contests/pku-2024-ds-and-algo-A-final.html","content":"D. MST 算法流程 先对 “(边权, ID)” 进行排序（从小到大） 初始化…… 并查集，两颗线段树（一棵维护从左向右的字符串哈希值，另一棵维护从右向左的字符串哈希值） 给并查集上的每一个连通块分配一个随机数（nnn 个，初始时并查集都不连通） 初始化 vector，记录一下每一个连通块内的所有点 遍历排序过的边…… (w,id)(w,id)(w,id) 首先，计算一下 ididid 对应的边集，(1,id−1),(2,id−2),…,(id−12,id−id−12)(1,id-1), (2,id-2), \\dots, (\\frac{id-1}{2}, id-\\frac{id-1}{2})(1,id−1),(2,id−2),…,(2id−1​,id−2id−1​) 从 l=id−12,r=id−id−12l=\\frac{id-1}{2},r=id-\\frac{id-1}{2}l=2id−1​,r=id−2id−1​ 开始向外二分查找下一条要连接的边 (l−M,r+M)(l-M, r+M)(l−M,r+M) 在并查集上连接这一条边对应的两个点 l−M,r+Ml-M,r+Ml−M,r+M， 同时维护 vector（更新同一个连通块内的点，这里需要用启发式合并）。 维护 vector 的同时，在线段树上同步修改对应点的值（修改成新连通块的值），即同步维护字符串的哈希值。 每合并两个点，就把边权加入答案 最后输出答案 正确性证明 考虑根据 Kruskal 算法的思路，我们总是尝试从边权最小的边 e=(u,v)e=(u,v)e=(u,v) 开始尝试加入 MST，如果 (u,v)(u,v)(u,v) 在并查集里不连通，那么就说明这条边可以加入 MST. 现在的话，边都是以 ai+ja_{i+j}ai+j​ 的形式给出，例如对于 aka_kak​，它所代表的边为 (1,k−1),(2,k−2),…(1,k-1),(2,k-2),\\dots(1,k−1),(2,k−2),…。 如果我们给并查集里的每一个连通块分配一个字母，那么考虑并查集里的两个点 i,ji,ji,j 且我们正在考虑 aka_kak​ 满足 k=i+jk=i+jk=i+j：那么我们可以发现的一点是，如果 i,ji,ji,j 已经连通，那么他们的字母应该是相同的，否则就不相同。如果字母相同，我们就不需要在 (i,j)(i,j)(i,j) 之间连边，因为他们已经在同一个连通块里（根据 Kruskal 算法，加入 (i,j)(i,j)(i,j) 这条边会产生一个环）；否则我们就可以加入这条边，在并查集里把他们连起来。 然后我们就可以发现一件事：如果对每一条 aka_kak​ 对应的边 (i,j)(i,j)(i,j) 来说，都不需要向 MST 里添加这条边，这意味着 i,ji,ji,j 对应的值相等；而 i+j=ki+j=ki+j=k，因此总是有 val[i]=val[k−i]val[i]=val[k-i]val[i]=val[k−i]，也就是说 aka_kak​ 所对应的点 1…k−11\\dots k-11…k−1 是回文串！ 这意味着我们可以通过不断判断某一段前后缀是否回文，来看这一段前后缀是不是已经在并查集（也即 MST）上连接完毕。我们可以用二分快速进行查找和判断。 时间复杂度分析 根据 MST 的性质，MST 是一棵树，最多 n−1n-1n−1 条边，而因为每一次二分必将连接一条边，因此“二分枚举待连接的边”这个操作最多进行 n−1n-1n−1 次，即 O(n)O(n)O(n) 次。每一次二分最多在包含 nnn 条边的集合内搜索，因此二分次数是 O(nlog⁡n)O(n\\log n)O(nlogn) 的。每一次二分都需要在线段树上进行区间查询，而单次区间查询是 O(log⁡n)O(\\log n)O(logn) 的，所有二分部分的时间复杂度是 O(nlog⁡2n)O(n\\log^2 n)O(nlog2n) 接着考虑启发式合并部分的复杂度。由于每一次都将小的集合添加到大的集合里，因此每一次合并，被添加的元素所在的集合大小都会翻倍，由于最多有 nnn 个点，因此最坏情况下一个元素会被添加 log⁡n\\log nlogn 次，因此启发式合并一共会产生 O(nlog⁡n)O(n\\log n)O(nlogn) 次添加元素操作。 但是在每次添加元素的时候，我们还要维护线段树，对单点修改、区间查询线段树而言，修改一次的复杂度是 O(log⁡n)O(\\log n)O(logn)，因此启发式合并以及维护线段树的总体复杂度就是 O(nlog⁡2n)O(n\\log^2 n)O(nlog2n) 因此总体时间复杂度为 O(nlog⁡2n)O(n\\log^2n)O(nlog2n) Code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#include &lt;algorithm&gt;#include &lt;cassert&gt;#include &lt;functional&gt;#include &lt;iostream&gt;#include &lt;numeric&gt;#include &lt;random&gt;#include &lt;utility&gt;#include &lt;vector&gt;using u64 = unsigned long long;constexpr u64 P = 4816069;std::mt19937_64 rng(std::random_device&#123;&#125;());std::vector&lt;u64&gt; base, rd;struct ModTree &#123; struct Node &#123; int l, r; u64 fhash, bhash; &#125;; std::vector&lt;Node&gt; tree; int n; Node update(Node l, Node r) &#123; Node res; res.l = l.l, res.r = r.r; res.fhash = l.fhash * base[r.r - r.l + 1] + r.fhash; res.bhash = r.bhash * base[l.r - l.l + 1] + l.bhash; return res; &#125; void init(int n) &#123; tree.assign(n * 4, Node()); this-&gt;n = n; &#125; void build(std::vector&lt;u64&gt; &amp;vec, int p, int l, int r) &#123; tree[p].l = l, tree[p].r = r; if (l == r) &#123; tree[p].fhash = tree[p].bhash = vec[l]; return; &#125; int mid = (l + r) &gt;&gt; 1; build(vec, p &lt;&lt; 1, l, mid); build(vec, p &lt;&lt; 1 | 1, mid + 1, r); tree[p] = update(tree[p &lt;&lt; 1], tree[p &lt;&lt; 1 | 1]); &#125; void modify(int p, int pos, u64 val) &#123; if (tree[p].l == tree[p].r) &#123; tree[p].fhash = tree[p].bhash = val; return; &#125; int mid = (tree[p].l + tree[p].r) &gt;&gt; 1; if (pos &lt;= mid) modify(p &lt;&lt; 1, pos, val); else modify(p &lt;&lt; 1 | 1, pos, val); tree[p] = update(tree[p &lt;&lt; 1], tree[p &lt;&lt; 1 | 1]); &#125; Node query(int p, int l, int r) &#123; if (l &lt;= tree[p].l &amp;&amp; tree[p].r &lt;= r) return tree[p]; int mid = (tree[p].l + tree[p].r) &gt;&gt; 1; if (r &lt;= mid) return query(p &lt;&lt; 1, l, r); else if (l &gt; mid) return query(p &lt;&lt; 1 | 1, l, r); Node res = update(query(p &lt;&lt; 1, l, mid), query(p &lt;&lt; 1 | 1, mid + 1, r)); return res; &#125;&#125;;int main() &#123; std::ios::sync_with_stdio(false); std::cin.tie(nullptr); std::cout.tie(nullptr); int n; std::cin &gt;&gt; n; [&amp;] &#123; base.resize(n + 1), rd.resize(n + 1); base[0] = 1; for (int i = 1; i &lt;= n; i++) base[i] = base[i - 1] * P, rd[i] = rng(); &#125;(); std::vector&lt;std::pair&lt;u64, int&gt;&gt; vec; std::vector&lt;std::vector&lt;int&gt;&gt; nodes(n + 1); for (int i = 3; i &lt; n * 2; i++) &#123; u64 x; std::cin &gt;&gt; x; vec.emplace_back(x, i); &#125; std::vector&lt;int&gt; fa(n + 1, 0); std::iota(fa.begin(), fa.end(), 0); std::function&lt;int(int)&gt; find = [&amp;](int x) &#123; return fa[x] == x ? x : fa[x] = find(fa[x]); &#125;; ModTree tree; tree.init(n); tree.build(rd, 1, 1, n); for (int i = 1; i &lt;= n; i++) nodes[i].push_back(i); auto unite = [&amp;](int u, int v) &#123; u = find(u), v = find(v); if (u == v) return; if (nodes[u].size() &gt; nodes[v].size()) std::swap(u, v); fa[u] = v; for (int i : nodes[u]) &#123; nodes[v].emplace_back(i); rd[i] = rd[v]; tree.modify(1, i, rd[i]); &#125; nodes[u].clear(); &#125;; std::sort(vec.begin(), vec.end()); long long ans = 0; for (auto &amp;[E, id] : vec) &#123; int l = (id - 1) / 2, r = id - l; int size = std::min(l, n - r + 1); int L = l - size + 1, R = r + size - 1; while (r &lt;= R) &#123; // std::cerr &lt;&lt; l &lt;&lt; &#x27; &#x27; &lt;&lt; r &lt;&lt; &#x27; &#x27; &lt;&lt; L &lt;&lt; &#x27; &#x27; &lt;&lt; R &lt;&lt; &#x27; &#x27;; // std::cerr &lt;&lt; tree.query(1, L, l).fhash &lt;&lt; &#x27; &#x27; &lt;&lt; tree.query(1, r, R).bhash &lt;&lt; &#x27; &#x27;; // for (int i = 1; i &lt;= n; i++) std::cerr &lt;&lt; i &lt;&lt; &quot;: &quot; &lt;&lt; rd[i] &lt;&lt; &#x27; &#x27;; if (tree.query(1, L, l).fhash == tree.query(1, r, R).bhash) break; int lb = 0, ub = R - r; while (lb &lt;= ub) &#123; int mid = (lb + ub) &gt;&gt; 1; if (tree.query(1, l - mid, l).fhash != tree.query(1, r, r + mid).bhash) ub = mid - 1; else lb = mid + 1; &#125; l -= lb, r += lb; unite(l, r); ans += E; &#125; &#125; std::cout &lt;&lt; ans &lt;&lt; &#x27; &#x27;; return 0;&#125;"},{"title":"CAPM 资本资产定价模型","path":"/wiki/fina/CAPM.html","content":"公式 E[Rp]=Rf+βp(RM−Rf)\\mathbb E[R_{p}]=R_f+\\beta_{p}(R_M-R_f) E[Rp​]=Rf​+βp​(RM​−Rf​) 市场风险溢价投资者购买股票会承担 Systematic Risk，并且这种 Risk 无法通过分散投资解除。因此投资者会要求股票提供更多的回报，来“补偿”他们承担的 Risk。 也可以理解为交易：企业通过发行股票得到融资，投资者当下付出了钱、而且还承担了风险（股票带来的系统性风险），但是作为交换，得到了股票的回报。即企业得到的融资=投资者的回报+投资者持有的股票。"},{"title":"安装与部署 Dify","path":"/wiki/dify/install-n-use.html","content":"下载 Dify 的 docker compose 文件 1git clone 启动 docker 确保 docker 可以共享文件夹 在 docker desktop -&gt; settings -&gt; resource share 里查看 folder 是否存在。 进入本地部署的 Dify 使用 ip a 查看 docker0 对应的 IP 地址（inet），然后用这个 IP 进入，而不是 localhost"},{"title":"IRR","path":"/wiki/fina/IRR.html","content":"Definition The IRR of the project is the single annual discount rate at which the NPV of the project is equal to zero. 公式 NPV=C0+∑i=1m(11+IRR)iCi=0NPV=C_0+\\sum_{i=1}^{m} \\Big(\\frac{1}{1+IRR}\\Big)^i C_i=0 NPV=C0​+i=1∑m​(1+IRR1​)iCi​=0 IRR Rule 定义 Minimum required return 计算 IRRIRRIRR，与 required return 进行比较 问题 还需要将 Interest Rate 纳入考量 解决办法 先算出两个 project 的 NPV 相等时的利率 RRR，然后根据现在的利率，判断选择那个 project. Amortization Schedule monthly: IRR12\\frac{IRR}{12}12IRR​"},{"title":"Bonds","path":"/wiki/fina/bonds.html","content":"Bonds 债券 不拥有 Ownership 不能投票 无论是否盈利，盈利多少，企业都必须支付本息；否则视为破产 有法律义务支付本息 企业先支付本息，再支付分红 由于支付利息视作企业运营的开支，因此可以抵扣税款 Terms Maturity Date Face Value (Par Value) FFF Coupon Payment CCC Variations Zero-Coupon Bonds: 无利息 Bond Valuation NPV Approach Yield to Maturity (YTM) YTM 使得债券 NPV 为 000 时的 discount rate 假设一年付 mmm 次，那么每次付的比率是 YTMm\\frac{YTM}{m}mYTM​；假设 nnn 年 −P0+C×1−(1+YTM/m)−nmYTM/m+F×(1+YTM/m)−nm=0-P_0+C\\times\\frac{1-(1+YTM/m)^{-nm}}{YTM/m}+F\\times\\Big( 1+YTM/m \\Big)^{-nm}=0 −P0​+C×YTM/m1−(1+YTM/m)−nm​+F×(1+YTM/m)−nm=0 The investors are getting an annual rate of return equal to the return they require on the investment The YTM can be obtained ONLY IF we hold the bond until maturity Risk of Investment 债券投资的风险 利率风险：市场利率上升导致债券价格下跌。 信用风险：发行人违约可能性（如公司破产）。 流动性风险：债券难以快速变现。 通胀风险：通胀削弱固定收益的实际购买力。"},{"title":"Capital Investment Decision","path":"/wiki/fina/capital-investment-decision-01.html","content":"Incremental Cash Flow=Operating Cash Flow−Incremental Cash Flow on Capital Spending−Incremental Cash Flow on Change in Net Working Capital\\begin{aligned} &amp;\\text{Incremental Cash Flow}\\\\ &amp;\\quad =\\text{Operating Cash Flow}\\\\ &amp;\\quad -\\text{Incremental Cash Flow on Capital Spending}\\\\ &amp;\\quad -\\text{Incremental Cash Flow on Change in Net Working Capital} \\end{aligned} ​Incremental Cash Flow=Operating Cash Flow−Incremental Cash Flow on Capital Spending−Incremental Cash Flow on Change in Net Working Capital​ EBIT approach 先计算 EBITSales Revenue −-− Fixed Cost −-− Variable Cost Tax Shield approach OCF=(PQ−FC−VC)(1−Tc)+Dep⋅TcOCF=(PQ-FC-VC)(1-T_c)+Dep\\cdot T_c OCF=(PQ−FC−VC)(1−Tc​)+Dep⋅Tc​"},{"title":"Capital Structure 资产结构","path":"/wiki/fina/capital-structure.html","content":"Without Tax and Backruptcy Costs 企业可以通过 发行股票 issue Equity 举债 debt, bonds, bank loan 进行融资。 Value of Firm=Value of Equity+Value of Debt\\text{Value of Firm}=\\text{Value of Equity}+\\text{Value of Debt} Value of Firm=Value of Equity+Value of Debt MM Proportion I 在无税收、无破产成本的情况下，公司价值 VVV 不受资本结构指股权 equity 与债务 debt 的比重组合影响，即无杠杆一种经济活动：举债，并投资于高风险的事业、活动公司的价值等于有杠杆的公司 VU=VL=OCFV_U=V_L=OCF VU​=VL​=OCF With Tax and Bankruptcy Costs MM Proportion I 在存在企业所得税的情况下，公司价值（VLV_LVL​​）随财务杠杆增加而提升 ，主要因利息税盾（Interest Tax Shield）债务利息可抵税，每年节省税额为 Interest×Tc\\text{Interest}\\times T_cInterest×Tc​​，其现值为 D×TcD\\times T_cD×Tc​​。的税收优惠效应。 VL=VU+D×TcV_L=V_U+D\\times T_c VL​=VU​+D×Tc​ MM Proportion II 在有税环境下，股权成本（RER_ERE​​）仍随杠杆增加而上升 ，但公式需调整以反映税盾效应： RE​=RU+(RU​−RD​)×ED​×(1−Tc​)R_E​=R_U+(R_U​−R_D​)\\times E_D​\\times (1−T_c​) RE​​=RU​+(RU​​−RD​​)×ED​​×(1−Tc​​) 税收降低了债务的实际成本（因利息税盾），但股权风险和成本仍随杠杆上升而增加。"},{"title":"Investment Decision Rule","path":"/wiki/fina/invest-decision-rule.html","content":"Investment Decision Rule … based on Returns 就是看最后拿到的钱 RewardRewardReward 是不是初始投资 InvestInvestInvest 多就行， … based on NPV NPV定义​ 净现值（Net Present Value, NPV）是评估投资项目价值的核心指标，计算方式为项目所有现金流（含初始投资）按贴现率折算后的现值总和。公式为： NPV=C0​+∑i=1TCi(1+R)iNPV=C_0​+\\sum_{i=1}^T\\frac{C_i}{(1+R)^i}NPV=C0​​+∑i=1T​(1+R)iCi​​ CtC_tCt​​: 第 ttt 期的现金流（C0C_0C0​​ 为初始成本，通常为负值）。 RRR: 贴现率（通常为金融市场可获得的回报率）。 ​决策规则​ ​NPV &gt; 0：项目收益高于金融市场回报，应接受。 ​NPV &lt; 0：项目收益低于金融市场回报，应拒绝。 ​NPV = 0：项目收益与金融市场相当，决策无差异（可结合其他因素考虑）。 ​关键原则​ ​基准比较：以金融市场回报率为基准，衡量项目是否创造额外价值。 ​现金流方向： 现金流入（收入）为正值（Ct​&gt;0）。 现金流出（成本）为负值（Ct​&lt;0）。 沉没成本忽略：过去已发生且不可逆的成本（如前期调研费用）不计入NPV，仅考虑未来现金流。 ​注意事项​ ​时间价值：现金流的时点影响现值，需准确对应贴现期数。 ​零NPV的意义：项目回报率等于贴现率（金融市场回报率），而非无收益。 总结：NPV规则通过量化项目相对于金融市场的净收益，为投资决策提供清晰标准，强调未来现金流和沉没成本的正确处理。 … based on IRR 预设项目的最少回报 (Minimum Return Required)，然后判断项目的 IRR 是否高于预期。 Internal Rate of Return (IRR) 是将单笔资金拓展到多笔资金，IRRIRRIRR 是年利率 (Annual Rate of Return)，使得整个项目周期结束后的 NPV 为零 NPV=0=C0+∑i=1T(11+IRR)iCiNPV=0=C_0+\\sum_{i=1}^T \\Big(\\frac{1}{1+IRR}\\Big)^i C_i NPV=0=C0​+i=1∑T​(1+IRR1​)iCi​ NPV or IRR 实际上，IRR 和 NPV 大致成反比关系。 Notice the role of the return from the financial sector (e.g., interest rate on bank deposit) in the NPV rule and IRR rule: in the NPV rule, it is the discount rate in the IRR rule, it is the (minimum) required return of the real investment project under consideration. conventional: 初始投资都是 cash outflow，后续所有现金流都是入账 cash inflow non-conventional: 不符合 conventional 的现金流情况 IRR Rule 的问题 当同时考虑多个项目时，先计算 xxx，使得这几个项目在 Discount Rate 为 xxx 时的 NPV 相同。 Payback Rule 考虑多久可以回本（不考虑 discount） Discounted Payback Rule 先 discount cash flow，再应用 Payback Rule Profitability Index PI=PV of future cash flowsinitial costPI=\\frac{\\text{PV of future cash flows}}{\\text{initial cost}} PI=initial costPV of future cash flows​"},{"title":"PV, FV, NPV","path":"/wiki/fina/pv-fv-npv.html","content":"PV, NPV Present Value 一笔在未来会获得的钱在当下的价值 Future Value 一笔现在的钱在未来的价值 实际上 PV 和 FV 非常简单，主要就是考虑利滚利 (Compounding of Interest)，把指数算清楚即可。 某一笔钱的 PV/FV 钱的 PV 相当于说，如果某笔 TTT 年后的钱 QQQ，且利率 rrr 保持不变，那么现在要存 PVPVPV 块钱，这样经过 TTT 年的利滚利，这 PVPVPV 块钱就变成了 QQQ 块钱。 PV×(1+r)T=QPV\\times (1+r)^T=Q PV×(1+r)T=Q 反过来，Future Value 就是，现在存的 QQQ 块钱，在 TTT 年后会变成 FVFVFV 块钱。 FV=Q×(1+r)TFV=Q\\times(1+r)^T FV=Q×(1+r)T 现金流（多笔钱）的 PV/FV 本质就是多笔钱的 FV/PV 全部换算到当前的时间点， PVaggregate=∑PViFVaggregate=∑FViPV_{aggregate}=\\sum PV_{i}\\\\ FV_{aggregate}=\\sum FV_{i}\\\\ PVaggregate​=∑PVi​FVaggregate​=∑FVi​ 由于每笔钱的利滚利时间不同，因此右式的和式通常又可以写成有限项等比数列求和的形式 Annuity, Perpetuity Perpetuity 表示永久持续的恒定现金流，用 CCC 表示每年的现金流（且第一笔现金在一年后） 必须满足两个条件： The cash flows and the interest rates are constant over time, The first cash flow occurs one year from today. 那么所有这些钱的 PV 就是 PV=∑i=1∞C×(11+r)i=Cr\\begin{aligned} PV&amp;=\\sum_{i=1}^{\\infin} C\\times(\\frac{1}{1+r})^i\\\\ &amp;=\\frac{C}{r} \\end{aligned} PV​=i=1∑∞​C×(1+r1​)i=rC​​ Present Value of Annuity 与 Perpetuity 的区别在于 Annuity 有时限。假设经过 TTT 年，那么 Annuity 的 PV 就等于 PV=C(11+r)+C(11+r)2+⋯+C(11+r)T=C×1−(11+r)TrPV=C(\\frac{1}{1+r})+C(\\frac{1}{1+r})^2+\\dots+C(\\frac{1}{1+r})^T=C\\times\\frac{1-(\\frac{1}{1+r})^T}{r} PV=C(1+r1​)+C(1+r1​)2+⋯+C(1+r1​)T=C×r1−(1+r1​)T​ 在一笔现金的时候我们知道，(11+r)T(\\frac{1}{1+r})^T(1+r1​)T 就是 Present Value Factor，所以也可以直接代入写成 PV=1−Present Value Factor(T)rCPV=\\frac{1-\\text{Present Value Factor}(T)}{r}C PV=r1−Present Value Factor(T)​C Future Value of Annuity 如果以第 TTT 年的价值作为基准，把每一笔钱的 FV 都加起来，就能得到 FV=(1+r)T−1rC=Future Value Factor(T)−1rCFV=\\frac{(1+r)^T-1}{r}C=\\frac{\\text{Future Value Factor}(T)-1}{r}C FV=r(1+r)T−1​C=rFuture Value Factor(T)−1​C Effects of Inflation 由于通胀 (Inflation) 的存在，尽管手上的钞票数量变多了，但这并不意味着购买力 (Purchasing Power) 提升。 Purchasing Power the quantity of goods and services that we can buy with our money. … on Nominal/Real Interest Rate Fisher’s Equation Nominal IR RRR: interest rate in terms of money Real IR rrr: interest rate in terms of purchasing power 我们用 hhh 表示物价的变化，即通胀率 (Inflation Rate)，则可以得出他们之间的关系 (Fisher Equation) (1+r)(1+h)=(1+R)(1+r)(1+h)=(1+R) (1+r)(1+h)=(1+R) 证明 考虑一开始的钱 QQQ，一开始的物价 PPP，则今年的购买力为 QP\\frac{Q}{P}PQ​，明年的购买力（即经过一年的通胀）为 Q(1+R)P(1+h)\\frac{Q(1+R)}{P(1+h)}P(1+h)Q(1+R)​ 根据 Real Interest Rate 的定义，我们有 Purchase Poweri+1−Purchase PoweriPurchase Poweri=Real IRQ(1+R)P(1+h)QP=1+r(1+r)(1+h)=1+R\\begin{aligned} \\frac{\\text{Purchase Power}_{i+1}-\\text{Purchase Power}_i}{\\text{Purchase Power}_i}&amp;=\\text{Real IR}\\\\ \\frac{\\frac{Q(1+R)}{P(1+h)}}{\\frac{Q}{P}}&amp;=1+r\\\\ (1+r)(1+h)&amp;=1+R \\end{aligned} Purchase Poweri​Purchase Poweri+1​−Purchase Poweri​​PQ​P(1+h)Q(1+R)​​(1+r)(1+h)​=Real IR=1+r=1+R​ 由于这些值都很小，所以我们通常直接近似为 r≊R−hr\\approxeq R-hr≊R−h … on PV &amp; NPV Inflation 并不影响 PV 和 NPV。但必须 discount nominal cash at a nominal rate 或者 discount real cash at a real rate 证明 考虑 inflation rate 为 hhh，nominal interest rate 为 RRR，考虑 nnn 年后的一笔钱 CCC (nominal)，那么用 Nominal Interest Rate 算的话 PVnominal=C(1+R)nPV_{nominal}=\\frac{C}{(1+R)^n} PVnominal​=(1+R)nC​ 用购买力计算的话，nnn 年后的购买力为 C(1+h)n\\frac{C}{(1+h)^n}(1+h)nC​，因此 discount at a real rate 的话，即为 PVreal=C(1+h)n(1+r)nPV_{real}=\\frac{\\frac{C}{(1+h)^n}}{(1+r)^n} PVreal​=(1+r)n(1+h)nC​​ 代入 Fisher’s Equation 1+R=(1+h)(1+r)1+R=(1+h)(1+r)1+R=(1+h)(1+r) 可得 PVreal=PVnominalPV_{real}=PV_{nominal} PVreal​=PVnominal​ 因此其实并不影响 Present Value。对于 Net PV，由于基准年的金额有 Nominal=RealNominal=RealNominal=Real，因此也不影响 NPV"},{"title":"Risk of Return","path":"/wiki/fina/risk-and-return.html","content":"Risk of Return 现实世界中，未来的 Return 并非确定的，而是一个概率分布。 Probability 我们可以计算 Expected Return E[R]=∑P(r)⋅r=4.30%\\mathbb E[R]=\\sum P(r)\\cdot r=4.30\\% E[R]=∑P(r)⋅r=4.30% 进一步的，可以计算其方差： Var(R)=Var(R)= Var(R)= 我们可以把 return 的方差视为 volatility，方差越大，股价越不稳定。 Risk Free Risk Free 的意思就是方差为 000，通常只有国债才能做到。 Portfolio Portfolio 就是 a basket of assets，每一个 asset 都有一定的比重 Risk Systematic 经济体内的每一家企业都会遇到的风险（例如政治稳定、税收等等） Unsystematic 这类风险只会影响个别企业（例如舆论） Risk Diversification"},{"title":"Valuation of Stocks","path":"/wiki/fina/valuation-of-stocks.html","content":"Stock 股票 拥有对企业的所有权 Ownership 可以投票 以分红的形式分享利益 没有法律义务支付一定数额的分红 企业先付 bonds 产生的利息，再支付分红 支付的分红不能用于抵扣税款 Dividend Discount Model (DDM 模型) stock 会产生分红现金流， PV of Dividend Stream=∑i=1+∞Di(1+R)i\\text{PV of Dividend Stream}=\\sum_{i=1}^{+\\infin}\\frac{D_i}{(1+R)^i} PV of Dividend Stream=i=1∑+∞​(1+R)iDi​​ 也被称为 funcdamental value of stock. 在市场完全竞争的情况下，价格 P0=PV of Dividend StreamP_0=\\text{PV of Dividend Stream}P0​=PV of Dividend Stream. 这个模型被称为 Dividend Discount Model. Special Case 1: 恒常分红现金流 当 Di=DjD_i=D_jDi​=Dj​ 时，通过等比数列求和可以简便地计算 P0P_0P0​ P0=D1RP_0=\\frac{D_1}{R} P0​=RD1​​ Special Case 2: 现金流等比增长 也被称为 Gordon’s Growth Model，设分红以每年 ggg 的速度增长，即 Di=(1+g)Di−1D_i=(1+g)D_{i-1}Di​=(1+g)Di−1​，则有 P0=D1R−gP_0=\\frac{D_1}{R-g} P0​=R−gD1​​ 一些指标 Dividend Yield 计算产生的分红 Dividend Yield=D1P0\\text{Dividend Yield}=\\frac{D_1}{P_0} Dividend Yield=P0​D1​​ Capital Gains Yield 如果在 ttt 时刻卖出 stock，可以计算获得的资本 Capital Gains Yield=Pt−P0P0\\text{Capital Gains Yield}=\\frac{P_t-P_0}{P_0} Capital Gains Yield=P0​Pt​−P0​​ Common vs. Preferred Common stock 有投票权 Preferred stock 享有优先分红的权利 由于 Preferred stock 的期限是无穷的，我们可以直接按 Perpetual Bond 来看待。如果每年分红 DpD_pDp​，利率 RpR_pRp​，初始股价 P0P_0P0​，根据 DDM 模型，有 P0=DpRpP_0=\\frac{D_p}{R_p} P0​=Rp​Dp​​"},{"title":"SVD","path":"/wiki/linear-algebra/SVD.html","content":"SVD 分解"},{"title":"MoE 架构","path":"/wiki/llm/Mixture-of-experts.html","content":"MoE 架构 MoE 代码实现：以 MiniMind 为例 Experts 首先定义专家模块，Experts 是 Experts(FeedForward) Code 12345678910class FeedForward(nn.Module): def __init__(self, config: LMConfig): super().__init__() self.w1 = nn.Linear(config.dim, config.hidden_dim, bias=False) self.w2 = nn.Linear(config.dim, config.hidden_dim, bias=False) self.w3 = nn.Linear(config.hidden_dim, condig.dim, bias=False) self.dropout = nn.Dropout(config.dropout) def forward(self, x): return self.dropout(self.w3(F.silu(self.w1(x)) * self.w2(x))) Router 然后，我们来实现 Router 路由器。Router 接收一个 Token，计算出概率取 Top K 后转发给对应的专家。 Code 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990class MoEGate(nn.Module): def __init__(self, config: LMConfig): super().__init__() self.config = config self.top_k = config.num_experts_per_token self.n_routed_experts = config.n_routed_experts self.scoring_func = config.scoring_func self.alpha = config.aux_loss_alpha self.seq_aux = config.seq_aux self.norm_topk_prob = config.norm_topk_prob self.gating_dim = config.dim self.weight = nn.Parameter( torch.empty((self.n_routed_experts, self.gating_dim)) ) self.reset_parameter() def reset_parameter(self) -&gt; None: import torch.nn.init as init init.kaiming_uniform_(self.weight, a=math.sqrt(5)) def forward(self, tokens: torch.Tensor): batch, seq_len, d = tokens.shape tokens = einops.rearrange(tokens, &quot;batch seq dim -&gt; (batch seq) dim&quot;) logits = F.linear(tokens, self.weight, None) if self.scoring_func == &quot;softmax&quot;: scores = logits.softmax(dim=-1) else: raise NotImplementedError( &quot;Unsupported scoring function&quot;, ) topk_weight, topk_idx = torch.topk( scores, k=self.top_k, dim=-1, sorted=False, ) if self.top_k &gt; 1 and self.norm_topk_prob: denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20 topk_weight = topk_weight / denominator if self.train and self.alpha &gt; 0.0: scores_for_aux = scores aux_topk = self.top_k topk_idx_for_aux_loss = topk_idx.view(batch, -1) if self.seq_aux: scores_for_seq_aux = scores_for_aux.view(batch, seq_len, -1) ce = torch.zeros( batch, self.n_routed_experts, device=tokens.device, ) ce.scatter_add_( 1, topk_idx_for_aux_loss, torch.ones( batch, seq_len * aux_topk, device=tokens.device ) ).div_(seq_len * aux_topk / self.n_routed_experts) aux_loss = ( (ce * scores_for_seq_aux.mean(dim=-1)) .sum(dim=1) .mean() * self.alpha ) else: mask_ce = F.one_hot( topk_idx_for_aux_loss.view(-1), num_classes=self.n_routed_experts, ) ce = mask_ce.float().mean(0) Pi = scores_for_aux.mean(0) fi = ce * self.n_routed_experts aux_loss = (Pi * fi).sum() * self.alpha else: aux_loss = 0.0 return topk_idx, topk_weight, aux_loss 以下分析约定： Batch size BBB，即一个 batch 包含 BBB 句句子 Sequence Length SSS，即一句句子包含 SSS 个 Token 词嵌入向量维度 HHH 专家数量 EEE 每个 Token 被转发到 Top KKK 个专家 参数含义 top_k 为每一个 Token 选择几个专家进行训练 n_routed_experts 为专家总数 scoring_func 用于计算 Token 和专家之间的评分 aux_loss_alpha 辅助损失的 alpha 参数 seq_aux 控制是否在序列级别上计算辅助损失 norm_topk_prob 是否对概率进行归一化 由于我们对每一个 Token 计算它被发送到某一个专家的概率，在 __init__() 函数里，我们令 Token 的维度为 d=d=d= self.gating_dim，专家数量为 n=n=n= self.n_routed_experts，那么我们希望 Router 输出的矩阵大小就是 Router:RB×S×H↦RB×S×E\\text{Router}:\\R^{B\\times S\\times H}\\mapsto \\R^{B\\times S\\times E} Router:RB×S×H↦RB×S×E 因此这里的 self.weight 是 RH×E\\R^{H\\times E}RH×E 大小的矩阵，负责计算一个 Token 的 Route 概率。 前向传播 forward() Tensor 输入是 B×S×HB\\times S\\times HB×S×H，因为我们只关心 Token 发送到哪个专家，所以我们首先把 Tensor 拍成二维 BS×HBS\\times HBS×H，并用 self.weight 计算概率，用 softmax() 归一化。F.linear(x, A, bias=None) 计算（F.linear 的诡异计算方式使得 self.weight 定义的大小为 E×HE\\times HE×H，transpose 了一下之后 size 就是对的） y=xA⊺y=xA^\\intercal y=xA⊺ 因此，这里的 score 大小为 BS×EBS\\times EBS×E 接着，我们调用 torch.topk() 选取前 self.top_k 个专家，返回 score 中对应的权重和下标。此时 topk_weight, topk_idxTensor size: (BS,K)(BS,K)(BS,K)，意义：所有的 token 排列在一起 大小均为 BS×KBS\\times KBS×K 然后对选择出来的 self.top_k 个专家的权重再进行一次归一化（除以 denominator）。不过这一步是可选的 接着进入 if self.train and self.alpha &gt; 0.0: 判断，目的是为了平衡专家之间的负载。首先，代码确保只在训练模式以及需要平衡负载的时候才会启用。 如果需要计算 sequence level 的 loss，那么 topkidx 首先被拍平成 (B,SK)(B,SK)(B,SK)（每一个句子所有 token 对应的所有专家全部拍到一起） 因为要对 sequence level 计算专家负载损失，所以我们先定义每一句句子上专家的负载损失，其大小为 (B,E)(B,E)(B,E). 然后遍历 topk_idx 中的每一个元素，用 scatter_add_() 将 topk_idx 中的每一个元素添加到对应的 sequence 里对应的专家中。 这个流程结束之后，ce (count experts) 保存的就是每一句句子的专家负载。随后，我们对每一句句子都归一化其专家负载：一句句子会产生 SKSKSK 个 counting，总数为 SKSKSK 而 scores 的大小为 (BS,E)(BS,E)(BS,E)（表示每句句子的每个 token 与每个 expert 之间的得分），我们沿 SSS 轴对其计算平均值 scores.mean(dim=1) 这就表示每句句子与每个 expert 之间的得分（token 与 experts 得分的平均），其大小变为 (B,E)(B, E)(B,E). 然后再将 ce 与 scores.mean() 对应位置相乘，ce 可以理解为每句句子中 expert 的频率（这个 expert 在这句句子里总是被分配处理 token），scores.mean() 可以理解为每句句子中 expert 对于每个 token 的重要程度（这个 expert 总是被 MoEGate 认为与 token 关联很大），大小变为 (B,E)(B,E)(B,E)，但此时仍然是每句句子与 expert 的关联。 因此再对 EEE 求和取平均 .sum(dim=1).mean()，把 expert 与不同句子之间的频率与关联度整合起来，即对于这些句子 expert 的频率与关联度，也即 expert 的负载，其大小先变为 (E,)(E,)(E,)，再变为 (1,)(1,)(1,)。最后再乘上标量 self.alpha. 这就是我们的 sequence level 的 aux loss. 值得一提的是，这里还额外乘了一个 EEE，推测是为了让梯度不至于太小 Why It Makes Sense 因为我们的训练目标是让 Loss 尽可能的小，对于这个 Aux Loss 而言，如果某一个专家的负载特别大，那么就说明 每一个 Batch 内的所有 Tokens，这个专家对应的 scores 都会比较大 由于 torch.topk 总是都把这个专家选上，因此 ce 计算出来的加权也比较大 所以根据排序不等式，负载越不均衡，计算出来的 aux_loss 也就越大。那么反过来说，如果 aux_loss 越小，说明专家之间的负载越均衡。 如果不关心 sequence level 的 loss，那么我们直接把这一个 batch 的所有 token 拍到一起（总共 B⋅SB\\cdot SB⋅S 个 token，总共被分配 BSKBSKBSK 个专家），我们把 (BSK,1)(BSK, 1)(BSK,1) 的专家重新 encode 为 one-hot vector，即 (BSK,E)(BSK,E)(BSK,E) 那么我们对 BSKBSKBSK 轴求平均，fi 向量大小变为 (1,E)(1,E)(1,E)，就计算出来了某个专家处理的 token 数占所有 token 的比值（即频率）。因为 one-hot vector 不是 000 就是 111. 类似的，我们也对 scores (大小为 (BS,E)(BS,E)(BS,E)) 做 token level 的计算，直接按 BSBSBS 轴取平均即可，Pi 大小变为 (1,E)(1,E)(1,E)，即每个专家在所有 token 上的平均得分（关联度） 同样地，我们直接将 fi 与 Pi 对应位置相乘，求和乘上 self.alpha，这一步的目的和 sequence level 的 aux loss 是一样的。 MoE Feed Forward (MoEFFN) 然后我们来把他们组合到一起：首先，我们为 MoEFFN 定义好门控和专家，以及共享专家（无论如何都要处理 token） 1234567891011121314class MOEFeedForward(nn.Module): def __init__(self, config: LMConfig): super().__init__() self.config = config self.experts = nn.ModuleList( [ FeedForward(config) for _ in range(config.n_routed_experts) ] ) self.gate = MoEGate(config) if config.n_shared_experts is not None: self.shared_experts = FeedForward(config) 然后编写训练和推理。训练和推理的区别在于 推理模式下，Token 只转发给最优的 Expert。但在训练模式下，Token 会被转发给每一个 Expert Training 我们来考察一下训练时的代码。 这里，x.repeat_interleave() 重复输入数据，目的是让一个 token 可以多次被不同的 expert 处理，提升 expert 的泛化性 然后 y 就是计算 token 经过专家计算后输出的结果，并且将类型转为半精度浮点数 float16，此时的张量形状为 (BS×K,H)(BS\\times K, H)(BS×K,H)，经过 .view(*topk_weight.shape, -1) 之后变为（topk_weight 的形状为 ()()()） 12345678910if self.training: # 训练模式下，重复输入数据 x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0) y = torch.empty_like(x, dtype=torch.float16) for i, expert in enumerate(self.experts): y[flat_topk_idx == i] = expert(x[flat_topk_idx == i]).to( y.dtype ) # 确保类型一致 y = (y.view(*topk_weight.shape, -1) * topk_weight.unsqueeze(-1)).sum(dim=1) y = y.view(*orig_shape) Inferencing 我们来考察前向传播过程。 提取出 x (输入的 Batch of sequence of tokens) 的维数信息之后，先经由 self.gate(x) 计算每一个 token 对应的专家，然后直接拍成 a sequence of tokens (BS,H)(BS,H)(BS,H)，topk_idx 则直接拍成 (BSK,)(BSK,)(BSK,) 1234567identity = xorig_shape = x.shapebsz, seq_len, _ = x.shape# 使用门控机制选择专家topk_idx, topk_weight, aux_loss = self.gate(x)x = x.view(-1, x.shape[-1])flat_topk_idx = topk_idx.view(-1) 随后进入 y = self.moe_infer(x, flat_topk_idx, topk_weight.view(-1, 1)).view(*orig_shape) 进行计算。 这里 .argsort() 的逻辑是：因为 flat_expert_indices 的值是专家的编号，通过 argsort()，idxs 把同一个专家要处理的 token 下标聚集到一起。 tokens_per_expert 以前缀和的方式，计算每一个专家处理的 token 下标的范围。 token_idxs 从 idxs 出发计算某一个 idxs[i] 对应的是第 token_idxs[i] 个 token。这是因为每一个 token 都会分给 top KKK 个专家，因此从下标来说，i∗K→(i+1)∗K−1i*K\\to (i+1)*K-1i∗K→(i+1)∗K−1 (idxs 保存的正好都是下标) 对应的都是第 iii 个 token，因此直接整数出除法可以计算出对应第几个 token. 123456@torch.no_grad()def moe_infer(self, x, flat_expert_indices, flat_expert_weights): expert_cache = torch.zeros_like(x) idxs = flat_expert_indices.argsort() tokens_per_expert = flat_expert_indices.bincount().cpu().numpy().cumsum(0) token_idxs = idxs // self.config.num_experts_per_tok 接着，我们枚举每一个专家，拿出它需要处理的所有 tokens (即代码里的 token_idxs[start_idx : end_idx] 以及 x[exp_token_idx]) 我们把这些 token 经过 expert(expert_tokens) 计算、输出，得到 expert_out，乘上（对于这个 token 而言）每一个 expert 的权重。通过 scatter_add_()，expert_cache 包含了每个 token 位置的加权专家输出总和。 123456789101112131415for i, end_idx in enumerate(tokens_per_expert): start_idx = 0 if i == 0 else tokens_per_expert[i - 1] if start_idx == end_idx: continue expert = self.experts[i] exp_token_idx = token_idxs[start_idx:end_idx] expert_tokens = x[exp_token_idx] expert_out = expert(expert_tokens).to(expert_cache.dtype) expert_out.mul_(flat_expert_weights[idxs[start_idx:end_idx]]) # 使用 scatter_add_ 进行 sum 操作 expert_cache.scatter_add_( 0, exp_token_idx.view(-1, 1).repeat(1, x.shape[-1]), expert_out )return expert_cache 除此之外，还需要加上共享专家的输出。不过这里的话，如果在推理模式，self.aux_loss 其实没作用 1234if self.config.n_shared_experts is not None: y = y + self.shared_experts(identity)self.aux_loss = aux_lossreturn y"},{"title":"Transformer 架构","path":"/wiki/llm/classic-transformer.html","content":"Before Transformer: RNN, Attention 在 RNN 里，我们用一个隐藏变量 hth_{t}ht​ 记录前 ttt 个 Input Token 所代表的 Context。如果要生成第 t+1t+1t+1 个 Output Token 的话，就利用输入的第 t+1t+1t+1 个 Input Token 和代表了前文信息的隐藏状态 hth_{t}ht​ 做 Attention Alignment 计算出一个向量，再转换成 Word. 这样一个 AutoRegressive 模型有两个显著的问题 由于是时序生成（必须一个一个 Token 地生成），无法并行 由于时序生成，如果要保存很久远的信息，隐藏状态 hth_tht​ 的大小就必须非常大，导致了 Computation &amp; Memory Overhead Model Arch Transformer 总体是 Encoder-Decoder 架构。Encoder 将 Input"},{"title":"Decoder-Only Transformer","path":"/wiki/llm/decoder-only-transformer.html","content":"Decoder-only Transformers Decoder-only 架构通常用于生成任务，代表作包括 GPT，GPT-2 等。 什么是生成任务 简单来说，Decoder-only 解决的生成任务是指：给定前 nnn 个单词 x1,x2,…,xnx_1,x_2,\\dots,x_nx1​,x2​,…,xn​，要求输出第 n+1n+1n+1 个单词。可以看出，这类生成任务的本质是 AutoRegressive 的。 近年来，出现了使用 Diffusion 作为 Language Model 的生成，达到了极快的生成速度。"},{"title":"Tokenizer, BPE 算法","path":"/wiki/llm/tokenizer-bpe.html","content":"Byte-Pair Encoding 算法 Byte-Pair Encoding（BPE）算法是一种常用于分词器（Tokenizer）中的无监督分词方法，其主要思想是将文本中最常见的字符对（或子词对）不断合并，从而构建出一个词汇表。由于进行多轮 （假设 kkk 轮）合并，而每一次合并都会基于统计频率将一对 Token 合并为一个新 Token，因此在 kkk 轮迭代后，BPE 算法可以将长度为 kkk 的单词合并为一个 Token 将输入的文本转化为 UTF-8 Encoding 统计 Byte-Pair 的频率 计算频率最高的 Byte-Pair，合并为一个新的 Token 用新的 Token 替换旧 Byte-Pair 出现的位置 回到第 222 步，重新统计 Byte-Pair (Token-Pair) 频率 直到词汇表大小达到预设值 BPE 算法通过这种逐步合并的方式，不仅能有效地表示常见词汇，还能灵活处理低频词和新词，对于大型语言模型的分词和词表构建有很大的优势。 BPE 代码实现"},{"title":"Tokenizer 分词器","path":"/wiki/llm/tokenizer.html","content":"Tokenizer Tokenizer 在 LLM（大型语言模型）的上下文中指的是负责将输入文本分解成称为 tokens 的更小单元的组件。这些 tokens 是模型处理的基本元素（例如单词、子词或字符）。Tokenizer 将原始文本转换为模型可以处理的数字表示，并且在处理之后，还能将 tokens 转换回人类可读的文本。 Hugging Face Tokenizer: tokenizer.json"},{"title":"PCA","path":"/wiki/ml/PCA.html","content":"PCA 主成分分析 算法流程 代码实现 SciKit Learn 我们使用 sklearn 库实现（这个库可以通过 pip install scikit-learn 进行安装） 先导入 PCA 库，这是 scikit-learn 封装好的 PCA 类，后续可以直接调用 .fit_transform() 对数据进行 Projection. 以及，由于 PCA 对数据量级敏感，我们需要先 standardization，将数据点放缩到正态分布 N(0,1)N(0,1)N(0,1)，即 X′=X−μσX&#x27;=\\frac{X-\\mu}{\\sigma}X′=σX−μ​。这一步操作 scikit-learn 里也有封装好的类 sklearn.preprocessing.StandardScaler 使用 standardization 而非 normalization 的原因是，PCA 需要计算 Data point 矩阵的协方差矩阵，而 normalization 无法保留数据点的协方差信息，只有 standardization 可以。 12from sklearn.decomposition import PCAfrom sklearn.preprocessing import StandardScaler 我们可以指定 PCA 将 Dimension 减少到多少个，例如减少到 505050 个 1pca = PCA(n_components=50)"},{"title":"Bagging","path":"/wiki/ml/bagging.html","content":"Bagging Algorithm 核心思想：对原数据集采样多次（可以有漏，可以有重）分别用于训练"},{"title":"Particle Filter 粒子滤波","path":"/wiki/ml/particle-filter.html","content":"Particle Filter: Overview 现实世界里，xxx 的维度太大、数量太多，计算的时间复杂度爆炸。我们很难精确计算出 P(X)P(X)P(X) 的 closed form，一个比较经典的方法就是利用蒙特卡罗方法，化连续为离散，用若干个点近似 P(X)P(X)P(X)，这就是粒子滤波的思想。 我们通过对这些点进行追踪，从而得到大致的分布。粒子的平均值代表对 state 的近似，粒子的分布代表对 state distribution 的近似 HMM view of PF PF 对于粒子滤波而言有这么几个东西比较重要： 粒子滤波算法流程 粒子滤波的流程大致可以分为这么几步 获得 observation oto_tot​，得到每一个粒子对真实 state 的近似程度 对粒子进行重采样 (resample)，越近似的粒子比重越大。重采样将近似程度低的粒子替换为近似程度高的粒子 sample：对每一个粒子进行状态转移，即 xt+1=sample(P(Xt+1∣Xt=xt))x_{t+1}=\\text{sample}(P(X_{t+1}|X_t=x_t))xt+1​=sample(P(Xt+1​∣Xt​=xt​))"},{"title":"Plurality Majority Voting","path":"/wiki/ml/plurality-majority.html","content":"Majority Voting 多个模型分别输出预测结果，然后取投票最多的那个标签作为最后的输出。 Voting 用数学语言描述就是 y^=model{Ci(x)},1≤i≤m\\hat y=model\\Big\\{ C_i(x) \\Big\\},1\\le i\\le m y^​=model{Ci​(x)},1≤i≤m 其中 CiC_iCi​ 表示训练的第 iii 个 Classifier 多个模型组合带来准确率提升 考虑训练了 2n+12n+12n+1 个分类器，每一个分类器的准确率为 rrr，那么组合后，由于需要超过半数投票，因此正确分类的概率为 ∑k=n+12n+1(kn)rk(1−r)2n+1−k\\sum_{k=n+1}^{2n+1} \\binom{k}{n}r^k(1-r)^{2n+1-k} k=n+1∑2n+1​(nk​)rk(1−r)2n+1−k 当 n=5,r=0.7n=5,r=0.7n=5,r=0.7 时，这个值约为 0.92180.92180.9218，可以看到，准确率有很大提升。 Weighted Majority Vote 在此基础上，给每一个模型的预测结果添加权重 y^=arg max⁡i∈A∑j=1mwj[Cj(x)=i]\\hat y=\\argmax_{i\\in A} \\sum_{j=1}^m w_j \\Big[ C_j(\\bold{x})=i \\Big] y^​=i∈Aargmax​j=1∑m​wj​[Cj​(x)=i] 其中 AAA 是所有的标签，方括号函数表示如果第 jjj 的分类器对于样本 x\\bold xx 给出的预测结果是 iii 类别的话则为 111，否则为 000. 因此，Weighted Vote 就相当于是枚举标签，然后看每一个模型预测结果的加权平均，取均值最大的那个对应的标签。 Soft Voting 有的模型可以输出概率，所以我们也可以对概率进行加权，最后取最高 y^=arg max⁡i∈A∑j=1mwj⋅Pj(i)\\hat y=\\argmax_{i\\in A}\\sum_{j=1}^m w_j\\cdot P_{j}(i) y^​=i∈Aargmax​j=1∑m​wj​⋅Pj​(i) 代码实现 下面的代码实现了一个 Majority Vote Classifier (vote='classlabel') 和 Soft Vote (vote='probability') 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768from sklearn.base import BaseEstimator, ClassifierMixin, clonefrom sklearn.preprocessing import LabelEncoderfrom sklearn.pipeline import _name_estimatorsimport numpy as npimport operatorclass MajorityVoteClassifier(BaseEstimator, ClassifierMixin): def __init__(self, classifiers, vote=&quot;classlabel&quot;, weights=None): &#x27;&#x27;&#x27; __init__ 函数接收分类器列表，进行初始化 vote 表示投票方法 &#x27;&#x27;&#x27; self.classifiers = classifiers self.named_classifiers = &#123; key: value for key, value in _name_estimators(classifiers) &#125; self.vote = vote self.weights = weights def fit(self, X, y): &#x27;&#x27;&#x27; fit() 根据输入的数据 + 标签， 对标签进行 encoding（方便 Soft Vote 获取概率） 然后对 classifier 模型进行训练，并存起来 &#x27;&#x27;&#x27; self.label_enc = LabelEncoder() self.label_enc.fit(y) self.classes = self.label_enc.classes_ self.trained_classifiers = [] for classifier in self.classifiers: trained_clf = clone(classifier).fit( X, self.label_enc.transform(y), ) self.trained_classifiers.append(trained_clf) return self def predict(self, X): &#x27;&#x27;&#x27; probability 部分比较容易理解 classlabel 部分的话，我们首先获取每一个模型的输出结果（`predictions`）， 然后 &#x27;&#x27;&#x27; if self.vote == &#x27;probability&#x27;: maj_vote = np.argmax(self.predict_proba(X), axis=1) else: predictions = np.asarray( [ clf.predict(X) for clf in self.trained_classifiers ] ).T maj_vote = np.apply_along_axis( lambda x: np.argmax(np.bincount(x, weights=self.weights)), axis=1, arr=predictions) maj_vote = self.label_enc.inverse_transform(maj_vote) return maj_vote def predict_proba(self, X): probas = np.asarray([clf.predict_proba(X) for clf in self.classifiers_]) avg_proba = np.average(probas, axis=0, weights=self.weights) return avg_proba"},{"title":"Support Vector Machine","path":"/wiki/ml/svm.html","content":"Linear SVM Kernel SVM 通过映射函数 ϕ()\\phi()ϕ()，将低维的特征向量 feature vector xxx 映射到高维空间中 vxv_xvx​，以期望在低维空间不可线性分割的 feature vector 在高维空间可以被线性分割。 但是当映射到高维空间之后，高维向量之间的点乘运算比较耗时，因此利用核函数 Kernel Function K(v1,v2)\\mathcal{K}(v_1,v_2)K(v1​,v2​) 替换点乘运算。例如常见的做法是 K(x(i),x(j))=exp⁡(−γ∥x(i)−x(j)∥2)\\mathcal{K}(x^{(i)},x^{(j)})=\\exp\\Big( -\\gamma\\|x^{(i)}-x^{(j)} \\|^2 \\Big) K(x(i),x(j))=exp(−γ∥x(i)−x(j)∥2) 当 γ=12σ2\\gamma=\\frac{1}{2\\sigma^2}γ=2σ21​ 时，就是高斯核函数。"},{"title":"Comparative Statics","path":"/wiki/microecon/comparative-statics.html","content":"Change of Demand Curve 令 xxx 表示商品，那么其 Demand Curve 可以表示为 Qxd=a+bPxd+…Q^d_x=a+bP^d_x+\\dots Qxd​=a+bPxd​+… Increase in Demand 直线向右上移动，可以是向上平移，也可以是向右平移 Movement 影响因素：Income Normal Good Income↑ ⟹ Qxd↑\\text{Income}\\uparrow \\implies Q^d_x\\uparrowIncome↑⟹Qxd​↑ Inferior Good Income↑ ⟹ Qxd↓\\text{Income}\\uparrow \\implies Q^d_x\\downarrowIncome↑⟹Qxd​↓ 影响因素：Population 影响因素：Price of Substitutes Psubstitute↑ ⟹ Qxd↑P_{\\text{substitute}}\\uparrow \\implies Q^d_x\\uparrow Psubstitute​↑⟹Qxd​↑ 当其他平替的价格上涨，消费者自然而然会转向价格更低的 xxx 影响因素：Price of Complement Pcomplement↑ ⟹ Qxd↓P_{\\text{complement}}\\uparrow \\implies Q^d_x\\downarrow Pcomplement​↑⟹Qxd​↓ 影响因素：Expectation The expectation of a higher (lower) price for a good in the future increases (decreases) current demand for the good. 影响因素：Tastes Supply Movement of Supply Curve Increase in Supply Technology Entry implies more sellers in the market increasing supply. Exit implies fewer sellers in the market decreasing supply. Sellers will supply less of a good if the price of an alternate good using the same inputs rises (and vice versa). (-)"},{"title":"Cost Benefit Analysis","path":"/wiki/microecon/cost-benefit.html","content":"Rational Decision 对每一件事进行衡量，其付出为 C(x)C(x)C(x)，获得为 B(x)B(x)B(x)，那么定义 Economic Surplus ES(x)ES(x)ES(x) 为 ES(x)=B(x)−C(x)\\boxed{ES(x)=B(x)-C(x)} ES(x)=B(x)−C(x)​ Cost Benefit Principle当且仅当 ES(x)≥0ES(x)\\ge 0ES(x)≥0 时，才会做 xxx，此时收益大于付出；否则不做。 Cost/Benefit 只包含会影响被决策的 Cost 和 Benefit Sunk Cost Marginal(Additional) Cost/Benefit Only if Marginal Benefit≥Marginal Cost\\text{Marginal Benefit}\\ge\\text{Marginal Cost} Marginal Benefit≥Marginal Cost Allocation of Resources 优先分配给 Marginal Benefit 多的 Opportunity Cost 所有不选的选项里，Economic Surplus(不选的选项的 Cost = Benefit，没有这部分的开销了) 最大的那个"},{"title":"竞争下的 Cost 与 Profit Maximization","path":"/wiki/microecon/cost-maxprofit-under-competition.html","content":"大前提 在完全竞争的市场下，任何一家企业都无法操控市场价格，只能根据市场价格调整自身的产量。即市场价格决定每一家企业的定价。 逻辑 由于企业无法操控市场价格，如果企业的定价高于市场价格，那么消费者必然转向其替代品；如果企业定价低于市场价格 企业运行的驱动动力：利益 企业必然追求利益。当产量为 QQQ 时，利润 Profit π(Q)\\pi(Q)π(Q) 定义为 π(Q)=TR(Q)−TC(Q)\\boxed{\\pi(Q)=TR(Q)-TC(Q)} π(Q)=TR(Q)−TC(Q)​ 这里的 Total Cost TC(Q)TC(Q)TC(Q) 包含 explicit cost 和 implicit cost. Economic Profit 与 Accounting Profit 这两个概念会在分析长期行为时提到，简而言之 Economic Profit 需要包含 Implicit Cost Accounting Profit 则不需要包含 后文提到的 Profit 若无特殊说明都是指 Economic Profit. 利益最大化 根据 Cost-Benefit Analysis，利润最大化的时候对应的产量 Q∗Q^\\astQ∗ 必然有 MR(Q∗)=MC(Q∗)MR(Q^\\ast)=MC(Q^\\ast) MR(Q∗)=MC(Q∗) 而且，此处的 MRMRMR 恒等于市场价格，即 MR(Q∗)=PMR(Q^\\ast)=PMR(Q∗)=P。因此，在完全竞争的市场下，总是有 P=MC(Q∗)\\boxed{P=MC(Q^\\ast)} P=MC(Q∗)​ 企业的成本 分为两种： 固定成本 (Fixed Cost)：短期内无法改变 Quantity，例如生产机器、办公楼等等 可变成本 (Variable Cost)：短期内可以改变 QUantity，例如劳动力、生产原料等等 在后面分析企业行为的时候也会用到这两个概念。简单来说，如果考察企业的短期行为，由于固定成本可以看作是已经产生费用，因此应当看作 Sunk Cost，不应参与短期行为决策；但对于长期行为而言，也应将固定成本考虑进去。 企业的市场行为 既然要赚取利益，那么企业是否继续参与市场必然与 Profit 有关。当 π(Q∗)&lt;0\\pi(Q^\\ast)\\lt 0π(Q∗)&lt;0 时，企业的利润小于零，企业会选择退出市场；否则就有利可图，会继续参与市场。 也可以理解为 π(Q=Q∗)\\pi(Q=Q^\\ast)π(Q=Q∗) 与 π(Q=0)\\pi(Q=0)π(Q=0) 之间进行比较 短期行为 从短期来看，企业的 Fixed Cost 不应计入决策过程，企业是否退出市场取决于利润 π(Q∗)≥π(0)TR(Q∗)−TC(Q∗)≥TR(0)−TC(0)PQ∗−TVC(Q∗)−TFC≥0−TVC(0)−TFCPQ∗≥TVC(Q∗)\\begin{aligned} \\pi(Q^\\ast)&amp;\\ge \\pi(0)\\\\ TR(Q^\\ast)-TC(Q^\\ast)&amp;\\ge TR(0)-TC(0)\\\\ PQ^\\ast-TVC(Q^\\ast)-TFC&amp;\\ge 0-TVC(0)-TFC\\\\ PQ^\\ast&amp;\\ge TVC(Q^\\ast)\\\\ \\end{aligned} π(Q∗)TR(Q∗)−TC(Q∗)PQ∗−TVC(Q∗)−TFCPQ∗​≥π(0)≥TR(0)−TC(0)≥0−TVC(0)−TFC≥TVC(Q∗)​ 所以有 P≥AVC(Q∗)\\boxed{P\\ge AVC(Q^\\ast)} P≥AVC(Q∗)​ 因此从短期来看，企业是否会退出市场，取决于市场价格（会影响收入）和自身生产的平均可变成本 (Average Variable Cost, AVC)。此时的利润为 π(Q∗)=(P−AC)×Q∗\\boxed{\\pi(Q^\\ast)=\\Big( P-\\textcolor{red}{AC} \\Big)\\times Q^\\ast} π(Q∗)=(P−AC)×Q∗​ 注意！ 计算利润时要注意包含 Fixed Cost。只有在做决策时才不计算 FC. 曲线的性质 MC 曲线总是和 AVC 曲线交于 AVC 曲线的最低点。 证明 略。 进一步的，我们可以推断，企业短期内的生产曲线 Supply Curve 由 MC Curve 和 Shutdown Decision 共同决定。 Supply Curve 长期行为 对于长期而言，Fixed Cost 此时也应该算入（例如办公楼续约费可以视为支出）。类似的，应有 P≥AC(Q∗)\\boxed{P\\ge AC(Q^\\ast)} P≥AC(Q∗)​ 当且仅当市场价格高于平均成本（含固定成本），企业才会考虑进入市场（不然无法收回固定成本）。 合并来看，Supply Curve 差不多长这样： Aggregate Supply Curve 例题 2020 Spring Final 题目 There are many identical firms (having the same production costs and producing the same product) in a competitive market. Their product is assumed to be perfectly divisible. When one firm produces qqq units, its marginal cost is MC(q)=qMC(q) = q MC(q)=q and the corresponding total variable cost is TVC(q)=0.5q2TVC(q) = 0.5q^2 TVC(q)=0.5q2 The demand curve in the market is Q=9000−90PQ = 9000 - 90P Q=9000−90P Moreover, assume that the industry is constant cost. The market is currently in a long-run equilibrium, and the equilibrium price is $45\\$45$45 per unit. Using the information, we conclude that there are [ Answer36 ] firms in the market. Using this information, we conclude that the fixed cost of an individual firm is [ Answer37 ] dollars. Using this information, we conclude that the short-run supply curve of this industry is of the form Q=A+B×PQ = A + B \\times P Q=A+B×P where AAA is equal to [ Answer38A ], and BBB is equal to [ Answer38B ] Suppose now the market demand has become Q=15300−153PQ = 15300 - 153P Q=15300−153P Using the information, we can calculate that in the long run, an individual firm will produce [ Answer39 ] units. 解答"},{"title":"Elasticity 弹性","path":"/wiki/microecon/elasticity.html","content":"弹性 量化某个变量随着另一个变量的变化而变化的程度 弹性大：因变量对自变量的变化很敏感 弹性小：因变量对自变量的变化不怎么敏感 Price Elasticity of Demand 量化 PED=percentage change in Quantity demandedpercentage change in Price=%ΔQd%ΔPPED=\\frac{\\textbf{percentage}\\text{ change in Quantity demanded}} {\\textbf{percentage}\\text{ change in Price}}=\\boxed{\\frac{\\%\\Delta Q^d}{\\%\\Delta P}} PED=percentage change in Pricepercentage change in Quantity demanded​=%ΔP%ΔQd​​ 两点 PED 计算公式 我们用中点代为计算 Percentage Change Percentage=New−Old(New+Old)/2\\text{Percentage}=\\frac{\\text{New}-\\text{Old}}{(\\text{New+Old})/2} Percentage=(New+Old)/2New−Old​ 那么 PEDPEDPED 的计算公式可以改写成 PED=Qnewd−QolddPnew−Pold×Pnew+PoldQnewd+Qoldd\\boxed{ PED=\\frac{Q^d_{new}-Q^d_{old}}{P_{new}-P_{old}}\\times \\frac{P_{new}+P_{old}}{Q^d_{new}+Q^d_{old}} } PED=Pnew​−Pold​Qnewd​−Qoldd​​×Qnewd​+Qoldd​Pnew​+Pold​​​ 当某个点 oldoldold 已经被固定了的时候，考虑其差值 ΔQ→0\\Delta Q\\to 0ΔQ→0，就有 PED=ΔQdΔP×2Pold+ΔP2Qoldd+ΔQd→ΔQdΔP×PoldQoldd→PQd×1slope\\begin{aligned} PED&amp;=\\frac{\\Delta Q^d}{\\Delta P}\\times \\frac{2P_{old}+\\Delta P}{2Q^d_{old}+\\Delta Q^d}\\\\ &amp;\\to \\frac{\\Delta Q^d}{\\Delta P}\\times\\frac{P_{old}}{Q^d_{old}}\\\\ &amp;\\to \\boxed{\\frac{P}{Q^d}\\times \\frac{1}{\\text{slope}}} \\end{aligned} PED​=ΔPΔQd​×2Qoldd​+ΔQd2Pold​+ΔP​→ΔPΔQd​×Qoldd​Pold​​→QdP​×slope1​​​ Observation Price Elasticity 随着点在 Quantity of Demand 曲线上的移动而变化；并且在中点处为 −1-1−1，往上 &lt;−1\\lt -1&lt;−1，往下 &gt;−1\\gt -1&gt;−1 如果两条 QdQ^dQd 曲线有交点，那么更加平缓的那条直线在这个点上的弹性更大。 Elasticity 与 Revenue 收入基本公式 Revenue=Quantity×Price\\text{Revenue}=\\text{Quantity}\\times\\text{Price} Revenue=Quantity×Price 因此考虑 Elasticity 的话，Revenue 是关于 Price 的二次函数，并且在 PED=−1PED=-1PED=−1 的时候，取到最大值 弹性需求（∣η∣&gt;1∣\\eta∣&gt;1∣η∣&gt;1）：降价增加总收益（需求量增幅 &gt;&gt;&gt; 价格降幅）。 非弹性需求（∣η∣&lt;1∣\\eta∣&lt;1∣η∣&lt;1）：降价减少总收益（需求量增幅 &lt;&lt;&lt; 价格降幅）。 单位弹性（∣η∣=1∣\\eta∣=1∣η∣=1）：总收益最大。 也可以在 QdQ^dQd 直线上直观地进行比较：找到点变化前后对应的矩形变化面积。更一般的，如果点在中点上方，则总收益一定增加；在下方则总收益减少。 Constant Elasticity 如果一条曲线在每一个点的 PPP Elasticity of QQQ 都相等为 −k-k−k，那么其曲线可以表示为 f(P,Q):PkQ=Cf(P,Q):\\boxed{P^{\\textcolor{red}{k}}Q=C} f(P,Q):PkQ=C​ 证明（不考） 考虑 Q-P 曲线 fff 在这一个点的 Elasticity，用点斜式即为 Elasticity=−k=PQ×dQdP\\text{Elasticity}=-k=\\frac{P}{Q}\\times\\frac{dQ}{dP} Elasticity=−k=QP​×dPdQ​ 把 xdxx\\mathop{dx}xdx 放到一起： −kPdP=1QdQ-\\frac{k}{P}\\mathop{dP}=\\frac{1}{Q}\\mathop{dQ} −Pk​dP=Q1​dQ 两边积分 −kln⁡P+CP=ln⁡Q+CQln⁡Q+kln⁡P=cPkQ=C\\begin{aligned} -k\\ln{P}+C_P&amp;=\\ln{Q}+C_Q\\\\ \\ln Q+k\\ln P&amp;=c\\\\ P^kQ&amp;=C \\end{aligned} −klnP+CP​lnQ+klnPPkQ​=lnQ+CQ​=c=C​ 左右取对数，曲线方程也可以写作 ln⁡Q=−kln⁡P+c\\boxed{ \\ln Q=\\textcolor{red}{-k} \\ln P+c } lnQ=−klnP+c​ 影响 Price Elasticity of Demand 的因素 Availability of Substitutes Time Horizon 产品有效期 Category of product (specific or broad) Necessities vs. Luxuries Purchase Size Substitutes Fewer substitutes makes it harder for consumers to adjust QQQ when PPP changes… so demand is more inelastic. Many substitutes? Switching brands when prices change is easy, so demand is more elastic. Time Horizon Category 另外两种 Elasticity Cross-Elasticity Exy=%ΔQd of X%ΔP of Y=PyQxd×ΔQxΔPy\\begin{aligned} E_{xy}&amp;=\\frac{\\%\\Delta Q^d \\text{ of X}}{\\%\\Delta P \\text{ of Y}}\\\\ &amp;=\\boxed{\\frac{P_y}{Q^d_x}\\times\\frac{\\Delta Q_x}{\\Delta P_y}} \\end{aligned} Exy​​=%ΔP of Y%ΔQd of X​=Qxd​Py​​×ΔPy​ΔQx​​​​ Exy&gt;0E_{xy}&gt;0Exy​&gt;0 说明是 Substitute ；反之，说明是 complement Income Elasticity EI=%ΔQd%ΔIncome=IQx×ΔQxΔI\\begin{aligned} E_I&amp;=\\frac{\\%\\Delta Q^d}{\\%\\Delta \\text{Income}}\\\\ &amp;=\\boxed{\\frac{I}{Q_x}\\times\\frac{\\Delta Q_x}{\\Delta I}} \\end{aligned} EI​​=%ΔIncome%ΔQd​=Qx​I​×ΔIΔQx​​​​ EI&gt;1E_I\\gt 1EI​&gt;1 说明是 Luxury EI&gt;0E_I\\gt 0EI​&gt;0 说明是 Normal Goods EI&lt;0E_I\\lt 0EI​&lt;0 为 Inferior Goods Price Elasticity of Supply 类似的，也有中点公式和点斜公式 性质 PES&gt;0PES\\gt 0PES&gt;0 若截距 &gt;0\\gt 0&gt;0，那么随着 QsQ^sQs 增加，PESPESPES 降低，但永远 &gt;1\\gt 1&gt;1 若过原点，则 PES≡1PES\\equiv 1PES≡1 影响因素 Change in Per-Unit Costs with Increased Production Time Horizon Share of Market for Inputs Geographic Scope Elasticity and Quick Predictions 把基准点放在 Equilibrium Point，记 ηs\\eta_sηs​ 为 Price Elasticity of Supply，ηd\\eta_dηd​ 为 Price Elasticity of Demand，则有 % change in Price from a shift in Demand ΔQd=% change in Demand ΔQdηs+∣ηd∣% change in Price from a shift in Supply ΔQs=−% change in Supply ΔQsηs+∣ηd∣\\begin{array}{rll} \\text{\\% change in \\textbf{Price} from a shift in \\textbf{Demand} }\\Delta Q^d\\\\ =\\frac{\\text{\\% change in \\textbf{Demand} }\\Delta Q^d}{\\eta_s+|\\eta_d|}\\\\ \\text{\\% change in \\textbf{Price} from a shift in \\textbf{Supply} }\\Delta Q^s \\\\ =\\textcolor{red}{-}\\frac{\\text{\\% change in \\textbf{Supply} }\\Delta Q^s}{\\eta_s+|\\eta_d|} \\end{array} % change in Price from a shift in Demand ΔQd=ηs​+∣ηd​∣% change in Demand ΔQd​% change in Price from a shift in Supply ΔQs=−ηs​+∣ηd​∣% change in Supply ΔQs​​"},{"title":"externalities","path":"/wiki/microecon/externalities.html","content":"Market Failure total surplus (both of the consumer and producer) is maximized in free markets. The market equilibrium price and quantity are socially optimal… (1) when all relevant production costs are incurred by sellers (2) when all relevant consumption benefits accrue to buyers. Sometimes costs or benefits that result from an activity accrue to people not directly involved in the activity Ex ternal cost = a cost paid by people other than the consumer or the producer trading in the market Social cost = the cost to everyone o Social cost = private cost + external cost Deadw eight Loss is the welfare loss because of quantity traded deviating from social optimal level"},{"title":"Monopoly 垄断","path":"/wiki/microecon/monopoly.html","content":"垄断定义 拥有定价权 价格不受 Demand Quantity 影响 Profit Maximizing Rule 依然遵循利益最大化原则，对于垄断企业来说，只需要考虑在 Demand Curve 上找到 Marginal Revenue =0=0=0 的那个点即可。 Marginal Revenue 对于离散的 Quantity，有 MR(Q)=TR(Q)−TR(Q−1)MR(Q)=TR(Q)-TR(Q-1) MR(Q)=TR(Q)−TR(Q−1) 对于连续的 Quantity，有 P(Q)=a−bQ ⟹ MR(Q)=a−2bQP(Q)=a-bQ\\implies MR(Q)=a-2bQ P(Q)=a−bQ⟹MR(Q)=a−2bQ 证明 由于是连续性变量，考虑极小值 ΔQ\\Delta QΔQ 的 Marginal Revenue，则有 MR(Q)=lim⁡ΔQ→0(a−b(Q+ΔQ))(Q+ΔQ)−(a−bQ)QΔQ=a−2bQ\\begin{aligned} MR(Q)&amp;=\\lim_{\\Delta Q\\to 0}\\frac{\\Big(a-b(Q+\\Delta Q)\\Big)(Q+\\Delta Q)-(a-bQ)Q}{\\Delta Q}\\\\ &amp;=a-2bQ \\end{aligned} MR(Q)​=ΔQ→0lim​ΔQ(a−b(Q+ΔQ))(Q+ΔQ)−(a−bQ)Q​=a−2bQ​ Select Price 选定 Quantity 之后，根据 Demand Curve 计算 Price 即可。 Price Elasticity of Demand 与 Monopoly Markup 完全竞争的市场里，无法有溢价，即必须满足 P=MCP=MCP=MC。但垄断市场里可以有溢价： P=ϵ1+ϵMCMarkup=P−MCMC=−11+ϵP=\\frac{\\epsilon}{1+\\epsilon}MC\\\\ Markup=\\frac{P-MC}{MC}=\\frac{-1}{1+\\epsilon} P=1+ϵϵ​MCMarkup=MCP−MC​=1+ϵ−1​ The More Inelastic the Demand Curve the More the Monopolist Raises Price Above Marginal Cost Welfare Analysis"},{"title":"Price Ceiling/Floor","path":"/wiki/microecon/price-ceiling-floor.html","content":"Price Ceiling 价格上限在 P−QP-QP−Q 图像上表示为一条水平线 Ceiling Line 黑色实线为坐标轴，纵轴 PPP，横轴为 QQQ 橙色线表示 P=F(Qs)P=F(Q^s)P=F(Qs) 黄色线表示 P=F(Qd)P=F(Q^d)P=F(Qd) 蓝色线表示 Price Ceiling， 注意！ 只有当蓝色低于 Equilibrium 点的时候才需要额外分析，如果价格上限高于市场价，那么对于市场完全没有影响 于是此时我们可以看到，需求量远远大于供给量，于是在供给量固定的情况下，市场上的买家需要争夺这些稀缺的供给……我们来看四种情况 Bribery 第一种解决办法，加价/拍卖。Total Value of bribery 会计入 Surplus 中 Waiting in Line Total Value of Time，会算作损失"},{"title":"Public Goods","path":"/wiki/microecon/public-goods.html","content":"Excludability and Rivalry Excludability: 排他性 Non-excludable goods: Cannot exclude non-payers (e.g., national defense, radio signals). Excludable goods: Can exclude non-payers (e.g., jeans, paid e-books). Rivalry: 竞争性 Rival goods : Use by one person reduces availability for others. Non-rival goods : One person’s use does not diminish availability for others. Excludability Rivalry Type Examples Excludable Rival Private Goods Jeans, hamburgers, gasoline Excludable Non-rival Nonrival Private Wi-Fi, satellite TV Non-excludable Rival Common Resources Timber in public land, bluefin tuna Non-excludable Non-rival Public Goods National defense, lighthouses 四种商品类型 Private Goods 可以直接高效地进入竞争市场 因为 rival，所以 excludable 不导致 inefficiency 因为 excludable，所以有 incentive 去消费和生产 经典微观经济学的假设： 所有生产成本由生产者承担 所有消费收益由消费者享受 Public Goods Under-provision : Free-rider problem leads to insufficient supply. Collective Action : Difficult to negotiate joint purchases, especially with large groups. 需要政府介入 Taxes to buy public goods (e.g., streetlights, highways). Optimal quantity determined where Marginal Social Benefit (MSB) = Marginal Social Cost (MSC) . MSB curve: 分段加和 Non-rival Private Common Resources"},{"title":"Supply Demand","path":"/wiki/microecon/supply-demand.html","content":"Demand Curve Normal good: When we have more income, we choose to buy more of the good. Inferior good: When we have more income, we choose to buy less of the good. Combination of Demand Curves Qtotald=Q1d+Q2dQ^d_{total}=Q^d_{1}+Q^d_2 Qtotald​=Q1d​+Q2d​ Supply Curve Horizontally: How many suppliers are willing and able to sell at a certain price. Vertically: The minimum price for which suppliers are willing to sell a certain quantity. Combination of Supply Curves Qtotals=Q1s+Q2sQ^s_{total}=Q^s_{1}+Q^s_2 Qtotals​=Q1s​+Q2s​ 计算 Equilibrium: Supply Curve 与 Demand Curve 的交点 Economic Surplus 这个 Surplus 可以这样理解：如果我预期 100100100 元买下，而我实际只花了 606060，那么其实我会觉得我赚了 100−60=40100-60=40100−60=40。 而在 Equilibrium 的情况下，交易价为 Equi Price，在 Demand Curve 上不同预期价（Price，纵坐标）有对应的人数（Quantity，横坐标，实际上应该是 ΔQ\\Delta QΔQ），因此对于这个预期价而言，他们获得的“赚了”感是 P×ΔQP\\times \\Delta QP×ΔQ 因此在下图的公式里，所有的 Surplus 是一个三角形 Total Economic Surplus=Consumer Surplus+Producer Surplus\\text{Total Economic Surplus}=\\text{Consumer Surplus}+\\text{Producer Surplus} Total Economic Surplus=Consumer Surplus+Producer Surplus 如果 Economic Surplus &lt;0\\lt 0&lt;0 那么交易就不会发生 红色部分就是 Total Economic Surplus 分别计算 Consumer 和 Producer 的 Surplus"},{"title":"Tax and Subsidy","path":"/wiki/microecon/tax-subsidy.html","content":"本章最核心的理论是，无论税收或者补贴，假设为 TTT 元，那么当达到新的平衡点时，消费者支付价格与生产者成本价格之间的差值必为 TTT 元 Pd−Ps=T\\boxed{P^d-P^s=T} Pd−Ps=T​ Tax Subsidy"},{"title":"Trading 交易与分工","path":"/wiki/microecon/trading.html","content":"Unit Requirement Table 与 Unit Productivity Table Requirement 和 Productivity Table 最重要的区别就是：前者给出生产一个物品需要的资源，后者给出在限定资源的情况下能生产多少物品。 有一个简单的转化： 1Requirement=Productivity\\frac{1}{\\text{Requirement}}=\\text{Productivity} Requirement1​=Productivity Opportunity Cost Opportunity Cost Copp()C_{opp}()Copp​() 描述某个人在生产某件物品的时候，能够生产多少的其他物品；直观理解就是这个人生产这件物品有多 efficient 重要公式Copp(A)=Time of ATime of B=Productivity of BProductivity of AC_{opp}(A)=\\frac{\\text{Time of }A}{\\text{Time of }B}=\\frac{\\text{Productivity of }B}{\\text{Productivity of }A}Copp​(A)=Time of BTime of A​=Productivity of AProductivity of B​ 这里的 Quantity 是在一段长度确定的时间内的。并且可以注意到 Copp(A)=1Copp(B)C_{opp}(A)=\\frac{1}{C_{opp}(B)}Copp​(A)=Copp​(B)1​ 如果对于两个人 X,YX,YX,Y，如果 Copp,X(A)&lt;Copp,Y(A)C_{opp,X}(A)\\lt C_{opp,Y}(A)Copp,X​(A)&lt;Copp,Y​(A)，即 XXX 在 AAA 上的 Opportunity Cost 更小，我们称 XXX 在 AAA 上有 Comparative Advantage. Specialization 分工 一个经济体里肯定会有分工，理性经济体里的分工由 Opportunity Cost 的大小来决定：让 Copp(A)C_{opp}(A)Copp​(A) 最小的人来负责这件 AAA （总是让最高效的人来处理这件事） 分工的存在，也可以让经济体达到 1+1&gt;21+1\\gt 21+1&gt;2 的效果。 Term of Trade (TOT) TOTTOTTOT 描述交易时的换算比例（例如 1.11.11.1 Tea/Cake 说明 111 个蛋糕能交易 1.11.11.1 包茶） 因为交易的双方都需要从交易中获利（否则根本不会进行交易），此时 Term of Trade 叫需要满足一些条件，使得双方都能获利。这里的获利的意思是，我从你这里买东西比我自己生产这个东西要好（你更加熟练，需要的资源更少）。 一个重要的公式就是，对于交易的双方，TOTAperBTOT_{A\\mathop{per}B}TOTAperB​（表示 1 unit B=TOT unit A\\text{\\(1\\) unit \\(B=TOT\\) unit \\(A\\)}1 unit B=TOT unit A）应该满足 Copp,X(A)≤TOTAperB≤Copp,Y(A)C_{opp,X}(A)\\le TOT_{A\\mathop{per}B}\\le C_{opp,Y}(A) Copp,X​(A)≤TOTAperB​≤Copp,Y​(A) 证明 我们假设经济体只生产 A,BA,BA,B 两件物品，且生产 AAA 的 XXX 与生产 BBB 的 YYY 进行交易。那么我们首先知道，根据分工，有 Copp,X(A)&lt;Copp,Y(A)Copp,X(B)&gt;Copp,Y(B)C_{opp,X}(A)\\lt C_{opp,Y}(A)\\\\ C_{opp,X}(B)\\gt C_{opp,Y}(B) Copp,X​(A)&lt;Copp,Y​(A)Copp,X​(B)&gt;Copp,Y​(B) 交易双方判断能否获利的准则是： （Requirement）生产相同数量时，相比自己生产，能否获得资源上的节约？ （Quantity）拥有相同数量资源时，相比自己生产，能否获得产品数量上的提升？ 现在假设 TOT 的计算是 111 个单位 BBB 能交易 TOTTOTTOT 单位的 AAA（那么其单位就是 A/BA/BA/B），用资源的节省量推导。 那么，对 XXX 而言，他生产的是 AAA，购买的是 BBB，那么他生产的 111 个单位的 AAA 能换来 1TOT\\frac{1}{TOT}TOT1​ 的 BBB，理论应该节约 RX(B)−1TOTRX(A)≥0 ⟺ TOT≥RX(A)RX(B)=Copp,X(A)\\begin{aligned} &amp;R_{X}(B)-\\frac{1}{TOT}R_X(A)\\ge 0\\\\ \\iff &amp;TOT\\ge \\frac{R_X(A)}{R_X(B)}=C_{opp,X}(A) \\end{aligned} ⟺​RX​(B)−TOT1​RX​(A)≥0TOT≥RX​(B)RX​(A)​=Copp,X​(A)​ 同理，对 YYY 而言，他生产的每单位 BBB 能换 TOTTOTTOT 单位的 AAA，理论上，生产 AAA 可以节约 RY(A)−TOT×RY(B)≥0 ⟺ TOT≤RY(A)RY(B)=Copp,Y(A)\\begin{aligned} &amp;R_Y(A)-TOT\\times R_Y(B)\\ge 0\\\\ \\iff&amp;TOT\\le\\frac{ R_Y(A)}{R_Y(B)}=C_{opp,Y}(A) \\end{aligned} ⟺​RY​(A)−TOT×RY​(B)≥0TOT≤RY​(B)RY​(A)​=Copp,Y​(A)​ 所以，对于交易的双方，TOTAperBTOT_{A\\mathop{per}B}TOTAperB​（表示 1 unit B=TOT unit A\\text{\\(1\\) unit \\(B=TOT\\) unit \\(A\\)}1 unit B=TOT unit A）应该满足 Copp,X(A)≤TOTAperB≤Copp,Y(A)C_{opp,X}(A)\\le TOT_{A\\mathop{per}B}\\le C_{opp,Y}(A) Copp,X​(A)≤TOTAperB​≤Copp,Y​(A) (PPC) Production Possibility Curve PPC 上的一条直线 我们假设纵轴表示 AAA 的生产，横轴表示 BBB 的生产，那么截距表示全力生产某一样物品的情况下，该物品的产量。 我们来考察这条直线的斜率 kkk，则有 k=−Productivity of AProductivity of B=−Copp(B)=−1Copp(A)\\begin{aligned} k&amp;=-\\frac{\\text{Productivity of }A}{\\text{Productivity of }B}\\\\ &amp;=-C_{opp}(B)\\\\ &amp;=-\\frac{1}{C_{opp}(A)} \\end{aligned} k​=−Productivity of BProductivity of A​=−Copp​(B)=−Copp​(A)1​​ 我们更关心 ∣k∣|k|∣k∣，这个绝对值的意义更加鲜明：多生产 111 个单位的 BBB 的 Opportunity Cost 为 ∣k∣|k|∣k∣ 的单位的 AAA.，而 ∣k∣=Copp(B)|k|=C_{opp}(B)∣k∣=Copp​(B) 多条直线：分工 Low-Hanging-Fruit Principle 这个原理描述一个经济体内多人合作分工时，若要扩大生产，一定先让 Lowest Opportunity Cost 的人去做（因为最高效） 如果扩大的是 BBB 的生产（横轴），那么从左向右斜率的绝对值越来越大，越来越陡峭；图像呈现向上凸。 如果扩大的是 AAA 的生产（纵轴），那么从下往上直线的斜率的绝对值越来越小，越来越平缓（因为 Copp(A)C_{opp}(A)Copp​(A) 与 Copp(B)C_{opp}(B)Copp​(B) 成反比）；不过图像仍是上凸的。 直线的相交位置：(Bi−1,Ai)(B_{i-1},A_i)(Bi−1​,Ai​) 影响 PPC 的因素 资源增多 科技进步 总结 通常来说，如果交易双方的 Copp()C_{opp}()Copp​() 差距越大，那么双方交易带来的资源节省和产能提升也会越大。 (CPC) Consumption Probalitity Curve Closed Economy: 无开放贸易 在无开放贸易的情况下，一个经济体的 CPC 和 PPC 是重合的。因为除了这几个人没有人需要生产的物品，因此这些人生产出来的东西只能被自己消耗。 有浪费会趋于减产，有不够会趋于增产，最终都会回归到 PPC 上，因此 CPC 与 PPC 重合。 Open Economy and Open Trade 我们可以从几个角度来看 Open Trade 对 Production 和 Consumption 的影响，然后来看一看相关的计算。 以下假设假设贸易市场上 AAA 的价格为 aaa，BBB 的价格为 bbb，假设 TOTTOTTOT 用 AperBA\\mathop{per} BAperB 计算，此时对于贸易市场来说，TOT=TOTAperB=abTOT=TOT_{A\\mathop{per}B}=\\frac{a}{b}TOT=TOTAperB​=ba​ 例子：如何生产使得收益最大化 贸易市场的价格可以用一根斜率确定、截距不定的直线在 PPC 图像（纵轴为 AAA，横轴为 BBB）上表示出来，这条直线的斜率就是 −TOT-TOT−TOT 为了让利益最大化，我们平移这条直线，让他和 PPC 产生交点，对于每一个交点计算收益，取最大值即可。 从交点倒推 TOT 和市场价格 一个很 tricky 的点是，图像上 PPC 的斜率是 −Copp(B)-C_{opp}(B)−Copp​(B)，但市场的直线的斜率是 −TOTAperB-TOT_{A\\mathop{per}B}−TOTAperB​。记得取倒数。 最大化组合消费 通常会问，若消费 nnn 单位的 AAA，那么最多能消费多少 BBB？ 我们把这个过程转化为，X,YX,YX,Y 两人先生产，通过贸易市场换成钱，再用钱在市场上买所需的物品。这里不考虑成“生产后的东西先拿出一部分满足消费”，是因为这两个思路是等价的 计算：在这个市场下，通过交易最多能赚多少钱 通过计算 TOTTOTTOT，判断出每一个应该生产什么（贸易市场的 TOTAperBTOT_{A\\mathbb{per}B}TOTAperB​ 更大，则生产 AAA；否则生产 BBB） 把生产出来的东西卖成钱 先购买需要消费的东西 然后就能计算最多能买多少了"},{"title":"VAE: Variational AutoEncoders","path":"/wiki/multimodal/VAE.html","content":"Representation 图像生成模型的本质是一个概率模型：如果我们知道了真实图像 xxx 的分布规律 p(x)p(x)p(x)，那么我们只需要从这个分布里随便采样 x′∼p(x)x&#x27;\\sim p(x)x′∼p(x)，那么 x′x&#x27;x′ 就是我们想要生成的图像。 不过通常，p(x)p(x)p(x) 很难表示和学习。我们考虑通过两个步骤生成图像： 先生成图片的特征，例如想要生成二次元图片，就先指定 tags 例如发色、动作等等 在根据特征，去生成图像 我们用 zzz 表示图像的“特征” (latent variable)，那么这样的过程就是如同下面所示 z⟶guidex\\boxed{z}\\overset{\\text{guide}}{\\longrightarrow} \\boxed{x} z​⟶guide​x​ 用数学语言描述就是这样一个恒等式 p(x)=∑zp(x∣z)⋅p(z)p(x)=\\sum_z p(x|z)\\cdot p(z) p(x)=z∑​p(x∣z)⋅p(z) VAE 的推理从数学的角度也就变成了 Sample zzz from p(z)p(z)p(z) Sample xxx from p(x∣z)p(x|z)p(x∣z) 当然，由于我们的目的是简化 p(x)p(x)p(x) 的建模，因此我们通常假设 p(z)∼N(0,1)p(x∣z)∼N(μθ(z),Σθ(z))\\begin{aligned} p(z)&amp;\\sim \\mathcal N(0,1)\\\\ p(x|z)&amp;\\sim \\mathcal N(\\mu_\\theta(z),\\Sigma_\\theta(z)) \\end{aligned} p(z)p(x∣z)​∼N(0,1)∼N(μθ​(z),Σθ​(z))​ 其中 μθ(⋅),Σθ(⋅)\\mu_\\theta(\\cdot),\\Sigma_\\theta(\\cdot)μθ​(⋅),Σθ​(⋅) 是神经网络。 这里也不一定非得是正态分布，其他容易计算的分布也可以。简单起见直接用正态分布了 Inference Inferencing Objective Function 给定一个数据集 D={x1,x2,…,xm}\\mathcal D=\\{x^{1}, x^{2}, \\dots, x^{m}\\}D={x1,x2,…,xm}，模型训练目标就是，从数据集 D\\mathcal DD 里训练的图像分布 pθ(x)p_\\theta(x)pθ​(x) 和真实的图像分布 p(x)p(x)p(x) 尽可能接近。衡量两个分布接近程度可以用 KL 散度，即训练目标为最小化 KL 散度： min⁡θDKL(pθ(x)∥p(x))\\min_\\theta D_{KL}\\Big( p_\\theta(x) \\big\\| p(x) \\Big) θmin​DKL​(pθ​(x)∥∥∥​p(x)) 最小化 KL 散度等同于最大化 Marginal Log-Likelilhood log⁡pθ(x)\\log p_\\theta(x)logpθ​(x) over D\\mathcal DD max⁡θ∑xi∈Dlog⁡pθ(xi)=max⁡θ∑xi∈Dlog⁡(∑zpθ(xi,z))\\begin{aligned} &amp;\\max_\\theta \\sum_{x^{i} \\in\\mathcal D} \\log p_\\theta(x^{i})\\\\ =&amp;\\max_\\theta \\sum_{x^{i} \\in\\mathcal D} \\log \\Big(\\sum_z p_\\theta(x^{i},z)\\Big) \\end{aligned} =​θmax​xi∈D∑​logpθ​(xi)θmax​xi∈D∑​log(z∑​pθ​(xi,z))​ 然而，zzz 是高维空间的隐变量，∑z\\sum_z∑z​ 需要遍历所有可能的 zzz、计算 pθ(xi,z)p_\\theta(x^{i},z)pθ​(xi,z)、再相加，几乎是不可能做到的，我们只能用各种方法去近似求解 log-likelihood via Monte Carlo 我们随机采样一些 zi∼p(z)z^{i} \\sim p(z)zi∼p(z)，用这些采样的 ziz^{i}zi 计算平均值： log⁡pθ(x)≈log⁡1k∑i=1kp(x∣zi),zi∼p(z)\\log p_\\theta(x)\\approx \\log \\frac{1}{k}\\sum_{i=1}^k p(x|z^{i}), \\quad z^{i}\\sim p(z) logpθ​(x)≈logk1​i=1∑k​p(x∣zi),zi∼p(z) 尽管理论上，蒙特卡洛估计方法是 no-bias 的，但是在实战中，用蒙特卡洛计算出来的梯度具有很大的方差。 via Importance Sampling 比起直接 maximize 目标，我们也可以构造出目标的 lower bound 然后通过 maximize 这个 lower bound 从而 maximize 目标。 此处，log⁡pθ(x)\\log p_\\theta(x)logpθ​(x) 的一个下界被称为 ELBO (Evidence Lower Bound) pθ(x)=∑zq(z)q(z)pθ(x,z)=∑zq(z)⋅pθ(x,z)q(z)=Ez∼q(z)[pθ(x,z)q(z)]log⁡pθ(x)=log⁡Ez∼q(z)[pθ(x,z)q(z)]=log⁡∑zq(z)⋅pθ(x,z)q(z)≥∑zq(z)⋅log⁡pθ(x,z)q(z)‾by Jensen’s Inequality=Ez∼q(z)[log⁡pθ(x,z)q(z)]≔ELBO(x;θ)=Lθ(x)\\begin{aligned} p_\\theta(x) &amp;=\\sum_z \\frac{q(z)}{q(z)}p_\\theta(x,z)\\\\ &amp;=\\sum_z q(z)\\cdot \\frac{p_\\theta(x,z)}{q(z)}\\\\ &amp;=\\mathbb E_{z\\sim q(z)}\\Big[\\frac{p_\\theta(x,z)}{q(z)}\\Big]\\\\ \\log p_\\theta(x)&amp;=\\log \\mathbb E_{z\\sim q(z)}\\Big[ \\frac{p_\\theta(x,z)}{q(z)} \\Big]\\\\ &amp;= \\log \\sum_z q(z)\\cdot \\frac{p_\\theta(x,z)}{q(z)}\\\\ &amp;\\ge \\underset{\\scriptsize\\text{by Jensen&#x27;s Inequality}}{\\underline{\\sum_z q(z)\\cdot \\log\\frac{p_\\theta(x,z)}{q(z)}}}\\\\ &amp;=\\mathbb E_{z\\sim q(z)}\\Big[ \\log\\frac{p_\\theta(x,z)}{q(z)} \\Big]\\\\ &amp;\\coloneqq \\text{ELBO}(x;\\theta)=\\mathcal L_{\\theta}(x) \\end{aligned} pθ​(x)logpθ​(x)​=z∑​q(z)q(z)​pθ​(x,z)=z∑​q(z)⋅q(z)pθ​(x,z)​=Ez∼q(z)​[q(z)pθ​(x,z)​]=logEz∼q(z)​[q(z)pθ​(x,z)​]=logz∑​q(z)⋅q(z)pθ​(x,z)​≥by Jensen’s Inequalityz∑​q(z)⋅logq(z)pθ​(x,z)​​​=Ez∼q(z)​[logq(z)pθ​(x,z)​]:=ELBO(x;θ)=Lθ​(x)​ 从 KL 散度的视角理解 ELBO 而实际上 log⁡pθ(x)=Ez∼q(z)[log⁡pθ(x,z)]+H(q)entropy of q(z)‾\\log p_\\theta(x)=\\mathbb E_{z\\sim q(z)}\\Big[ \\log p_\\theta(x,z) \\Big] + \\underset{\\overline{\\scriptsize\\text{entropy of }q(z)}}{H(q)} logpθ​(x)=Ez∼q(z)​[logpθ​(x,z)]+entropy of q(z)​H(q)​ 直觉上理解，我们选取的 q(z)q(z)q(z) 应该同模型从图像出发对特征的预测接近，即 DKL(q(z)∥pθ(z∣x))D_{KL}\\Big( q(z) \\big\\| p_\\theta(z|x) \\Big) DKL​(q(z)∥∥∥​pθ​(z∣x)) 越小越好。而 DKL()D_{KL}()DKL​() 具有非负性，移项后便是 ELBO 的形式。一般形式的，也有 log⁡pθ(x)=ELBO+DKL(q(z)∥pθ(z∣x))\\log p_\\theta(x)=\\text{ELBO}+D_{KL}\\Big( q(z)\\big\\|p_\\theta(z|x) \\Big) logpθ​(x)=ELBO+DKL​(q(z)∥∥∥​pθ​(z∣x)) 然后我们就又可以用 Monte Carlo 方法估计 ELBO 了。 ELBO(x;θ)≈1k∑i=1klog⁡pθ(x,zi)q(zi),zi∼q(z)\\text{ELBO}(x;\\theta)\\approx \\frac{1}{k}\\sum_{i=1}^k \\log\\frac{p_\\theta(x,z^{i})}{q(z^{i})},\\quad z^{i}\\sim q(z) ELBO(x;θ)≈k1​i=1∑k​logq(zi)pθ​(x,zi)​,zi∼q(z) VAE: Decoder 与 Encoder from Decoder to Encoder: Variational Inference 到目前位置，我们实际上只讨论了 Decoder 部分：pθ(x∣z)p_\\theta(x|z)pθ​(x∣z)。为了训练模型，我们肯定还需要 x→zx\\to zx→z 的推理与训练。这就是 VAE 里 Encoder 的作用。 Encoder 负责的就是 pθ(z∣x)p_\\theta(z|x)pθ​(z∣x)，但是 pθ(z∣x)p_\\theta(z|x)pθ​(z∣x) 很难从神经网络模型中推导出来。不过，根据上文对 ELBO 与 KL 散度 DKL(q(z)∥pθ(z∣x))D_{KL}\\Big( q(z)\\big\\| p_\\theta(z|x) \\Big)DKL​(q(z)∥∥∥​pθ​(z∣x)) 的分析，我们其实也可以通过优化 q(z)q(z)q(z) 让其近似 pθ(z∣x)p_\\theta(z|x)pθ​(z∣x) 来达成相同的目的。 所以，我们把 q(z)q(z)q(z) 也用神经网络建模为 qϕ(z)q_\\phi(z)qϕ​(z)，其中 ϕ\\phiϕ 为 Encoder 模型的参数，此时 ELBO 改写为 ELBO=Lθ,ϕ(x)=∑zqϕ(z)log⁡pθ(z,x)+H(qϕ(z))\\text{ELBO}=\\mathcal L_{\\theta,\\phi}(x)=\\sum_z q_\\phi(z)\\log p_\\theta(z,x)+H(q_\\phi(z)) ELBO=Lθ,ϕ​(x)=z∑​qϕ​(z)logpθ​(z,x)+H(qϕ​(z)) Amortized Inference 注意：这里的 Decoder 与 Encoder 本质上是对分布进行建模，即给定张量，输出一个分布。 如果 Encoder 部分我们为每一个输入的图像都训练一个 Encoder qϕ(z)q_\\phi(z)qϕ​(z)，计算代价无法承受。 因此，我们用神经网络对分布进行拟合，即 gλ:xi↦qϕi(z)g_\\lambda:x^{i} \\mapsto q_{\\phi^{i}}(z)gλ​:xi↦qϕi​(z)，这样就可以避免反复求解 ϕi\\phi^{i}ϕi 了。 而对 Decoder 部分就不用了，因为 pθ(x∣z)p_\\theta(x|z)pθ​(x∣z) 的 zzz 是由 Encoder 完成的，而每一个而 Encoder 总是输出的 qϕ(z)≈pθ(z∣x)q_\\phi(z)\\approx p_\\theta(z|x)qϕ​(z)≈pθ​(z∣x) 总是映射到同一个 random variable space 里. Training VAE 有一个 Encoder 架构，负责将图像 xxx 编码为 latent variable zzz；Decoder 架构则负责从 latent variable zzz 生成出图像 xxx. VAE 上文的 ELBO 则为我们优化 VAE 模型提供了一个良好的目标函数：（其实应该是求解上文的 λ\\lambdaλ） max⁡θ,ϕELBO=max⁡θ∑x∈Dmax⁡ϕEqϕ(z)[log⁡pθ(z,x)qϕ(z)]⇒max⁡θ,λ∑x∈Dmax⁡λEgλ(x)[log⁡pθ(z,x)gλ(x)]\\begin{aligned} \\max_{\\theta,\\phi}\\text{ELBO}&amp;=\\max_{\\theta}\\sum_{x\\in\\mathcal D}\\max_{\\phi}\\mathbb E_{q_\\phi(z)}\\Bigg[ \\log\\frac{p_\\theta(z,x)}{q_\\phi(z)} \\Bigg]\\\\ &amp;\\Rightarrow\\max_{\\theta,\\lambda}\\sum_{x\\in\\mathcal D}\\max_\\lambda\\mathbb E_{g_\\lambda(x)}\\Bigg[ \\log\\frac{p_\\theta(z,x)}{g_\\lambda(x)} \\Bigg] \\end{aligned} θ,ϕmax​ELBO​=θmax​x∈D∑​ϕmax​Eqϕ​(z)​[logqϕ​(z)pθ​(z,x)​]⇒θ,λmax​x∈D∑​λmax​Egλ​(x)​[loggλ​(x)pθ​(z,x)​]​ Stochastic Variational Inference 用随机梯度下降法进行学习 初始化 θ,ϕ1…m\\theta,\\phi^{1\\dots m}θ,ϕ1…m 随机一个 xi∈Dx^{i} \\in\\mathcal Dxi∈D 先优化 ϕi\\phi^{i}ϕi： ϕi←ϕi+η∇ϕiLθ,ϕ(xi)\\phi^{i}\\gets \\phi^{i}+\\eta abla_{\\phi^{i}}\\mathcal L_{\\theta,\\phi}(x^{i})ϕi←ϕi+η∇ϕi​Lθ,ϕ​(xi) 直到收敛为止 更新 θ\\thetaθ：θ←θ+η∇θLθ,ϕi(xi)\\theta\\gets\\theta+\\eta abla_{\\theta}\\mathcal L_{\\theta,\\phi^{i}}(x^{i})θ←θ+η∇θ​Lθ,ϕi​(xi)。回到 step 2 继续执行。 那么我们如何计算梯度呢？因为很有可能这个式子并不存在 closed form，我们依然采用 Monte Carlo 的方法解决问题，即 Eqϕ(z)[log⁡pθ(z,x)−log⁡qϕ(z)]≈1K∑i=1Klog⁡pθ(zi,x)−log⁡qϕ(zi)\\mathbb E_{q_\\phi(z)}\\Bigg[ \\log p_\\theta(z,x)-\\log q_\\phi(z) \\Bigg]\\approx \\frac{1}{K}\\sum_{i=1}^{K}\\log p_\\theta(z^{i},x)-\\log q_\\phi(z^{i}) Eqϕ​(z)​[logpθ​(z,x)−logqϕ​(z)]≈K1​i=1∑K​logpθ​(zi,x)−logqϕ​(zi) 其中 qϕ(z)q_\\phi(z)qϕ​(z) 应该容易采样和计算。由此，ELBO 关于 θ\\thetaθ 的导数即为 ∇θEqϕ(z)[log⁡pθ(z,x)−log⁡qϕ(z)]≈1K∑i=1K∇θlog⁡pθ(zi,x) abla_\\theta \\mathbb E_{q_\\phi(z)}\\Bigg[ \\log p_\\theta(z,x)-\\log q_\\phi(z) \\Bigg]\\approx \\frac{1}{K}\\sum_{i=1}^K abla_\\theta \\log p_\\theta(z^{i},x) ∇θ​Eqϕ​(z)​[logpθ​(z,x)−logqϕ​(z)]≈K1​i=1∑K​∇θ​logpθ​(zi,x) 然而 ELBO 关于 ϕi\\phi^iϕi 的导数不那么好算，因为期望本身依赖于这个参数。一般而言，可以使用强化学习的方法进行学习，也可以使用 Reparameterization 的方法。 Reparam 我们把 qϕ(z)∼N(μ,σ2I)q_\\phi(z)\\sim \\mathcal N(\\mu, \\sigma^2 I)qϕ​(z)∼N(μ,σ2I)，即 ϕi=(μ,σ)\\phi^i=(\\mu,\\sigma)ϕi=(μ,σ)，那么从这个正态分布采样就等同于 ϵ∼N(0,1)z=μ+σϵ=gϕ(ϵ)\\epsilon\\sim \\mathcal N(0,1)\\\\ z=\\mu+\\sigma\\epsilon=g_\\phi(\\epsilon) ϵ∼N(0,1)z=μ+σϵ=gϕ​(ϵ) 借用这个想法，我们可以改写 ELBO，这里先让 r(z)=log⁡qϕ(z)r(z)=\\log q_\\phi(z)r(z)=logqϕ​(z) 简化计算，稍后再代入 Ez∼qϕ(z)[r(z)]=∑zqϕ(z)r(z)=Eϵ∼N(0,1)[r(gϕ(ϵ))]=∫N(ϵ)r(μ+σϵ)dϵ∇ϕEqϕ(z)[r(z)]=∇ϕEϵ[r(gϕ(ϵ))]=Eϵ[∇ϕr(gϕ(ϵ))]≈1K∑i=1Kr(gϕ(ϵi))‾Monte Carlo Estim\\begin{aligned} \\mathbb E_{z\\sim q_\\phi(z)}[r(z)]&amp;=\\sum_z q_\\phi(z)r(z)\\\\ &amp;=\\mathbb E_{\\epsilon\\sim\\mathcal N(0,1)}[r(g_\\phi(\\epsilon))]\\\\ &amp;=\\int \\mathcal N(\\epsilon) r(\\mu+\\sigma\\epsilon) d\\epsilon\\\\ abla_\\phi \\mathbb E_{q_\\phi(z)}[r(z)]&amp;= abla_\\phi \\mathbb E_\\epsilon [r(g_\\phi(\\epsilon))]\\\\ &amp;=\\mathbb E_{\\epsilon}[ abla_\\phi r(g_\\phi(\\epsilon))]\\\\ &amp;\\approx \\underset{\\text{Monte Carlo Estim}}{\\underline{\\frac{1}{K}\\sum_{i=1}^K r(g_\\phi(\\epsilon^i))}} \\end{aligned} Ez∼qϕ​(z)​[r(z)]∇ϕ​Eqϕ​(z)​[r(z)]​=z∑​qϕ​(z)r(z)=Eϵ∼N(0,1)​[r(gϕ​(ϵ))]=∫N(ϵ)r(μ+σϵ)dϵ=∇ϕ​Eϵ​[r(gϕ​(ϵ))]=Eϵ​[∇ϕ​r(gϕ​(ϵ))]≈Monte Carlo EstimK1​i=1∑K​r(gϕ​(ϵi))​​​"},{"title":"ViT","path":"/wiki/multimodal/ViT.html","content":"372f7a9688aae3bb4ec6e6afab19ca439bb95e6d91164a50dda5c3f9a4f813ff Password is needed."},{"title":"LightRAG","path":"/wiki/rag/LightRAG.html","content":"LightRAG LightRAG"},{"title":"基于 Nano GraphRAG 的二次开发","path":"/wiki/rag/build-on-nano-graphrag.html","content":"本地部署 LLM"},{"title":"Dify 中使用 HTTP Request 配合 Flask 进行高级数据处理","path":"/wiki/rag/custom-data-process.html","content":"Flask Server"},{"title":"GraphRAG 解读","path":"/wiki/rag/graph-rag.html","content":"算法流程 GraphRAG WorkFlow Overview NanoGraphRAG 工作流程 Chunk Documents graph LR A(Document) --> B(Text Chunk 1) A --> C(Text Chunk 2) 像普通的 VectorRAG 一样，提取出来的 Text Chunk 可以用于后续 LLM 的知识来源。 NanoGraphRAG 源码解读 Extract Entity and Relationships Graph Indexing Graph Decomposition"},{"title":"NanoGraphRAG项目思路（一）：文档分块","path":"/wiki/rag/nano-graph-rag-p1.html","content":"Phase 1: 文档预处理 (论文 3.1.1) 在 Nano GraphRAG 里，预处理文档的任务在 GraphRAG.ainsert() 中完成。GraphRAG.ainsert() 由 GraphRAG.insert() 调用，并且使用了 asyncio.get_event_loop() 保证执行完毕。 文档去重 ainsert() 首先计算文档的 MD5 哈希值，检查文档是否已经添加过了。如果已经添加过了，那么就不用再插入数据库。 new_docs 首先准备所有待检查的字符串，计算其哈希值。 1234new_docs = &#123; compute_mdhash_id(c.strip(), prefix=&quot;doc-&quot;): &#123;&quot;content&quot;: c.strip()&#125; for c in string_or_strings&#125; 然后丢入 self.full_docs 检查是否有重复的字符串并剔除 1_add_doc_keys = await self.full_docs.filter_keys(list(new_docs.keys())) 剔除完成后的字符串拼接成 &#123; hash: str &#125; 形式的字典重新赋值回 new_docs. 1new_docs = &#123;k: v for k, v in new_docs.items() if k in _add_doc_keys&#125; 如果没有新文档，直接退出。然后输出一点调试日志 1234if not len(new_docs): logger.warning(f&quot;All docs are already in the storage&quot;) returnlogger.info(f&quot;[New Docs] inserting &#123;len(new_docs)&#125; docs&quot;) 文档分块 Text Chunk 否则，准备插入文档。先对文档 get_chunk() 拆分成若干个 text chunks 12345678910111213inserting_chunks = get_chunks( new_docs=new_docs, # 要切分的文档 chunk_func=self.chunk_func, # chunk 切分方法，默认按 token 数量 overlap_token_size=self.chunk_overlap_token_size, # chunk 之间重叠的 token 数量，充当上下文的作用 max_token_size=self.chunk_token_size, # 一个 chunk 最多多少 token) 然后 get_chunk() 加载 Tokenizer (这里调用的是 OpenAI tiktoken 的库)，将 docs（纯文本）转化为 Tokens，再调用 chunk_func 进行切块，每一块都标注成 123456&#123; &quot;tokens&quot;: token 个数, &quot;content&quot;: token 内容（已经不是文本了）, &quot;chunk_order_index&quot;: 是第几个 chunk, &quot;full_doc_id&quot;: 所属文档的哈希值,&#125; 标注完成后，再为每一个 text chunk 计算哈希值 chunk-xxxxx，放入 inserting_chunk 并返回 Text Chunk 去重 对 text chunks 也进行去重 12345678910111213_add_chunk_keys = await self.text_chunks.filter_keys( list(inserting_chunks.keys()))inserting_chunks = &#123; k: v for k, v in inserting_chunks.items() if k in _add_chunk_keys&#125;if not len(inserting_chunks): logger.warning(f&quot;All chunks are already in the storage&quot;) returnlogger.info(f&quot;[New Chunks] inserting &#123;len(inserting_chunks)&#125; chunks&quot;)if self.enable_naive_rag: logger.info(&quot;Insert chunks for naive RAG&quot;) await self.chunks_vdb.upsert(inserting_chunks) 和文档去重的逻辑很相似，就是在 KVStorage 里按哈希值查找，去重 更新数据库 这里有另一点比较重要的是，如果需要插入新的 chunk，那么首先需要把 self.community_reports 清空。因为插入新块后，可能这个文档的 community 就会改变 1await self.community_reports.drop()"},{"title":"HMM","path":"/wiki/rl/HMM.html","content":"Hidden Markov Model 以下约定： 大写字母表示随机变量，脚标表示时刻，小写字母表示变量具体的取值 Distri(Xt)\\text{Distri}(X_t)Distri(Xt​) 表示第 ttt 时刻的状态（的分布），xtx_txt​ 表示第 ttt 时刻具体的状态（例如具体的位置） Distri(Et)\\text{Distri}(E_t)Distri(Et​) 表示第 ttt 时刻的观测（的分布），ete_tet​ 表示第 ttt 时刻观测到的具体的值 HMM 主要有三个概率模型： Component Distribution Initial Distribution Distri(X0)\\text{Distri}(X_0)Distri(X0​) Transition Model Distri(Xt+1∣xt)\\text{Distri}(X_{t+1} | x_t)Distri(Xt+1​∣xt​) Observation Model Distri(Et∣xt)\\text{Distri}(E_t | x_t)Distri(Et​∣xt​) Model 因此，利用 Bayes Net 的概率分析方法，我们可以得出，整个网络的概率模型 P(x0,e1,x1,e2,x2,…,et,xt)=P(x0)⋅∏tP(xt∣xt−1)⋅P(et∣xt)P(x_0,e_1,x_1,e_2,x_2,\\dots, e_t,x_t)=P(x_0)\\cdot \\prod_t P(x_t|x_{t-1})\\cdot P(e_t|x_t) P(x0​,e1​,x1​,e2​,x2​,…,et​,xt​)=P(x0​)⋅t∏​P(xt​∣xt−1​)⋅P(et​∣xt​) Filtering 给定所有时刻的观测，Filter 关注当前时刻的状态，即求解 P(xt∣e1:t)P(x_t|e_{1:t}) P(xt​∣e1:t​) Filter Forward Filtering P(xt∣e1:t)=P(xt∣et,e1:t−1)=P(xt,et,e1:t−1)P(e1:t)=P(xt,et∣e1:t−1)⋅P(e1:t−1)P(e1:t)=α⋅P(xt,et∣e1:t−1)=α⋅∑xt−1P(xt,et,xt−1∣e1:t−1)=α⋅∑xt−1P(et∣xt,xt−1,e1:t−1)⋅P(xt∣xt−1,e1:t−1)⋅P(xt−1∣e1:t−1)=α⋅∑xt−1P(et∣xt)⋅P(xt∣xt−1)⋅P(xt−1∣e1:t−1)=α⋅P(et∣xt)⋅∑xt−1P(xt∣xt−1)⋅P(xt−1∣e1:t−1)\\begin{aligned} P(x_t|e_{1:t})&amp;=P(x_t|e_t,e_{1:t-1})\\\\ &amp;=\\frac{P(x_t,e_t,e_{1:t-1})} {P(e_{1:t})}\\\\ &amp;=\\frac{P(x_t,e_t|e_{1:t-1})\\cdot P(e_{1:t-1})}{P(e_{1:t})}\\\\ &amp;=\\alpha\\cdot P(x_t,e_t|e_{1:t-1})\\\\ &amp;=\\alpha\\cdot \\sum_{x_{t-1}} P(x_t,e_t,x_{t-1}|e_{1:t-1})\\\\ &amp;=\\alpha\\cdot\\sum_{x_{t-1}}P(e_t|x_t,x_{t-1},e_{1:t-1})\\cdot P(x_t|x_{t-1},e_{1:t-1})\\cdot P(x_{t-1}|e_{1:t-1})\\\\ &amp;=\\alpha\\cdot\\sum_{x_{t-1}}P(e_t|x_t)\\cdot P(x_t|x_{t-1})\\cdot P(x_{t-1}|e_{1:t-1})\\\\ &amp;=\\alpha\\cdot P(e_t|x_t)\\cdot\\sum_{x_{t-1}}P(x_t|x_{t-1})\\cdot P(x_{t-1}|e_{1:t-1}) \\end{aligned} P(xt​∣e1:t​)​=P(xt​∣et​,e1:t−1​)=P(e1:t​)P(xt​,et​,e1:t−1​)​=P(e1:t​)P(xt​,et​∣e1:t−1​)⋅P(e1:t−1​)​=α⋅P(xt​,et​∣e1:t−1​)=α⋅xt−1​∑​P(xt​,et​,xt−1​∣e1:t−1​)=α⋅xt−1​∑​P(et​∣xt​,xt−1​,e1:t−1​)⋅P(xt​∣xt−1​,e1:t−1​)⋅P(xt−1​∣e1:t−1​)=α⋅xt−1​∑​P(et​∣xt​)⋅P(xt​∣xt−1​)⋅P(xt−1​∣e1:t−1​)=α⋅P(et​∣xt​)⋅xt−1​∑​P(xt​∣xt−1​)⋅P(xt−1​∣e1:t−1​)​ Prediction 给定所有时刻的观测，希望知道后续时刻的状态，因为后续时刻的状态还没有被观测，因此是 Prediction。即求解 P(xt+k∣e1:t),k&gt;0P(x_{t+k}|e_{1:t}),\\quad k\\gt 0 P(xt+k​∣e1:t​),k&gt;0 Prediction Smoothing Smoothing 关注的给定所有时刻的观测，更新过去的状态（Filter 关注的是当前状态），即求解 P(xk∣e1:t),k&lt;tP(x_k|e_{1:t}),\\quad k\\lt t P(xk​∣e1:t​),k&lt;t Smoothing Explanation 给定所有时刻的观测 e1:te_{1:t}e1:t​，我们希望找到这样一组 sequential state x1:tx_{1:t}x1:t​，这一组 x1:tx_{1:t}x1:t​ 最有可能产生这些观测。 arg max⁡x1:tP(x1:t∣e1:t)\\argmax_{x_{1:t}} P(x_{1:t}|e_{1:t}) x1:t​argmax​P(x1:t​∣e1:t​) Explanation CS188 Project 4 解析 CS188 Project 4 是用 HMM, Bayes Net, Particle Filter 的概念完成 Pacman 小游戏"},{"title":"Actor Critic Methods","path":"/wiki/rl/actor-critic.html","content":"Actor-Critic Methods Actor-Critic 是一种强化学习方法，强调的是：LLM 作为动作执行者 Actor，其输出的 Token 经过第三方评判之后 (Critic)，得到 Loss 并用于优化自身的模型。 Advantage Function Advantage Function 定义为在当前状态 sts_tst​ 下采取某个动作 ata_tat​ 能够带来的 Reward Aπθ(st,at)=Qπθ(st,at)−Vπθ(st)A^{\\pi_\\theta}(s_t,a_t)=Q^{\\pi_\\theta}(s_t,a_t)-V^{\\pi_\\theta}(s_t) Aπθ​(st​,at​)=Qπθ​(st​,at​)−Vπθ​(st​) 有了这个定义之后，我们可以等价改写 Policy Gradient Objective Function 了： ∇θJ(θ)=Eτ∼πθ[∑t=0T∇θlog⁡πθ(at∣st)Aπθ(st,at)] abla_\\theta J(\\theta)=\\mathbb E_{\\tau\\sim\\pi_\\theta}\\Big[ \\sum_{t=0}^T abla_\\theta \\log\\pi_\\theta(a_t|s_t)A^{\\pi_\\theta}(s_t,a_t) \\Big] ∇θ​J(θ)=Eτ∼πθ​​[t=0∑T​∇θ​logπθ​(at​∣st​)Aπθ​(st​,at​)] Generalized Advantage Estimation"},{"title":"GRPO 算法","path":"/wiki/rl/deepseek-math.html","content":"GRPO GRPO 算法在 GAE 的基础上更进一步：既然计算 VπθV^{\\pi_\\theta}Vπθ​ 和估算 A^πθ\\hat A^{\\pi_\\theta}A^πθ​ 那么费劲，那么我直接不计算了，我直接使用 Monte Carlo 方法对 A^π\\hat A^\\piA^π 进行估计"},{"title":"Markov","path":"/wiki/rl/markov-model.html","content":"Markov Model 符号约定： 当前 state sts_tst​ 采样于机器人当前的 state space St\\mathbb S_tSt​ 机器人可以采取的 action ata_tat​ 采样于当前的 action space At\\mathbb A_tAt​ 机器人在 state sss，采取 action aaa，进入新 state s′s&#x27;s′，可以得到 Reward rrr。这个 rrr 由奖励模型 R(s,a,s′)R(s,a,s&#x27;)R(s,a,s′) 给出"},{"title":"PPO 算法","path":"/wiki/rl/ppo.html","content":"PPO 算法 PPO 想处理的一个问题是，在 Actor-Critic 架构下，我们训练的过程是：πt−1\\pi_{t-1}πt−1​ 里计算 mmm 步 Trajectory 用于更新 πt\\pi_tπt​，这样的一个显著的问题就是计算开销太大，每一次都需要用刚刚训练好的大模型进行采样（而且大模型通常 32B 参数以上，训练成本极大） 于是一个想法就是，我们能否将采样的 mmm 步 Trajectory 给缓存下来呢？ Importance Sampling Importance Sampling 的核心是，从已知的分布里进行采样 E(st,at)∼πθ[∇θlog⁡πθ(at∣st)Aπθ(st,at)]= E(st,at)∼πref[πθ(at∣st)πref(at∣st)∇θlog⁡πθ(at∣st)Aπθ(st,at)]\\begin{aligned} &amp;\\mathbb E_{(s_t,a_t)\\sim \\pi_\\theta}\\Big[ abla_\\theta\\log\\pi_\\theta(a_t|s_t)A^{\\pi_\\theta}(s_t,a_t) \\Big]\\\\ =\\ &amp;\\mathbb E_{(s_t,a_t)\\sim \\boxed{\\pi_{ref}}}\\Big[ \\boxed{\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{ref}(a_t|s_t)}} abla_\\theta\\log\\pi_\\theta(a_t|s_t)A^{\\pi_\\theta}(s_t,a_t) \\Big] \\end{aligned} = ​E(st​,at​)∼πθ​​[∇θ​logπθ​(at​∣st​)Aπθ​(st​,at​)]E(st​,at​)∼πref​​​[πref​(at​∣st​)πθ​(at​∣st​)​​∇θ​logπθ​(at​∣st​)Aπθ​(st​,at​)]​ 直观上理解的话，我们从 πref\\pi_{ref}πref​ 采样出来的 Trajectory 显然不能完全以相同权重直接用于更新后几步的 Policy πθ\\pi_{\\theta}πθ​，因此，用 πθ(at∣st)πref(at∣st)\\pi_\\theta(a_t|s_t)\\over{\\pi_{ref}(a_t|s_t)} πref​(at​∣st​)πθ​(at​∣st​)​ 对每一步 Trajectory 的权重进行调整后再反向传播更新参数。 Training Constraint: KL Penalty Training Constraint: Clip Objective"},{"title":"Q Learning","path":"/wiki/rl/q-learning.html","content":"Q-Learning 我们的 Agent 每次从环境接收 transition=(s,a,r,s′)\\texttt{transition}=(s,a,r,s&#x27;)transition=(s,a,r,s′) 的反馈，以此进行学习。由于无法建模出转移概率 T(s,a,s′)T(s,a,s&#x27;)T(s,a,s′)，我们用采样的方式（蒙特卡洛）来训练 Qt+1(s,a)←(1−α)Qt(s,a)+α[R(s,a,s′)+γmax⁡a′Qt(s′,a′)]Q_{t+1}(s,a) \\gets (1-\\alpha)Q_{t}(s,a)+\\alpha\\Big[ R(s,a,s&#x27;)+\\gamma \\max_{a&#x27;} Q_t(s&#x27;,a&#x27;) \\Big] Qt+1​(s,a)←(1−α)Qt​(s,a)+α[R(s,a,s′)+γa′max​Qt​(s′,a′)] 这个式子也可以等价地写作 Qt+1(s,a)←Qt(s,a)+α[r+γmax⁡a′Qt(s′,a′)−Qt(s,a)]Q_{t+1}(s,a)\\gets Q_t(s,a)+\\alpha\\Big[ r+\\gamma \\max_{a&#x27;}Q_t(s&#x27;,a&#x27;)-Q_t(s,a) \\Big] Qt+1​(s,a)←Qt​(s,a)+α[r+γa′max​Qt​(s′,a′)−Qt​(s,a)] Approx Q-Learning 在 Approx Q-Learning 算法里，我们把 Q(s,a)Q(s,a)Q(s,a) 分解为多个关于 state sss 和行动 aaa 的 feature\\tt featurefeature 之线性组合（feature\\tt featurefeature 不一定需要和 s,as,as,a 成线性） Q(s,a)=∑iwi×fi(s,a)\\boxed {Q(s,a)=\\sum olimits_i w_i \\times f_i(s,a)} Q(s,a)=∑i​wi​×fi​(s,a)​ 令当次从环境的采样为 (s,a,s′,r)(s,a,s&#x27;,r)(s,a,s′,r)，表示从状态 sss 执行动作 aaa 转移到状态 s′s&#x27;s′ 得到奖励 rrr，定义 Sample Difference 为 Δ=r+γmax⁡a′Q(s′,a′)−Q(s,a)\\Delta=r+\\gamma\\max_{a&#x27;}Q(s&#x27;,a&#x27;)-Q(s,a) Δ=r+γa′max​Q(s′,a′)−Q(s,a) feature weights\\texttt{feature weights}feature weights 的更新则为 wi←wi+α×Δ×fi(s,a)w_i\\gets w_i+\\alpha\\times\\Delta\\times f_i(s,a) wi​←wi​+α×Δ×fi​(s,a) 推导 核心公式 GetAction()π(s)=arg max⁡aQ(s,a)UpdateWeights()wi←wi+α×[Δ]×fi(s,a)\\begin{array}{|r|cl|} \\hline \\text{GetAction()}&amp;\\pi(s)&amp;=\\argmax_{a}Q(s,a)\\\\ \\hline \\text{UpdateWeights()}&amp;w_i&amp;\\gets w_i+\\alpha\\times[\\Delta]\\times f_i(s,a)\\\\ \\hline \\end{array} GetAction()UpdateWeights()​π(s)wi​​=argmaxa​Q(s,a)←wi​+α×[Δ]×fi​(s,a)​​ 实现代码 12345678910111213141516171819class ApproxQLearningAgent(): def __init__(self): self.gamma = # reward discount rate self.alpha = # weight update factor self.epsilon = # learning rate self.weights = &#123; &quot;f1&quot;: 0.0, # correspond to w1*f1(s,a) &quot;f2&quot;: 0.0, # correspond to w2*f2(s,a) # ... &#125; def get_legal_actions(self): &#x27;&#x27;&#x27; 获取 &#x27;&#x27;&#x27; pass def"},{"title":"强化学习：Markov Chain 与贝尔曼方程","path":"/wiki/rl/rl-bellman-equation.html","content":"Markov Decision Process 因此进一步定义： 世界（环境）模型 T(s,a,s′)T(s,a,s&#x27;)T(s,a,s′)，表示++机器人在状态 sss 时，如果采取 aaa 行动，那么有 T(s,a,s′)T(s,a,s&#x27;)T(s,a,s′) 的概率进入状态 s′s&#x27;s′。++这个模型也就是机器人与环境交互的入口 奖励函数 R(s,a,s′)R(s,a,s&#x27;)R(s,a,s′)，表示如果机器人在状态 sss 时采取 aaa 行动并进入状态 s′s&#x27;s′，就能获得 R(s,a,s′)R(s,a,s&#x27;)R(s,a,s′) 的奖励。 我们希望，机器人在世界模型和奖励函数（这两个是事先给定的）中，学习到在某个环境下该采取何种行动这一个 objective，这样一个 objective 的数学本质是 π:S↦A\\pi:\\mathbb S\\mapsto\\mathbb Aπ:S↦A，即 π(s)=a\\pi(s)=aπ(s)=a，函数输入状态，输出该采取什么行动。我们把这个 π(s)\\pi(s)π(s) 称为 Policy How to Learn a Policy Evaluation 很显然，我们需要一个 criterion 才能评判一个 Policy 到底好不好。 当我们的机器人根据 πi()\\pi_i()πi​() 运行了一段时间后，会得到一连串的 Reward 和一个 Accumulative Reward，而由于世界模型是概率模型，因此同一个 πi()\\pi_i()πi​() 可能会产生不同的 Reward Sequence 和不同的 Accumulative Reward。 我们也要考虑步数的影响（不然机器人来回踱步刷分数），因此引入 Discount，在每一步的 Reward 上乘的衰减系数 γ\\gammaγ，表明 Reward 随着步数的增长而减少。 我们在此基础上定义，即为 Policy 的 Utility 为 Reward 的期望值。 State 的 Utility 为从这个 State sss 出发的 Expected Utility，即为 V(s)V(s)V(s)，用上标 ∗\\ast∗ 表示最优策略 MDP Search Tree MDP Search Tree Value of State 定义： Q-State 为机器人选择完行动后，但还没有执行（还没有转移到 s′s&#x27;s′）的中间状态。 根据这棵树的结构可以推导出 V∗(s)=max⁡aQ∗(s,a)Q∗(s,a)=∑s′T(s,a,s′)[R(s,a,s′)+γV∗(s′)]so we get…⟹V∗(s)=max⁡a∑s′T(s,a,s′)[R(s,a,s′)+γV∗(s′)]\\begin{aligned} V^\\ast(s)&amp;=\\max_a Q^\\ast(s,a)\\\\ Q^\\ast(s,a)&amp;=\\sum_{s&#x27;} T(s,a,s&#x27;)\\Big[R(s,a,s&#x27;)+\\gamma V^\\ast(s&#x27;)\\Big]\\\\ &amp;\\textbf{so we get}\\dots\\\\ \\Longrightarrow V^\\ast(s)&amp;=\\max_a \\sum_{s&#x27;}T(s,a,s&#x27;)\\Big[R(s,a,s&#x27;)+\\gamma V^\\ast(s&#x27;)\\Big] \\end{aligned} V∗(s)Q∗(s,a)⟹V∗(s)​=amax​Q∗(s,a)=s′∑​T(s,a,s′)[R(s,a,s′)+γV∗(s′)]so we get…=amax​s′∑​T(s,a,s′)[R(s,a,s′)+γV∗(s′)]​ 求解 Policy 从 V(s) 求解 policy 数值迭代算法 从 V0(s)=0V_0(s)=0V0​(s)=0 开始 用上一次的 Vt(s)V_t(s)Vt​(s) 更小当次的 Vt+1(s)V_{t+1}(s)Vt+1​(s) Vt+1(s)←max⁡a∑s′T(s,a,s′)[R(s,a,s′)+γVt(s′)]V_{t+1}(s)\\gets \\max_a\\sum_{s&#x27;} T(s,a,s&#x27;)\\Big[R(s,a,s&#x27;)+\\gamma V_t(s&#x27;) \\Big] Vt+1​(s)←amax​s′∑​T(s,a,s′)[R(s,a,s′)+γVt​(s′)] 这里的 γ\\gammaγ 表示步数的 Discount 直到收敛 V∗(s)V^\\ast(s)V∗(s)，时间复杂度 O(S2A)O(S^2A)O(S2A) 从 V∗(s)V^\\ast(s)V∗(s) 提取 Policy π∗(s)=arg max⁡a∑s′T(s,a,s′)[R(s,a,s′)+γV∗(s′)]\\pi^\\ast(s)=\\argmax_a\\sum_{s&#x27;} T(s,a,s&#x27;)\\Big[R(s,a,s&#x27;)+\\gamma V^\\ast(s&#x27;) \\Big] π∗(s)=aargmax​s′∑​T(s,a,s′)[R(s,a,s′)+γV∗(s′)] 为每一个 state 选择一个 action 策略迭代算法 当 Policy 固定为 πi()\\pi_i()πi​() 时，此时不用考虑最优策略，等同于不需要取 max⁡a\\max_amaxa​，因此从 state sss 出发的 expected utility 就只有单纯的求和了，为 Vπi(s)=∑s′T(s,πi(s),s′)[R(s,πi(s),s′)+γVπi(s′)]V^{\\pi_i}(s)=\\sum_{s&#x27;} T(s,\\pi_i(s),s&#x27;)\\Big[R(s,\\pi_i(s),s&#x27;)+\\gamma V^{\\pi_i}(s&#x27;)\\Big] Vπi​(s)=s′∑​T(s,πi​(s),s′)[R(s,πi​(s),s′)+γVπi​(s′)] Policy Evaluation. 为选定的 Policy 计算 Utility（非 Optimal Utility） Vt+1πi(s)←∑s′T(s,πi(s),s′)[R(s,πi(s),s′)+γVtπi(s′)]V_{t+1}^{\\pi_i}(s)\\gets \\sum_{s&#x27;}T(s,\\pi_i(s), s&#x27;)\\Big[R(s,\\pi_i(s),s&#x27;)+\\gamma V^{\\pi_i}_{t}(s&#x27;) \\Big] Vt+1πi​​(s)←s′∑​T(s,πi​(s),s′)[R(s,πi​(s),s′)+γVtπi​​(s′)] Policy Improvement. 优化 Policy πt+1(s)←arg max⁡a∑s′T(s,a,s′)[R(s,a,s′)+γVπi(s′)]\\pi_{t+1}(s)\\gets \\argmax_a \\sum_{s&#x27;} T(s,a,s&#x27;)\\Big[R(s,a,s&#x27;)+\\gamma V^{\\pi_i}(s&#x27;) \\Big] πt+1​(s)←aargmax​s′∑​T(s,a,s′)[R(s,a,s′)+γVπi​(s′)] 直到 Policy 收敛 从 Q∗(s,a)Q^\\ast(s,a)Q∗(s,a) 提取 Policy π∗(s)=arg max⁡aQ∗(s,a)\\pi^\\ast(s)=\\argmax_a Q^\\ast(s,a) π∗(s)=aargmax​Q∗(s,a) 从 Q-State 求解 Policy Q∗(s,a)=∑s′T(s,a,s′)[R(s,a,s′)+γmax⁡a′Q∗(s′,a′)]Q^\\ast(s,a)=\\sum_{s&#x27;} T(s,a,s&#x27;)\\Big[R(s,a,s&#x27;)+\\gamma\\max_{a&#x27;}Q^\\ast(s&#x27;,a&#x27;) \\Big] Q∗(s,a)=s′∑​T(s,a,s′)[R(s,a,s′)+γa′max​Q∗(s′,a′)] 提取 Policy： π∗(s)=arg max⁡aQ∗(s,a)\\pi^\\ast(s)=\\argmax_a Q^\\ast(s,a) π∗(s)=aargmax​Q∗(s,a) Summary","categories":[null]},{"title":"强化学习、与大语言模型","path":"/wiki/rl/rl-and-llm.html","content":"什么是强化学习 RL 强化学习在大模型语言里的应用 我们考虑大模型语言的生成过程： 123token1 token2 | token3 | token4 token5 ... ^ ^ ^ state action state(next) 如图所示，现阶段的大模型都是采用自回归生成的。我们可以将“大模型生成的前 t−1t-1t−1 个单词”视为状态 sts_tst​，那么生成第 ttt 个单词视作第 ttt 时刻采取的动作 ata_tat​，得到下一时刻的状态 st+1s_{t+1}st+1​。与上图进行对应的话：大模型本身就是智能体，环境和 Reward 可以抽象地理解为人类偏好（人类依据自己的偏好，给模型的输出打分，大模型的目标就是得分尽可能得高，对应着更加优秀的大模型）。 Trajectory 在普通的强化学习过程中，Trajectory 指的是一连串的 state, action, reward，记作 τ\\tauτ τ=(s0,a0,r1,s1,a1,r2,… )\\tau=(s_0,a_0,r_1,s_1,a_1,r_2,\\dots) τ=(s0​,a0​,r1​,s1​,a1​,r2​,…) Policy Gradient Theorem: Directly Learn Policy (LLM) 如果我们想直接学习 Policy π()\\pi()π() 的话，我们考虑把 π()\\pi()π() 参数化为函数 πθ()\\pi_\\theta()πθ​()，因此将 Objective Function 建模为梯度上升问题（这里的 ∼\\sim∼ 表示 trajectory 从 πθ\\pi_\\thetaπθ​ 中采样得到） J(θ)=Eτ∼πθ[R(τ)]J(\\theta)=\\mathbb E_{\\tau \\sim \\pi_\\theta}[R(\\tau)] J(θ)=Eτ∼πθ​​[R(τ)] 设 trajectory τ\\tauτ 包含 TTT 步，每一步为 ttt，计算其梯度，得到 ∇θJ(θ)=Eτ∼πθ[∑t=0T∇θlog⁡πθ(at∣st)⋅R(τ)] abla_\\theta J(\\theta)=\\mathbb E_{\\tau\\sim\\pi_\\theta}\\Big[ \\sum_{t=0}^T abla_\\theta \\log \\pi_\\theta(a_t|s_t)\\cdot R(\\tau) \\Big] ∇θ​J(θ)=Eτ∼πθ​​[t=0∑T​∇θ​logπθ​(at​∣st​)⋅R(τ)]"},{"title":"Isaac Lab(Sim) 简介","path":"/wiki/simulation/isaac-lab-brief.html","content":"Assets Isaac Sim Has more built-in scenes and robots available Classic: Cartpole, Humanoid, Ant Fixed-Arm and Hands: UR10, Franka, Allegro, Shadow Hand Quadrupeds: Anybotics Anymal-B, Anymal-C, Anymal-D, Unitree A1, Unitree Go1, Unitree Go2, Boston Dynamics Spot Humanoids: Unitree H1, Unitree G1 Quadcopter: Crazyflie Procedure Isaac Lab: Manager Method: More specified control Direct Method: Similar to Maniskill. Example of Direct Method: 1234567891011121314151617181920212223242526272829303132333435def _get_rewards(self) -&gt; torch.Tensor: total_reward = compute_rewards( self.cfg.rew_scale_alive, self.cfg.rew_scale_terminated, self.cfg.rew_scale_pole_pos, self.cfg.rew_scale_cart_vel, self.cfg.rew_scale_pole_vel, self.joint_pos[:, self._pole_dof_idx[0]], self.joint_vel[:, self._pole_dof_idx[0]], self.joint_pos[:, self._cart_dof_idx[0]], self.joint_vel[:, self._cart_dof_idx[0]], self.reset_terminated, ) return total_reward@torch.jit.scriptdef compute_rewards( rew_scale_alive: float, rew_scale_terminated: float, rew_scale_pole_pos: float, rew_scale_cart_vel: float, rew_scale_pole_vel: float, pole_pos: torch.Tensor, pole_vel: torch.Tensor, cart_pos: torch.Tensor, cart_vel: torch.Tensor, reset_terminated: torch.Tensor,): rew_alive = rew_scale_alive * (1.0 - reset_terminated.float()) rew_termination = rew_scale_terminated * reset_terminated.float() rew_pole_pos = rew_scale_pole_pos * torch.sum(torch.square(pole_pos).unsqueeze(dim=1), dim=-1) rew_cart_vel = rew_scale_cart_vel * torch.sum(torch.abs(cart_vel).unsqueeze(dim=1), dim=-1) rew_pole_vel = rew_scale_pole_vel * torch.sum(torch.abs(pole_vel).unsqueeze(dim=1), dim=-1) total_reward = rew_alive + rew_termination + rew_pole_pos + rew_cart_vel + rew_pole_vel return total_reward Tasks"},{"title":"ManiSkill 物理仿真：编写 Tasks for RL","path":"/wiki/simulation/maniskill-testcase.html","content":"Task Components 较为繁琐的说法 Setting up the Task Class Loading (Robots, Assets, Sensors, etc.) (run once) Episode initialization / Randomization (run every env.reset) Success/Failure Condition (run every env.step) Extra Observations (run every env.step) (Optional) Dense Reward Function (run every env.step) (Optional) Setting up cameras/sensors for observations and rendering/recording (run once) 最简工作流示例 env init env.step 负责根据 Action，然后在物理仿真，模拟无理式解的变化，并计算 Reward。 env.step 简单来说，一个类包含这些元素： @register_env() 方便外部调用 class CustomEnv(BaseEnv) 使用继承，快速开发新 Testcase （成员变量）SUPPORTED_ROBOTS = [] 定义该 Testcase 里使用的 Robot （成员变量）agent: Union[...] Robot，也即 Agent Environment Class 首先，我们定义一个类继承 BaseEnv，这个类是我们初始化 Environment 的入口。同时需要调用 mani_skill.utils.registeration.register_env() 函数进行“注册”（主要是定义名称和限定最大迭代步数） 12@register_env(&quot;CustomEnv-v1&quot;, max_episode_steps=200)class CustomEnv(BaseEnv): 然后我们在这个环境里定义我们需要的 Agent 定义物体 位置与朝向 建议在 _load_scene() 的时候就设置一次位置与朝向，然后在 _initialize_episode() 中"},{"title":"django 表单","path":"/wiki/web_fullstack/django-forms.html","content":"django.forms.Form 类 在 Form 里定义需要填写的栏目，这样就可以直接在 template 里渲染了"},{"title":"Django 模板","path":"/wiki/web_fullstack/django-template.html","content":"Django 模板 Django 模板的作用是，可以根据数据动态地展示网页。例如，可以根据用户的权限，选择向用户展示 dashboard 或者登录界面。 与 views.py 交互"}]