[{"title":"Design Patterns 的分类与原则","path":"/topic/design patterns/design-patterns-classification/","content":"Categorisation Category Explanation Creational Patterns provide object creation mechanisms that increase flexibility and reuse of existing code. Structural Patterns explain how to assemble objects and classes, while keeping the structures flexible and efficient. Behavioral Patterns take care of effective communication and the assignment of responsibilities between objects. Design Principle Code Reuse Extensibility Encapsulation Identify the aspects of your application that vary and separate them from what stays the same. on Method Level Beforedef getOrderTotal(order): total = 0 for item in order.items: total += item.price * item.quantity # 处理 tax 的逻辑和计算总价的逻辑放在一起了 if order.country == US: total += total * 0.07 elif order.country == EU: total += total * 0.2 return total Afterdef getOrderTotal(order): total = 0 for item in order.items: total += item.price * item.quantity # 处理 tax 的逻辑 与 计算总价的逻辑 拆开了 total += total * getTaxRate(order.country) return totaldef getTaxRate(country): if country == US: return 0.07 elif country == EU: return 0.2 else: return 0 on Class Level 随着项目进展，很有可能会在一个 method/class 里加入更多职责，而这些职责通常有属于自己的字段，这些添加的字段会逐渐模糊掉当前 method/class 原本的任务。 如想在 method 开头加入 auth 授权，那么如果直接写进 class，那么就需要在 class 内额外添加字段。 所以更好的解决办法是把不属于自己原本职责的内容拆分出去，拆分到一个新的 class 里去。 Program to Interface Program to an interface, not an implementation. Depend on abstractions, not on concrete classes. 现在假设 class A 和 class B 需要通信。与其让 class A 的 method 里直接以 class B 为类型，最好使用 Interface 让其更加 flexible，以防后续需要对 class B 扩充（例如，不止接受 class B 为参数，还需要接受 class C 为参数） 找出 class A 里，究竟使用 B 里面的什么方法 把这些方法写成 interface 或者 abstract(virtual) class 让 class A 的参数接受的是实现了这个 interface 的 class（或者是 abstract class 的子类的 class） 接着让 class B 去实现这个 interface 或者继承这个 abstract class 如图所示，Food interface 让 Cat 不仅可以吃 Sausage 还可以接受其他食物 Composition over Inheritance Interface 有几个问题： subclass 必须继承所有 parent class 的 interface，不能只继承一个子集；且必须实现所有父类的 abstract 方法 overriding 父类方法时，需要确保行为兼容 有时在实践中，父类需要知道子类的信息 父类子类耦合紧密，父类 method 的行为变化可能导致子类的行为变化 继承可能形成 parallel inheritance 结构 Inheritance 代表的是 “is a” 逻辑，而 Composition 代表的是 “has a” 逻辑。 Composition 与 Aggregation 逻辑也比较类似，其区别在于 life cycle 不同。Composition 中 component 与主体的 life cycle 相同（如汽车与引擎），而 Aggregation 中 components 的 life cycle 却不一定相同（如汽车与司机）。 SOLID Principle Single Responsibility Principle A class should have just one reason to change. 每个类只负责单一职责。 Open/Closed Principle Classes should be open for extension but closed for modification. 尽量不修改原有代码，而是基于原有代码新增"},{"title":"First Principle Design Thinking","path":"/first-principle-design-thinking/","content":"FinTech is innovative in offering financial services through technology and user experience (UX) First Principle Thinking FPDT It is the practice of actively questioning every assumption you think you ‘know’ about a given problem or scenario, and then creating new knowledge and solutions from scratch 3 Single Steps Identify and define current assumptions Breakdown the problem into its fundamental principles Create new solutions from scratch Examples Motorola flip phone →\\to→ iPhone touchscreen, no keyboard Rocket engineering techniques and approach 50 years ago →\\to→ SpaceX modern approach Bank Services Design Thinking Solution-based approach to problem solving Human based approach: observing and empathizing with the target user Questions the problem, assumptions and the implications Identify alternative solutions that might not be instantly apparent Non-linear and iterative: sketching, prototyping, testing, and trying out concepts Design Thinking Design Sprint Understand/Empathize: Research your users’ needs Define: State your users’ needs and problems Ideate/Sketch: Challenge assumptions and create ideas An addition phase can be to “Decide” or “Vote” Prototype: Start to create solutions Validate/Test: Try your solutions out Understand User Who are your users? Are there different types? Do you describe them with personas? How User Journey What is the end-to-end user experience? How do users arrive or begin? What are the entry points? What is the ideal or target path or flow? What are the key moments or touchpoints along the way? Is this a single or multi-session experience? How does the experience end? What are the exit points? How do we reach or serve users after they have finished? How Might We…? Think of the end to end user journey From the point of view of all types of users Write pain points as opportunities 当前假设：我们需要 XXX 才能 YYY. HMW: 我们如何在没有 XXX 进行 YYY. Define 然后利用 HMW 问题，可以定义出 outcome，如 Goal: Start by thinking about the big picture: What are you trying to help users do? What problem are you trying to solve? Measure: Next, consider what change in user behaviour or opinion would indicate you have been successful in your goals Sketch Decide More than one HMW statement can be worked on in the Sketch phase Vote for the best solution Prototype Create a prototype of the chosen concept in the decide phase You are building just what you need to make the prototype real enough to get an authentic response from a potential user in the Validate phase. Map out the exact flow for the experience and only building the steps you want to test No need to build a full functional back-end or to solve for every flow in your product Anything can be prototyped in a day if it is clearly mapped out You can create a prototype to present for your group project Validate Gather feedback from users who interact with your prototype Stakeholder and technical feasibility reviews Re-iterate on your solution after gathering feedback Test and test again Go back to the relevant phases if user tests fails"},{"title":"Non-Fungible Tokens","path":"/Non-Fungible-Tokens/","content":"Non-Fungible Tokens 正式提案于 ERC-721，Non-Fungible Token 可以简单理解为 Unique token. 像现实世界里的艺术品那样拥有独特性. Generative NFT Collections 这玩意儿感觉就是个“人为创造稀缺性”的时尚小垃圾罢了，真有什么收藏价值么……顶多可以炒一炒感觉 More than Collections 其实 NFT 除了收藏品之用以外，适用场景还是有的。比如说电子身份证、电子证书，这个东西就必须满足独特性；或者证券型代币。","tags":["区块链"]},{"title":"ERC20 协议","path":"/ERC20-tokens/","content":"ERC-20 Protocol ERC-20 ERC-20 是一个协议标准，定义了在 Ethereum 网络上发行 tokens 的规则与标准。 Basically, an ERC-20 token is just a contract that keeps track of who owns how much of that token, and some functions so those users can transfer their tokens to other addresses ERC-20 tokens 是直接在 Ethereum blockchain 上发行和交易的。","tags":["区块链"]},{"title":"2025 ICPC World Final","path":"/2025icpc-wf-journey/","content":"2025 ICPC World Final：阿塞拜疆，巴库 这个赛季属于是没有什么遗憾的了，倒不如说是运气带来的意外之喜，也算是来开开眼界了。虽然说大二的我水平比大一有很大进步，但是相比我的两位 NOI 银牌队友显然是不够看的。感谢队友带我开眼界（ 31 Aug"},{"title":"Codeforces 刷题 2025.08.26","path":"/topic/dailycf/20250826-dp/","content":"2119D - Token Removing 我们发现光是从一个序列 { ai }\\set{a_i}{ai​} 计算其 f({ a })f(\\set{a})f({a}) 的值都必须要 O(n)O(n)O(n)，更别说枚举所有可能序列了。 所以我们反着考虑：我们考虑一个 token removal sequence 可能被几个序列统计到。对于单个 token 设其位置为 ppp，则如果 aia_iai​ 可以把 ppp 取走，那么应该满足 ai≤p≤i a_i\\le p\\le i ai​≤p≤i我们考虑有多少 [l,r][l,r][l,r] 满足 l≤p≤rl\\le p\\le rl≤p≤r，则满足这个条件的数对表示 ar=la_r=lar​=l 时刻把 token ppp 取走。那么考虑 token 被取走的集合（不是顺序）1≤p1p2p3…pk≤n1\\le p_1\\lt p_2\\lt p_3\\lt \\dots p_{k}\\le n1≤p1​p2​p3​…pk​≤n，我们从 pkp_{k}pk​ 往 p1p_1p1​ 数有多少 { [l,r] }i∈[1,k]\\set{[l,r]}_{i\\in [1,k]}{[l,r]}i∈[1,k]​ 满足 rir_iri​ 互不相同且 pi∈[li,ri]p_i\\in [l_i,r_i]pi​∈[li​,ri​]，这样的 [l,r][l,r][l,r] set 集合表示这个 token sequence 会被 ar=la_r=lar​=l (其余元素为 000) 的序列统计到。而有多少这样的 { [l,r] }i\\set{[l,r]}_i{[l,r]}i​ 集合呢？ ∏i=k1pi(n+1−pi−(k−i)) \\prod_{i=k}^1 p_i(n+1-p_i-(k-i)) i=k∏1​pi​(n+1−pi​−(k−i))−(k−i)-(k-i)−(k−i) 表示比 pip_ipi​ 打的 token 位置里已经被取走 k−ik-ik−i 个。所以我们最后希望统计的是 ∑f={ p }∏i=∣f∣1pi(n+1−pi−(f−i)) \\sum_{f=\\set{p}}\\prod_{i= |f|}^1 p_i(n+1-p_i-(f-i)) f={p}∑​i=∣f∣∏1​pi​(n+1−pi​−(f−i))我们显然不能枚举 { p }\\set{p}{p}. 所以我们转而考虑如何递推，我们从长度出发进行递推，考虑每一个位置上的 token 会被几个 token seq 计算到。 我们令 fℓ,pf_{\\ell, p}fℓ,p​ 表示长度为 ℓ\\ellℓ，其中最小值 min⁡=p\\min=pmin=p 的和式结果。则就有递推式 fℓ,p=(∑p′pfℓ−1,p′)×p×(n+1−p−(ℓ−1)) f_{\\ell, p} = \\Big(\\sum_{p\\gt p}f_{\\ell-1, p}\\Big) \\times p\\times (n+1-p-(\\ell-1)) fℓ,p​=(p′p∑​fℓ−1,p′​)×p×(n+1−p−(ℓ−1))括号内的和式很容易用前缀和优化掉，所以整体的时间复杂度为 O(n2)O(n^2)O(n2) 的. 尝试从长度递推之后，一个自然而然的问题就是，多出的那个元素应该是什么？最大值？最小值？如果你尝试用最大值的话，就会发现你还需要额外保存 ∑pmax⁡p\\sum_{p\\lt \\max} p∑pmax​p，这很麻烦。所以应该选择 min⁡\\minmin 作为额外添加的那个元素。 AC Code #include bits/stdc++.hconstexpr int N = 5005;int n, m;int F[N][N], g[N]; // len, minvoid run() std::cin n m; for (int i = 0; i = n + 1; i++) for (int j = 0; j = n + 1; j++) F[i][j] = 0; F[0][n + 1] = 1; for (int len = 1; len = n; len++) for (int i = n + 1; i = 1; i--) g[i] = (g[i + 1] + F[len - 1][i]) % m; for (int i = 1; i = n; i++) F[len][i] = 1ll * g[i + 1] * i % m * (n - i + 1 - (len - 1)) % m; int ans = 1; for (int i = 1; i = n; i++) for (int j = 1; j = n; j++) ans = (ans + F[i][j]) % m; std::cout ans ;int main() std::cin.tie(0)-sync_with_stdio(0); int t; std::cin t; while (t--) run();"},{"title":"Codeforces 刷题 2025.08.22","path":"/topic/dailycf/20250822-dp/","content":"2126G1 - Bigs Wins (Easy Version) 考虑到 aia_iai​ 值域较小，考虑 V×O(n)V\\times O(n)V×O(n) 或者 V×O(nlog⁡n)V\\times O(n\\log n)V×O(nlogn) 的做法。于是可以想到一个经典的转化：枚举中位数 med\\texttt{med}med，令 bi={−1if aimed1if ai≥med b_i=\\begin{cases} -1 \\texttt{if \\(a_i\\lt\\)med}\\\\ 1 \\texttt{if \\(a_i\\ge\\)med} \\end{cases} bi​={−11​if ai​medif ai​≥med​这样就有 median(a[l,r])≥med ⟺ ∑i∈[l,r]bi≥0\\text{median}(a[l,r])\\ge \\texttt{med} \\iff \\sum_{i\\in [l,r]} b_i\\ge 0median(a[l,r])≥med⟺∑i∈[l,r]​bi​≥0. 固定了 med\\texttt{med}med 后，我们希望最大化答案，即找到 [l,r][l,r][l,r] 使得 median(a[l,r])≥med\\text{median}(a[l,r])\\ge \\texttt{med}median(a[l,r])≥med 且 min⁡a[l,r]\\min a[l,r]mina[l,r] 最小. 这里有一个非常精彩的转化。比如说 [l,r][l,r][l,r] 这个区间已经固定了，那么 med−min⁡i∈[l,r]ai ⟺ min⁡i∈[l,r]{ med−ai } \\texttt{med}-\\min_{i\\in [l,r]} a_i\\iff \\min_{i\\in[l,r]}\\set{\\texttt{med}-a_i} med−i∈[l,r]min​ai​⟺i∈[l,r]min​{med−ai​}所以，这里衍生出两种做法： （左式衍生做法）枚举区间，然后在区间内找最小值。这个就是非常经典的序列处理办法了，用数据结构维护左端点，然后把右端点放到数据结构里查。这里，我们只需要对某个右端点 rrr，找到 leftmost lll 使得 ∑i∈[l,r]bi≥0\\sum_{i\\in[l,r]} b_i\\ge 0∑i∈[l,r]​bi​≥0 然后找出 min⁡i∈[l,r]ai\\min_{i\\in [l,r]} a_imini∈[l,r]​ai​ 即可. （右式衍生做法）枚举 aia_iai​，对 aia_iai​ 检查是否有一个区间 [l,r][l,r][l,r] 包含了 aia_iai​ 且且中位数 ≥med\\ge \\texttt{med}≥med。只需要找能包含 aia_iai​ 的最长区间即可，所以我们可以在计算 bib_ibi​ 前缀和 pip_ipi​ 的基础上，维护 pip_ipi​ 的 prefix min 与 suffix max 即可。只要 suffix max −-− prefix min ≥0\\ge 0≥0，那么 suffix max 和 prefix min 之间的所有 aia_iai​ 就都会被计算一边，能确保答案不漏。 第一种做法理论也可行。但是我的实现是 O(Vnlog⁡2n)O(Vn\\log^2 n)O(Vnlog2n) 显然会被卡 -_- 所以还是得用第二种方法，时间复杂度是 O(Vn)O(Vn)O(Vn) 的. AC Code #include bits/stdc++.hconstexpr int N = 2e5 + 10;int n;std::arrayint, N a, prefix, minpre, maxsuf;void mark(int m) for (int i = 1; i = n; i++) prefix[i] = prefix[i - 1] + (a[i] m ? -1 : 1); minpre[0] = 0; for (int i = 1; i = n; i++) minpre[i] = std::min(minpre[i - 1], prefix[i]); maxsuf[n] = prefix[n]; for (int i = n - 1; i = 1; i--) maxsuf[i] = std::max(maxsuf[i + 1], prefix[i]);void run() std::cin n; for (int i = 1; i = n; i++) std::cin a[i]; int ans = 0; for (int med = 100; med = 0; med--) mark(med); for (int i = 1; i = n; i++) if (maxsuf[i] = minpre[i - 1]) ans = std::max(ans, med - a[i]); std::cout ans ;int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0), std::cerr.tie(0); int t; std::cin t; while (t--) run();"},{"title":"LangChain API: 多模态输入","path":"/topic/LLMApplication/langchain/langchain-multimodal-inputs/","content":"LangChain: How to Pass messages to Models 一般的框架是 llm = CHatOllama() # ... just an examplemessage = role: user, content: [ # ...... ],response = llm.invoke([message])print(response.text()) 如果需要传入多模态的输入的话，我们就在 message 的 content 栏位填入我们需要的东西即可 Texts content: [ type: text, text: ......, ] Images 对于本地文件，可以传 base64 encode image。对于网络资源，可以传 URL type: image, source_type: base64, mime_type: image/jpeg, # or image/png, etc. data: base64 data string,# OR... type: image, source_type: url, url: https://..., Documents (PDF) 有的模型支持直接上传 PDF type: file, source_type: base64, mime_type: application/pdf, data: base64 data string, Audio type: audio, source_type: base64, mime_type: audio/wav, # or appropriate mime-type data: base64 data string,"},{"title":"RAG：添加引用文献功能","path":"/topic/LLMApplication/RAG-reference-system/","content":"通过 Prompting 实现文献引用 一个最简单的方法便是通过 prompting 显式告诉 LLM 你需要为文字添加文献引用。 好处很明显，it’s so easy. 然而弊端也很明显，LLM 也有可能输出不准确，尽管当使用的模型越来越大时这个情况会比较少见，但也仍有可能出现，尤其当模型比较小的时候几乎是必然发生的情况。 Tool Calling"},{"title":"Tensor 的内存分布","path":"/topic/cs336/tensor-storage/","content":"Tensro Storage 我们考虑 2 dimension 4×44\\times 44×4 tensor. 按理解来说，"},{"title":"Claude Code 提示词工程指南（一）","path":"/claude-code-prompting/","content":"CLAUDE.md CLAUDE.md 在每次 claude code 启动的时候都会载入 context，因此适合在 CLAUDE.md 里放一些运行、编译命令、环境之类的东西，比如说： 核心文件、工具函数 代码风格 测试说明 仓库约定 环境配置 …… 当运行 /init 的时候，claude 会默认生成一个 CLAUDE.md，会放入目录分析、文件分析等等内容 微调 CLAUDE.md 实际上 claude.md 就是 prompt 的功能。在 claude console 里键入 # 然后给 claude instruction，claude 也可以自动往 claude.md 里记录 command, instructions。 MCP Tools, Shell Commands Shell Commands claude 知道一些常用的 shell 命令，但是对于那些自定义的命令，还是需要完整的 prompt 一下。可以直接在 claude.md 里写明命令名字和使用例；也可以让 claude 自己运行 xxx --help。 建议把常用命令写进 claude.md 里 MCP 把 MCP server 写进项目目录的 .mcp.json Github Operations claude 会自己使用 gh 命令完成各种操作，比如 pull, push, commit 等等。"},{"title":"AI 训练的浮点数、整数数据格式","path":"/topic/cs336/fp-int-formats/","content":"Data Formats Implications on Training 用 fp32 训练会消耗大量资源、内存 用 fp8, fp16, bf16 训练可能导致 instability 可以尝试 Mixed Precision Training. bf16"},{"title":"模型计算量 FLOPs","path":"/topic/cs336/model-computational-resources/","content":""},{"title":"终端里给任何字体都加上自定义图标和中文支持","path":"/terminal-chn-fonts/","content":"给字体打个补丁 Linux 运行 fc-cache -f 会 重新加载字体文件夹下的所有字体文件 读取 ~/.config/fontconfig/fonts.conf 这个 fonts.conf 可以自定义字体 fallback，可以给每一个字体都单独进行设置： alias familyInconsolata/family prefer familyFiraCode Nerd Font/family familyLXGW Wenkai/family /prefer/alias 上面的 config 就设置了两个东西： 先使用 Inconsolata 的英文字体 对于图标等特殊字符和中文，先在 FiraCode Nerd Font 里找（能找到图标这类的特殊字符） 对于中文，再到 LXGW Wenkai 里找（能找到中文字体） 然后 nerd font 和中文字体都可以正常显示了~"},{"title":"JOISC2021 - IOI 热病","path":"/topic/dailycf/joisc2021-ioi-netsubyou/","content":"太难写了，吐了 bro IOI 热病 我们不妨钦定居民 a1a_1a1​ 的方向为 d1=(1,0)d_1=(1, 0)d1​=(1,0)（向右走）。首先，我们需要推理出关于居民方向的有关性质。 性质 aia_iai​ 的方向 did_idi​ 满足"},{"title":"P3631 [APIO2011] 方格染色","path":"/topic/dailycf/LuoguP3631/","content":"P3631 [APIO2011] 方格染色[1] 题目里说把 111 看成红色，000 看成蓝色，那么任意一个 2×22\\times 22×2 的区域，其异或和为 111. xi,j⊕xi−1,j⊕xi,j−1⊕xi−1,j−1=1 x_{i,j}\\oplus x_{i-1,j}\\oplus x_{i,j-1}\\oplus x_{i-1,j-1}=1 xi,j​⊕xi−1,j​⊕xi,j−1​⊕xi−1,j−1​=1接着考虑和这个 2×22\\times 22×2 区域相交了两个格子的区域 [i−2,i−1]×[j−1,j][i-2,i-1]\\times[j-1,j][i−2,i−1]×[j−1,j]，由于异或相消，有 xi,j⊕xi−1,j⊕xi,j−1⊕xi−1,j−1=1⊕xi−1,j⊕xi−2,j⊕xi−1,j−1⊕xi−2,j−1=1xi,j⊕xi−2,j⊕xi,j−1⊕xi−2,j−1=0 \\begin{array}{c|lllll} x_{i,j}\\oplus \\cancel{x_{i-1,j}}\\oplus x_{i,j-1}\\oplus \\cancel{x_{i-1,j-1}}=1\\\\ \\oplus \\cancel{x_{i-1,j}}\\oplus x_{i-2,j}\\oplus \\cancel{x_{i-1,j-1}}\\oplus x_{i-2,j-1}=1\\\\ \\hline x_{i,j}\\oplus x_{i-2,j}\\oplus x_{i,j-1}\\oplus x_{i-2,j-1}=0 \\end{array} ⊕​xi,j​xi−1,j​​xi,j​​⊕xi−1,j​​⊕xi−2,j​⊕xi−2,j​​⊕xi,j−1​⊕xi−1,j−1​​⊕xi,j−1​​⊕xi−1,j−1​​⊕xi−2,j−1​⊕xi−2,j−1​​=1=1=0​​进一步代入 i′←i−2i\\gets i-2i′←i−2 可以推广到 xi,j⊕xi−2k,j⊕xi,j−1⊕xi−2k,j−1=0 x_{i,j}\\oplus x_{i-2k,j}\\oplus x_{i,j-1} \\oplus x_{i-2k,j-1}=0 xi,j​⊕xi−2k,j​⊕xi,j−1​⊕xi−2k,j−1​=0把 j′←j−1j\\gets j-1j′←j−1 代入发现 xi,j⊕xi−2k,j⊕xi,j−1⊕xi−2k,j−1=0⊕xi,j−1⊕xi−2k,j−1⊕xi,j−2⊕xi−2k,j−2=0xi,j⊕xi−2k,j⊕xi,j−2⊕xi−2k,j−2=0 \\begin{array}{c|lllll} x_{i,j}\\oplus x_{i-2k,j}\\oplus \\cancel{x_{i,j-1}}\\oplus \\cancel{x_{i-2k,j-1}}=0\\\\ \\oplus\\cancel{x_{i,j-1}}\\oplus \\cancel{x_{i-2k,j-1}}\\oplus x_{i,j-2}\\oplus x_{i-2k,j-2}=0\\\\ \\hline x_{i,j}\\oplus x_{i-2k,j}\\oplus x_{i,j-2}\\oplus x_{i-2k,j-2}=0\\\\ \\end{array} ⊕​xi,j​xi,j−1​​xi,j​​⊕xi−2k,j​⊕xi−2k,j−1​​⊕xi−2k,j​​⊕xi,j−1​​⊕xi,j−2​⊕xi,j−2​​⊕xi−2k,j−1​​⊕xi−2k,j−2​⊕xi−2k,j−2​​=0=0=0​​所以可以推广到 xi,j⊕xi−2k,j⊕xi,j−t⊕xi−2k,j−t=0 x_{i,j}\\oplus x_{i-2k,j}\\oplus x_{i,j-t}\\oplus x_{i-2k,j-t}=0 xi,j​⊕xi−2k,j​⊕xi,j−t​⊕xi−2k,j−t​=0同理有 xi,j⊕xi,j−2k⊕xi−t,j⊕xi−t,j−2k=0 x_{i,j}\\oplus x_{i,j-2k}\\oplus x_{i-t,j}\\oplus x_{i-t,j-2k}=0 xi,j​⊕xi,j−2k​⊕xi−t,j​⊕xi−t,j−2k​=0也就是说 Code #include algorithm#include iostream#include utility#include vectorusing namespace std;constexpr int N = 1e5 + 5;constexpr int M = 1e9;using vi = vectorint;using constraint = pairint, int;struct ColouredCell int x, y, c; cc[N];int n, m, k;int mapping(int x, int y) if (y == 0) return x; else return y + n;vectorconstraint G[N 1];int vis[N 1], val[N 1];int main() cin n m k; for (int i = 1; i = k; i++) cin cc[i].x cc[i].y cc[i].c; if (cc[i].x % 2 == 1 cc[i].y % 2 == 1) int a = mapping(cc[i].x, 0); int b = mapping(0, cc[i].y); G[a].push_back(b, cc[i].c); G[b].push_back(a, cc[i].c); else int a = mapping(cc[i].x, 0); int b = mapping(0, cc[i].y); G[a].push_back(b, cc[i].c ^ 1); G[b].push_back(a, cc[i].c ^ 1); auto dfs = [](auto F, int u) - bool vis[u] = 1; // cerr u ; for (auto [v, cons] : G[u]) if (vis[v]) if ((val[v] ^ val[u]) != cons) return false; continue; val[v] = val[u] ^ cons; if (!F(F, v)) return false; return true; ; int cnt = -2; for (int i = 0; i = n + m; i++) if (vis[i]) continue; cnt++; if (!dfs(dfs, i)) cout 0 ; return 0; // cerr ; int b = 2, p = cnt; int res = 1; while (p) if (p 1) res = 1ll * res * b % M; b = 1ll * b * b % M; p = 1; cout res ; LuoGu Blog ↩︎"},{"title":"P3632 [APIO2011] 寻路","path":"/topic/dailycf/LuoguP3632/","content":"寻路 因为只能横平竖直地移动，且不能穿过矩形，所以，我们希望能够建出下图的绿色点 格点 这样的绿色点的特征是：我们确保了可以在绿色点上换飞行方向，从而沿着建出来的边走。这样我们就可以跑最短路算法了。 现在考虑怎么建出这样的点。假定在某个点 (a,b)(a,b)(a,b) 并且向左走，那么我们画一条 y=by=by=b 的直线，在 (a,b)(a,b)(a,b) 左侧且最靠近 (a,b)(a,b)(a,b) 的点 (c,d)(c,d)(c,d) 就是 Dee 的落脚点，所以在 (c,d),(a,b)(c,d),(a,b)(c,d),(a,b) 之间连边。 处理完不同矩形之间的边后，我们还需要处理同一个矩形上的点之间的边，因为上面扫描线这一过程可能会产生不在角上的点（图中在边上的绿色点）。我们需要把这些点连接到矩形的角点上。具体而言，我们把扫描线产生的额外点 assign 到矩形的上、下、左、右边上，然后对每个矩形枚举四条边，sort - unique 之后相邻的点之间连边。 最后，我们在 start 和 end 之间跑一个裸的最短路即可。 Code 实现细节：我们处理两次，一次处理矩形的长 x=Lix=L_ix=Li​，一次处理矩形的宽 y=Hiy=H_iy=Hi​. 处理长的时候，我们按 yyy 坐标从小到大加入待处理队列（我用 mapint,int 同时维护顺序和线段范围），那么我们可以直接遍历 map，尝试在 it 代表的矩形边和 next(it) 代表的矩形边之间建一条边（如果这两条矩形边在不同的矩形上）。 #include algorithm#include cassert#include chrono#include ext/pb_ds/assoc_container.hpp#include ext/pb_ds/hash_policy.hpp#include iostream#include map#include queue#include utility#include vectorusing ll = long long;using edge = std::pairint, ll;using vi = std::vectorint;using pil = std::pairint, ll;using vpil = std::vectorpil;constexpr int N = 1e3 + 5;struct Rec int l, r, t, b; vi onl, onr, ont, onb; R[N];int n, index0;std::mapstd::pairint, int, int ind; // (x, y) - indexstd::mapint, std::pairint, int ind2; // index - (x, y)std::vectorvpil G;int insert(int x, int y) if (ind.find(x, y) != ind.end()) return ind[x, y]; ind[x, y] = ++index; ind2[index] = x, y; G.push_back(); return index;void run() G.push_back(); // index 0 is unused [] int a, b, c, d; std::cin a b c d; std::cin n; for (int i = 1; i = n; i++) std::cin R[i].l R[i].b R[i].r R[i].t; if (R[i].l R[i].r) std::swap(R[i].l, R[i].r); if (R[i].b R[i].t) std::swap(R[i].b, R[i].t); R[n + 1] = a, a, b, b; // start rectangle R[n + 2] = c, c, d, d; // end (); [] struct segment int id, x, y, end; // end: 0 = bottom, 1 = top segment(int i, int x, int y, int e) : id(i), x(x), y(y), end(e) ; std::vectorsegment cand; for (int i = 1; i = n + 2; i++) cand.emplace_back(i, R[i].l, R[i].b, 0); // left bottom cand.emplace_back(i, R[i].l, R[i].t, 1); // left top cand.emplace_back(i, R[i].r, R[i].b, 0); // right bottom cand.emplace_back(i, R[i].r, R[i].t, 1); std::sort(cand.begin(), cand.end(), [](const segment a, const segment b) if (a.y == b.y) if (a.x == b.x) return a.end b.end; // bottom before top else return a.x b.x; // sort by x first else return a.y b.y; ); std::mapint, int mp; // x - id for (auto it = cand.begin(); it != cand.end();) int y = it-y; auto tit = it; while (tit != cand.end() tit-y == y) mp.insert(tit-x, tit-id), tit++; // create points std::vectorint indexes; for (auto [x, id] : mp) assert(x == R[id].l || x == R[id].r); int k = insert(x, y); if (x == R[id].l) R[id].onl.push_back(k); else R[id].onr.push_back(k); indexes.push_back(k); // connect edges auto p = indexes.begin(); for (auto pit = mp.begin(); std::next(pit) != mp.end(); pit++, p++) auto nit = std::next(pit); auto np = std::next(p); if (nit-second == pit-second) continue; // 同一个蜂巢 ll len = nit-first - pit-first; G[*p].emplace_back(*np, len); G[*np].emplace_back(*p, len); // update segments for (; it != tit; it++) if (it-end == 1) mp.erase(it-x); (); [] struct segment int id, x, y, end; ; std::vectorsegment cand; for (int i = 1; i = n + 2; i++) cand.emplace_back(i, R[i].l, R[i].t, 0); cand.emplace_back(i, R[i].r, R[i].t, 1); cand.emplace_back(i, R[i].l, R[i].b, 0); cand.emplace_back(i, R[i].r, R[i].b, 1); std::sort(cand.begin(), cand.end(), [](const segment a, const segment b) if (a.x == b.x) if (a.y == b.y) return a.end b.end; else return a.y b.y; else return a.x b.x; ); std::mapint, int mp; // y - id for (auto it = cand.begin(); it != cand.end();) int x = it-x; auto tit = it; while (tit != cand.end() tit-x == x) mp.insert(tit-y, tit-id), tit++; // create points vi indexes; for (auto [y, id] : mp) assert(y == R[id].t || y == R[id].b); int k = insert(x, y); indexes.push_back(k); if (y == R[id].t) R[id].ont.push_back(k); else R[id].onb.push_back(k); // update edges auto p = indexes.begin(); for (auto pit = mp.begin(); std::next(pit) != mp.end(); pit++, p++) auto nit = std::next(pit); auto np = std::next(p); if (nit-second == pit-second) continue; ll len = nit-first - pit-first; G[*p].push_back(*np, len); G[*np].push_back(*p, len); // update segments for (; it != tit; it++) if (it-end == 1) mp.erase(it-y); (); auto clean = [](int which) // onl vi *v = R[which].onl; std::sort(v-begin(), v-end(), [](int i, int j) return ind2[i].second ind2[j].second; ); v-erase(std::unique(v-begin(), v-end()), v-end()); for (int i = 0; i + 1 v-size(); i++) int low = v-at(i), high = v-at(i + 1); int len = ind2[high].second - ind2[low].second; G[low].push_back(high, len); G[high].push_back(low, len); // onr v = R[which].onr; std::sort(v-begin(), v-end(), [](int i, int j) return ind2[i].second ind2[j].second; ); v-erase(std::unique(v-begin(), v-end()), v-end()); for (int i = 0; i + 1 v-size(); i++) int low = v-at(i), high = v-at(i + 1); int len = ind2[high].second - ind2[low].second; G[low].push_back(high, len); G[high].push_back(low, len); // onb v = R[which].onb; std::sort(v-begin(), v-end(), [](int i, int j) return ind2[i].first ind2[j].first; ); v-erase(std::unique(v-begin(), v-end()), v-end()); for (int i = 0; i + 1 v-size(); i++) int low = v-at(i), high = v-at(i + 1); int len = ind2[high].first - ind2[low].first; G[low].push_back(high, len); G[high].push_back(low, len); // ont v = R[which].ont; std::sort(v-begin(), v-end(), [](int i, int j) return ind2[i].first ind2[j].first; ); v-erase(std::unique(v-begin(), v-end()), v-end()); for (int i = 0; i + 1 v-size(); i++) int low = v-at(i), high = v-at(i + 1); int len = ind2[high].first - ind2[low].first; G[low].push_back(high, len); G[high].push_back(low, len); ; for (int i = 1; i = n + 2; i++) clean(i); int s = ind[R[n + 1].l, R[n + 1].b]; int e = ind[R[n + 2].l, R[n + 2].b]; std::vectorll dis(G.size(), 1e18); vi vis(G.size(), false); dis[s] = 0; std::priority_queuepil, vpil, std::greaterpil pq; pq.push(0, s); while (!pq.empty()) auto [d, u] = pq.top(); pq.pop(); if (vis[u]) continue; vis[u] = true; for (auto [v, len] : G[u]) if (vis[v]) continue; if (dis[v] dis[u] + len) dis[v] = dis[u] + len; pq.push(dis[v], v); if (dis[e] == 1e18) std::cout No Path ; else std::cout dis[e] ; [] // clear!! G.clear(); ind.clear(); ind2.clear(); index = 0; for (int i = 1; i = n + 2; i++) R[i].onl.clear(); R[i].onr.clear(); R[i].ont.clear(); R[i].onb.clear(); ();int main() int T; std::cin T; while (T--) run();"},{"title":"P9368 [ICPC2022 Xi'an] Streets","path":"/topic/dailycf/LuoguP9368/","content":"P9368 [ICPC 2022 Xi’an R] Streets 根据题意，我们可以想到的一个式子，假设选择 x1≤x2,y1≤y2x_1\\le x_2, y_1\\le y_2x1​≤x2​,y1​≤y2​，则 max⁡(x2−x1)(y2−y1)s.t.(x2−x1)(b1+b2)+(y2−y1)(a1+a2)≤C \\begin{array}{rc} \\max(x_2-x_1)(y_2-y_1)\\\\ s.t.(x_2-x_1)(b_1+b_2)+(y_2-y_1)(a_1+a_2)\\le C \\end{array} maxs.t.​(x2​−x1​)(y2​−y1​)(x2​−x1​)(b1​+b2​)+(y2​−y1​)(a1​+a2​)≤C​这个式子里，a1+a2,b1+b2a_1+a_2,b_1+b_2a1​+a2​,b1​+b2​ 都是单独给定的，无法进行合并，而且我们发现 x2−x1,y2−y1x_2-x_1,y_2-y_1x2​−x1​,y2​−y1​ 都是以这个形式一起出现的。而且，倘若 Δx\\Delta xΔx 值相同，那么我们肯定希望选择 ai+aja_i+a_jai​+aj​ 最小的那对 xi,xjx_i,x_jxi​,xj​. 所以，我们考虑处理出 f(dx)=min⁡∣xj−xi∣=dxai+ajf(dx)=\\min\\limits_{|x_j-x_i|=dx} a_i+a_jf(dx)=∣xj​−xi​∣=dxmin​ai​+aj​ 和 g(dy)=min⁡∣yj−yi∣=dybi+bjg(dy)=\\min\\limits_{|y_j-y_i|=dy}b_i+b_jg(dy)=∣yj​−yi​∣=dymin​bi​+bj​. 这一步可以在 O(n2+m2)O(n^2+m^2)O(n2+m2) 的时间内完成。 所以，我们的目标就变成了 max⁡dx⋅dys.t.dx⋅g(dy)+f(dx)⋅dy≤C \\begin{array}{rc} \\max dx\\cdot dy\\\\ s.t. dx\\cdot g(dy)+f(dx)\\cdot dy\\le C \\end{array} maxs.t.​dx⋅dydx⋅g(dy)+f(dx)⋅dy≤C​我们有 TTT 次询问。一个想法是，如果给定 (dy,g(dy))(dy,g(dy))(dy,g(dy))，那么我们就需要找一个满足约束条件且最大的 dxdxdx。改写式子可以得到： f(dx)≤−g(dy)dy⋅dx+Cdy f(dx)\\le -\\frac{g(dy)}{dy}\\cdot dx+\\frac{C}{dy} f(dx)≤−dyg(dy)​⋅dx+dyC​把 (dx,f(dx))(dx, f(dx))(dx,f(dx)) 标在坐标系上，所以其实我们想要的是直线 y=−g(dy)dyx+Cdyy=-\\frac{g(dy)}{dy}x+\\frac{C}{dy}y=−dyg(dy)​x+dyC​ 这条直线下方 xxx 最大的那个点。如果可以以 O(F)O(F)O(F) 的时间求出单次查询，那么我们遍历 (dy,g(dy))(dy,g(dy))(dy,g(dy)) 就可以对每一次查询求出其答案，TTT 次查询的总时间复杂度就是 O(TVF),V=105O(TVF),V=10^5O(TVF),V=105. 那么很显然，我们不能直接遍历 dxdxdx，这样 O(F)=O(V)O(F)=O(V)O(F)=O(V) 会直接 TLE. 这里，我初始的想法是直接对 (dx,f(dx))(dx,f(dx))(dx,f(dx)) 构造凸包后，直接在凸包上二分，找到一个在凸包上的点。 但是这样会有一个错误的点，就是虽然点 (x′,f(x′))(x,f(x))(x′,f(x′)) 不在凸包上，但是在直线下方，且比找到的凸包上的点靠右。 Illustration 图里，蓝色边是凸包上的边，其两个端点是凸包上的点；黄色线是约束条件变形而来的直线；zi色的点就是刚刚说的“可能会被遗漏的点”。 那么我们怎么才能算上这样的紫色点呢？我们把优化问题转化成存在性问题：既然我们要找直线下方最靠右的点 (x′,f(x′))(x,f(x))(x′,f(x′))，也就是说当 dx≤x′dx\\le xdx≤x′ 的时候，凸包上都至少有一个点在直线下方（要么是 x′xx′ 在凸包上，要么是 dx0x′dx_0\\lt xdx0​x′ 在凸包上）；当 dxx′dx\\gt xdxx′ 的时候，没有点在直线下方。 所以，我们就可以通过二分把优化问题转化成存在性问题。二分答案 xmidx_{mid}xmid​，判断 dx≥xmiddx\\ge x_{mid}dx≥xmid​ 形成的凸包中，是否有点在直线下方。 判断凸包上是否有点在一条直线下方，可以先在凸包上找到点 (x′,f(x′))(x,f(x))(x′,f(x′))，使得直线平移后会和凸包在这个点相切，再判断这个点是否在直线下方. 时间复杂度是 O(log⁡V)O(\\log V)O(logV) 的. 这个算法的时间复杂度由二分套二分决定：O(V)O(V)O(V) 枚举 (dy,g(dy))(dy,g(dy))(dy,g(dy))；对于每个 dydydy，O(log⁡V)O(\\log V)O(logV) 在 dx∈[1,V]dx\\in[1,V]dx∈[1,V] 里二分最靠右的点，每次二分里的 check_mid 还需要 O(log⁡V)O(\\log V)O(logV) 在凸包上二分找点。不过，考虑到同时维护凸包的话，把点从凸包里弹出、插入又需要 O(V)O(V)O(V)，所以整体时间复杂度会来到 O(T(V2log⁡V+Vlog⁡2V))O(T(V^2\\log V+V\\log^2 V))O(T(V2logV+Vlog2V)) 能不能再优化一下呢？答案其实具有不劣性：因为我们希望 max⁡dx⋅dy\\max dx\\cdot dymaxdx⋅dy，如果对于 dydydy 来说 dx0dx_0dx0​ 是可行的，那么对于 dy−1dy-1dy−1 来说，我们其实不需要关心 dx≤dx0dx\\le dx_0dx≤dx0​ 的所有 dxdxdx，因为 (dy−1)dx≤(dy−1)dx0dy⋅dx0(dy-1)dx\\le (dy-1)dx_0\\lt dy\\cdot dx_0(dy−1)dx≤(dy−1)dx0​dy⋅dx0​ 根本不可能更新 dydydy 计算出的答案。这就是答案的不劣性（虽然是我自己起的名字） 所以，上文里，我们并不需要每次都“在 dx∈[1,V]dx\\in[1,V]dx∈[1,V]” 里二分最靠右的点，而是可以维护一个下界的指针 ppp 并且从大到小枚举 dydydy。假设 dydydy 对应最大的 dxdxdx 是 dx0dx_0dx0​，那么对于 dy−1dy-1dy−1，我们只需要从 dx0+1dx_0+1dx0​+1 开始 find 对应的 dx′dxdx′ 即可。 由于当 dydydy 递减的时候，查找的下界也会至少 +1+1+1，所以我们最多进行 O(V)O(V)O(V) 次 find。但是考虑到如果在 [dx0+1,V][dx_0+1,V][dx0​+1,V] 上使用二分，不断地从凸包里删点、加点很可能超时。我们还需要想办法优化掉凸包的频繁删点、加点。 凸包的删点加点的复杂度来源就是虽然二分中 mid→mid′mid\\to midmid→mid′ 可以 O(1)O(1)O(1) 跳跃，但是对应的凸包却需要 O(mid′−mid)O(mid-mid)O(mid′−mid) 的时间删点加点进行维护，这部分的时间很有可能达到 O(V2)O(\\frac{V}{2})O(2V​)。 我们在预处理凸包的时候，记录下尝试加入 dx′dxdx′ 时删掉了哪些点 p[dx′]p[dx]p[dx′]（这说明点集 p[dx′]p[dx]p[dx′] 加上原本就在凸包里的点就是 dx≥dx′dx\\ge dxdx≥dx′ 对应的凸包）。那么直接考虑顺序枚举 dxdxdx，这样我们的凸包只需要弹掉 dxdxdx，加入 dx+1dx+1dx+1 和 p[dx+1]p[dx+1]p[dx+1]。而每个点最多加入、弹出 stack 各一次，因此这样维护的话总体是 O(V)O(V)O(V) 的，即均摊 O(1)O(1)O(1)。于是优化掉凸包维护后，整体的时间复杂度就是 O(TVlog⁡V)O(TV\\log V)O(TVlogV) Code #include algorithm#include cassert#include cstdio#include iostream#include ostream#include vectorusing i64 = long long;using vi = std::vectorint;constexpr int NM = 5e3 + 5;constexpr int V = 1e5 + 5;constexpr int inf = 1e9;int n, m, T;i64 C;int x[NM], y[NM], a[NM], b[NM];int vdx[V], vdy[V];int root-1;int stp = -1;vi stack, tstack;vi popped[V];bool left_side(int i, int j, int k) assert(i = j j = k); int xs = j - k; int ys = vdx[j] - vdx[k]; int xx = i - k; int yx = vdx[i] - vdx[k]; return 1ll * xs * yx = 1ll * ys * xx;i64 cost(int dx, int dy) return 1ll * dx * vdy[dy] + 1ll * dy * vdx[dx]; std::ostream operator(std::ostream os, vi v) os [; for (auto x : v) os x ,; os ] ; return os;int main() std::cin n m T; for (int i = 1; i = n; i++) std::cin x[i]; for (int i = 1; i = n; i++) std::cin a[i]; for (int i = 1; i = m; i++) std::cin y[i]; for (int i = 1; i = m; i++) std::cin b[i]; for (int i = 0; i V; i++) vdx[i] = vdy[i] = inf; for (int i = 1; i = n; i++) for (int j = i; j = n; j++) vdx[x[j] - x[i]] = std::min(vdx[x[j] - x[i]], a[j] + a[i]); for (int i = 1; i = m; i++) for (int j = i; j = m; j++) vdy[y[j] - y[i]] = std::min(vdy[y[j] - y[i]], b[j] + b[i]); for (int dx = V - 1; dx = 0; dx--) if (vdx[dx] == inf) continue; while (stp = 1 left_side(dx, stack[stp], stack[stp - 1])) popped[dx].push_back(stack.back()); stack.pop_back(); stp--; if (root == -1) root = dx; stack.push_back(dx); stp++; tstack.resize(stack.size()); std::copy(stack.begin(), stack.end(), tstack.begin()); while (T--) std::cin C; int now_dx = 0; i64 ans = 0; stack.clear(); for (auto v : tstack) stack.push_back(v); auto next = [] if (now_dx root) return; // 维护凸包，先弹掉顶部的点 assert(stack.back() == now_dx stack not match); stack.pop_back(); // 再加入点 for (auto it = popped[now_dx].rbegin(); it != popped[now_dx].rend(); it++) stack.push_back(*it); now_dx++; // 移动到下一个可以得到的 dx while (now_dx = root vdx[now_dx] == inf) now_dx++; if (now_dx = root) assert(stack.back() == now_dx stack not match after next()); ; // 二分，判断是否有点 x, f(x) 在直线下方，且 x = dx. auto BS = [](int dx, int dy) if (dx == 0) return true; std::reverse(stack.begin(), stack.end()); stack.push_back(V - 1); int l = -1, r = stack.size() - 1; while (l + 1 r) int mid = l + ((r - l) 1); int middx = stack[mid]; int nxtdx = stack[mid + 1]; if (cost(middx, dy) cost(nxtdx, dy)) l = mid; else r = mid; bool res = cost(stack[r], dy) = C; stack.pop_back(); std::reverse(stack.begin(), stack.end()); return res; ; for (int dy = V - 1; dy = 1 now_dx = root; dy--) if (vdy[dy] == inf) continue; // 跳过无法取到的 dy. // std::fprintf(stderr, dy=%d, now_dx=%d , dy, now_dx); while (now_dx = root BS(now_dx, dy)) ans = std::max(ans, 1ll * dy * now_dx); next(); std::cout ans ;"},{"title":"P4151 [WC2011] 最大XOR和路径","path":"/topic/dailycf/LuoguP4151/","content":"P4151 [WC2011] 最大XOR和路径 这道题的关键在于对路径的处理。 我们先考虑如果给定的图是树的特殊情况。这种情况下，1→n1\\to n1→n 的最大异或和必定是 1→n1\\to n1→n 的简单路径，因为如果走分支，那么分支上边一定会走两次（沿着边往下走，沿着边走回来），其值异或两次后抵消。 然后我们给树加上一些非树边。比如说考虑 1 21 33 43 52 4 2 4 是非树边，我们想求 1→51\\to 51→5 的最大异或和路径。不同于 1→3→51\\to 3\\to 51→3→5，我们这次可以选择先绕 1→3→4→2→1→31\\to 3\\to 4\\to 2\\to 1\\to 31→3→4→2→1→3 走一圈，再走到 555. 这条路径的异或和就是 1→51\\to 51→5 的树上简单路径加上 1,2,3,41,2,3,41,2,3,4 这个环的异或值。 所以，类似的，我们对原图求出一棵 DFS 树，对于非树边 (u,v)(u,v)(u,v)，记下环 (u,v,LCA(u,v))(u,v,LCA(u,v))(u,v,LCA(u,v)) 上的异或和 Cu,vC_{u,v}Cu,v​；同时找出 1→n1\\to n1→n 的树上简单路径的异或和 PPP。我们的答案，一定就是 P,Ci,jP,C_{i,j}P,Ci,j​ 异或出来的最大值（其中 PPP 是必选的）。我们对 Ci,jC_{i,j}Ci,j​ 构造线性基，用贪心法找最大值即可。 Code #include iostream#include tuple#include vectorconstexpr int N = 5e4 + 5;constexpr int M = 1e5 + 5;constexpr int B = 63;using i64 = long long;using pil = std::tupleint, i64;using pii = std::tupleint, int;using edge = std::tupleint, i64;using vpil = std::vectorpil;using vi = std::vectorint;int n, m;vi G[N], T[N];edge E[M];int on_tree[M], vis[N];std::vectori64 basis;i64 sinceRoot[N];void tree(int u, int fa) vis[u] = true; for (auto eid : G[u]) auto [uv, w] = E[eid]; int v = uv ^ u; if (v == fa) continue; if (vis[v]) basis.push_back(w ^ sinceRoot[u] ^ sinceRoot[v]); continue; sinceRoot[v] = sinceRoot[u] ^ w; on_tree[eid] = 1; T[u].push_back(eid); T[v].push_back(eid); tree(v, u); int main() // std::cin.tie(0)-sync_with_stdio(0); std::cin n m; for (int i = 1, u, v; i = m; i++) i64 w; std::cin u v w; E[i] = u ^ v, w; G[u].emplace_back(i), G[v].emplace_back(i); tree(1, 0); // linear basis int row = 0; int len = basis.size(); auto checkbit = [](i64 x, int b) return (x b) 1; ; for (int col = B; col = 0 row len; col--) for (int to = row; to len; to++) if (checkbit(basis[to], col)) std::swap(basis[to], basis[row]); break; if (not checkbit(basis[row], col)) continue; for (int i = 0; i len; i++) if (i == row) continue; if (checkbit(basis[i], col)) basis[i] ^= basis[row]; row++; i64 ans = sinceRoot[n]; for (int i = 0; i row; i++) if ((ans ^ basis[i]) ans) ans ^= basis[i]; std::cout ans ;","tags":["线性基"]},{"title":"软件工程基础：面向对象编程","path":"/topic/swe/swe-basic-oop/","content":"抽象 Abstraction 建模，模拟真实对象的特定属性和行为。 封装 Capsulation 接口 interface: 它是对象的公有部分， 能够同其他对象进行交互 封装是指一个对象对其他对象隐藏其部分状态和行为，而仅向程序其他部分暴露有限的接口的能力 继承 Inheritance 多态 多态是指程序能够检测对象所属的实际类，并在当前上下文不知道其真实类型的情况下调用其实现的能力"},{"title":"Vim 快捷键操作","path":"/vim-keys-001/","content":"基础移动 h j k l 当移动的命令为小写时，连续的标点也会被视为单词。 w 移动到下一个单词的开头 word b 上一个单词的开头 backward e 下一个单词的末尾 end ge 上一个单词的末尾 go end 但是当移动的命令为大写时，夹在字母中间的标点会被视为单词的一部分： W B E gE 还有比较实用的移动是搜索 (find)下一个字符并移动： fcharacter 搜索到下一个 character 字符，然后光标移动到其上。 Fcharacter 搜索上一个 character 字符，移动到其上 tcharacter 搜索到下一个 character 字符，然后光标移动到它之前。 until Tcharacter 搜索到上一个 character 字符，然后移动到它之前。 输入 ; 快速查找下一个相同的字符，输入 , 查找上一个相同的字符。 例如 fd;;v -- v ------ v ------- viniti dext tump dtaius tytyd Advanced Movings 行内移动： 0 直接移动到开头 ^ 直接移动到当前行第一个非空白字符 $ 行末 g_ 最后一个非空白字符 行间移动： 跳过下一个段落（段落是连续的行，中间没有空行隔开） 类似，但是跳到上一个段落之前 ctrl + d (down) 往下翻半页 ctrl + u (up) 往上翻半页 搜索并移动：（之前提到的 fcharacter 之内在行内搜索单个字符） /pattern 搜索下一个匹配 pattern 的字符串并移动到那里。可以是字符串，也可以是正则 ?pattern 类似，但是是上一个匹配的字符串 输入完后会高亮匹配的字符串，输入 Enter 进行跳转 再输入 n 跳转到下一个匹配的字符串，N 则是上一个 (next) /enter 直接输入斜杠然后回车，vim 会执行上一次搜索过的 pattern。?enter 也是同理。 或者，输入 * 搜索下一个当前光标所在的单词，# 则是上一个 文件跳转 gd (goto definition) 跳转到光标所在的东西的定义（例如函数定义、变量定义） gf (goto file) jump to a file in an import Some More gg 文件开头 G 文件末尾 % 如果光标不在括号上，跳转到包含当前单词的左括号上；如果在左/右括号上，跳到与之匹配的另一个括号上 linegg 快速跳转到行"},{"title":"ManiSkill 里的 MuJoCo Robot File","path":"/mjcf-urdf/","content":"MuJoCo XML in ManiSkill MuJoCo 物件由 .xml 定义的，这个 .xml 真的巨复杂，而且疑似 ManiSkill 里 .xml 的标准和 LIBERO 里的还不一样，醉了…… asset.texture asset.mesh asset.material geom.contype 如果 geom 里有 contype=1 属性，那么代表这个部件有碰撞体积. contype: int，默认值 1 该属性和下一个属性共同定义了 32 位整型的位掩码，用于对动态生成的接触对进行过滤。详见 “Computation” 章节中的 Collision detection。一对几何体只有在「几何体 A 的 contype 与几何体 B 的 conaffinity 兼容」或「几何体 B 的 contype 与几何体 A 的 conaffinity 兼容」时才会碰撞。所谓“兼容”，指的是两个位掩码至少有一个相同的比特被置为 1。 conaffinity: int，默认值 1 用于接触过滤的位掩码；说明参见上文 contype。[1] geom.group MuJoCo XML 里，group=0, 1, 2 则可见，否则不可见（但是 ManiSkill/SAPIEN 里 group=1 却是不可见） MuJoCo XML Reference ↩︎"},{"title":"HuggingFace, GitHub 添加 SSH Key","path":"/git-hf-sshkey/","content":"本地生成 SSH Key 首先需要本地生成一个 SSH Key，这个 Key 可以和之前已经创建过的相同，也可以放到别的地方。只需要自己指定存放位置即可。 ssh-keygen -t ed25519 -C ...# -t 选择加密算法，-C 后面是注释 这样就会生成两个 keygen，一个是 private key（后缀无 .pub），一个是 public key（后缀带 .pub）。 然后重要的一点：一定要把 keygen 加入 ssh agent ssh-add ssh private key location"},{"title":"Haskell, Parameterized Types","path":"/topic/coding/functional/haskell/haskell-param-types/","content":"Maybe T 和 Rust 里的 OptionT 差不多。 Just (something) Nothing Either T U 保存着两个类型中的一个。例如 Either Int Bool 可以是 Left 0, Right False i_want_a_string :: Either Int String - Stringi_want_a_string (Left num) = show numi_want_a_string (Right str) = str"},{"title":"Haskell 快速入门","path":"/topic/coding/functional/haskell/haskell-crash-course/","content":"Variables Variables: Local Definition 使用 let ... in 语法，或者 where 语法。where 后置变量定义，let ... in 则是前置。 示例： solve :: Int - Int - [Int] - Stringsolve n m doors = do case indices of [] - YES _ - case end - start + 1 of x | x = m - YES _ - NO where -- 这里就是 local definition 定义的局部变量 indices = elemIndices 1 doors end = last indices start = head indices Pattern Matching Pattern Matching 定义函数的时候，可以根据参数的不同，触发不同分支。 greet :: String - String - Stringgreet Finland name = Hei, ++ namegreet Italy name = Ciao, ++ namegreet England name = How do you do, ++ namegreet _ name = Hello, ++ name Guards Condition Guards 和 Pattern Matching 类似，用 Condition 代替具体的值进行 Matching. 示例： describe :: Int - Stringdescribe n | n == 2 = Two | even n = Even | n == 3 = Three | n 100 = Big!! | otherwise = The number ++ show n 当然，Guards 和 Pattern Matching 也可以一起用。 guessAge :: String - Int - StringguessAge Griselda age | age 47 = Too low! | age 47 = Too high! | otherwise = Correct!guessAge Hansel age | age 12 = Too low! | age 12 = Too high! | otherwise = Correct!guessAge name age = Wrong name! case ... of 我觉得 case ... of 语法也算 Pattern Matching 的一部分 case xxx of value1 - return_value1 value2 - return_value2 case ... of 可以用在“当多个分支都需要使用同一个函数时” Recursion Helper Function: arguments of the helper function are variables you update in your loop; Tail Recursion Optimization Haskell programs often use the apostrophe to name helper functions and alternative versions of functions. Haskell 常用数据结构 List / [a] head :: [a] - a -- returns the first elementlast :: [a] - a -- returns the last elementtail :: [a] - [a] -- returns everything except the first elementinit :: [a] - [a] -- returns everything except the last elementtake :: Int - [a] - [a] -- returns the n first elementsdrop :: Int - [a] - [a] -- returns everything except the n first elements(++) :: [a] - [a] - [a] -- lists are catenated with the ++ operator(!!) :: [a] - Int - a -- lists are indexed with the !! operatorreverse :: [a] - [a] -- reverse a listnull :: [a] - Bool -- is this list empty?length :: [a] - Int -- the length of a list 实用函数 show :: Any - String: 将任何东西变成可以输出的字符串"},{"title":"数学分析一 Ep.2：实数、有理数","path":"/topic/analysis/math-analysis-2/","content":"从这里开始，我们开始开从实数拓展到有理数 命题一：实数包括有理数 实数定理 R\\RR 包含所有有理数 Q\\mathbb QQ，即存在单射 f:Q↦Rf:\\mathbb{Q}\\mapsto \\Rf:Q↦R，使得对 ∀x,y∈Q\\forall x, y\\in\\mathbb{Q}∀x,y∈Q，有 f(x+Qy)=f(x)+f(y)f(x⋅Qy)=f(x)⋅f(y)x≤Qy ⟹ f(x)≤f(y) f(x+_{\\mathbb{Q}}y)=f(x)+f(y)\\\\f(x\\cdot_{\\mathbb{Q}}y)=f(x)\\cdot f(y)\\\\x\\le_\\mathbb{Q}y\\implies f(x)\\le f(y) f(x+Q​y)=f(x)+f(y)f(x⋅Q​y)=f(x)⋅f(y)x≤Q​y⟹f(x)≤f(y)其中，+Q,⋅Q+_{\\mathbb{Q}},\\cdot_{\\mathbb{Q}}+Q​,⋅Q​ 是有理数上的加法和乘法。映射 fff 依然保持序关系和域关系。"},{"title":"Tokenization, BPE 算法","path":"/topic/cs336/tokenization-bpe/","content":"BPE Training 的流程和实现 CAUTION 本部分对应的是 Section 2 的 BPE. Tokenization 是如何工作的呢？假定我们的文本放在 corpus.txt 文件里（这里的文本带有 special tokens 如 |endoftext|） 通常，我们的文本文件很大，例如几个 GB，我们不可能把这些字符全都载入内存里（肯定爆炸），因此，第一步，我们需要对训练文本分块，以便发挥多线程的优势，让计算机并行处理多个任务。分块需要注意，每一块都必须是以 special token 结尾的，否则如果横跨了某个单词，那么这个单词被 tokenize 的结果很可能与 expected 的不同。 chunkify 实现 根据上面所说的，我们把文本进行分块。我们用 Binary IO 的方式打开文本，最后返回 List[int] 表示 chunk 的边界为 B[i-1] ~ B[i] 具体实现的话，我们用 file.tell() 的方式获取 bytes 数量后，直接先均分成 num 块（或者按内存大小计算块的大小，然后反过来计算块的数量）。然后，对每一块寻找下一个 special token 出现的位置，调整这一块的 boundary，这样，boundary 的每一个 int 都表示了 special token 的位置，也就满足了对 special token 出现位置的约束。最后排序去重就是最终的 boundary 了。 def chunkify(file: BinaryIO, specials: List[bytes], num_chunks: int) - List[int]: # get total bytes file.seek(0, os.SEEK_END) file_size = file.tell() file.seek(0) chunk_size = file_size // num_chunks boundaries = [i * chunk_size for i in range(num_chunks + 1)] boundaries[-1] = file_size mini_chunk = 4096 for i in range(1, len(boundaries) - 1): init_pos = boundaries[i] file.seek(init_pos) # 这里就是不断读取 4096 个字节，然后找有没有 special token while True: sub_chunk = file.read(mini_chunk) if sub_chunk == b: boundaries[i] = file_size break special_pos = [ sub_chunk.find(token) for token in specials if sub_chunk.find(token) != -1 ] # 读进来的小 chunk 有 special token，那么直接截断 if len(special_pos) != 0: special_pos = min(special_pos) boundaries[i] = init_pos + special_pos break # 否则继续找 special token init_pos += mini_chunk return sorted(set(boundaries)) 然后就要 tokenize 了。但是对于很大的语料库而言，会包含很多相同的单词（例如 the 这个单词可以在很多地方出现很多次），如果我们 naive 地对一长串 bytes 计算 byte-pair count，时间复杂度是很高的（bytes 实在太多了）。比如说…… print(len(这是一段文字))6 print(len(这是一段文字.encode(utf-8)))18 bytes 数量差了 2 倍！所以我们需要基于“很多单词是重复的，如果单词出现 aaa 次，那么这个单词所构成的 byte-pairs 也至少出现 aaa 次”这一观察，对 chunk of text 进行 Pre-Tokenization，得到词频统计。而词频统计字典的单词数通常比 byte string 长度短太多了。 对于英文文本来说，OpenAI 在 GPT2 里曾使用过 RegEx 分割单词（主要通过空格、句号等）；对于中文、日文等不依赖空格的语言，基本上也有对应的库，如 jieba 等等。这里就只展示英文 pre-tokenization 的做法。Pre-Tokenization 也可以利用多线程并行。 # file: 文件# specials: 特殊 token，需要先按 special token 将段落分割成互不干扰的小段落。# start, end: 因为是多线程进行 pre-tokenization，这里的 start, end 对应 chunkify 出来的一个 chunkdef count_token(file: BinaryIO, specials: List[str], start: int, end: int): file.seek(start) data = file.read(end - start).decode(utf-8, errors=ignore) # Pretokenization, using split # regex.escape 用来转义 |endoftext| 中的竖线，正则里的竖线表示“或者” sentences = regex.split( |.join(regex.escape(special) for special in specials), data, ) # 正则分割出 pre-token 然后计数 PAT = r(?:[sdmt]|ll|ve|re)| ?\\pL+| ?\\pN+| ?[^\\s\\pL\\pN]+|\\s+(?!\\S)|\\s+ tokens_count = defaultdict(int) for sentence in sentences: # 注意这里必须逐个句子处理 # 如果 .join 的话，可能 join 的 也会被当成 token 的一部分。 pre_tokens = regex.finditer( PAT, sentence, ) for token in pre_tokens: tokens_count[token.group(0)] += 1 assert not any(tok in tokens_count for tok in specials) # assert | not in tokens_count return tokens_count 得到词频统计后，我们就可以 merge bytes 了。merge bytes 的过程是不断合并出现次数最多的 byte-pair，具体来说就是： 我们首先需要把单词表示成 a sequence of bytes，我们的字典保存的是 Dict[Tuple[bytes, ...], int]，即词频（但是词是 tuple of bytes） 遍历词典，统计 byte-pair count 取出 count 最多的 byte-pair，如果有多个，则取 byte value 更大的那个 merges 记录下这个要合并的 byte-pair，vocabulary 也新增一个条目记录这个 byte-pair（他们即将成为一个新的 token） 遍历词典，如果某个单词含有这个 byte-pair 则合并，在词典里更新其 tuple of bytes representation。 如，我想合并 x 和 y，而某个单词的 tuple of bytes 是 [a, b, c, x, y, d, f, e]，那么合并后就变成了 [a, b, c, xy, d, f, e]. 这里有一个小小的优化：显然，合并完一个 byte-pair 之后，只有“在这个单词里和这个 byte-pair 相交的其他 byte-pair 的数量会受到影响”。基于这一点，我们遍历单词的时候，同时检查和当前 byte-pair 相交的其他 byte-pair，然后减去单词的出现次数，并新增条目（受影响的 byte-pair 的一部分和新的 byte-pair 形成的 token）。这样就可以相对高效地进行 merge. def bytepair( tokens: defaultdict[str, int], vocab_size: int, init_vocab: defaultdict[int, bytes], init_merge: List[Tuple[bytes, bytes]],) - Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]: vocabulary = init_vocab merges = init_merge # 这一步，我们先把所有 str 表示的单词转成 tuple of bytes toks = defaultdict(int) for token, count in tokens.items(): btoken = token.encode(utf-8) toks[tuple(btoken[i : i + 1] for i in range(len(btoken)))] = count # 先插入初始的 256 的 bytes for i in range(256): insert_vocabulary(vocabulary, bytes([i])) # bp_cnt 的作用就是统计 byte-pair counting bp_cnt = construct(toks) for _ in tqdm( # 这里加了一个进度条可视化 range(vocab_size - len(vocabulary)), desc=merging byte token pairs, total=vocab_size, initial=len(vocabulary), ): # 提取出现次数最多的 byte-pair count, pair = max([(cnt, tokpair) for tokpair, cnt in bp_cnt.items()]) newbyte = pair[0] + pair[1] logger.debug(fmerging pair[0] and pair[1]) # 插入词汇表 insert_vocabulary(vocabulary, pair[0] + pair[1]) # 记录 merge merges.append(pair) # 当前的 pair 会被记录成一个 token，不算 byte-pair 了 # 因此从 byte-pair counting 里删除 bp_cnt.pop(pair) affected = [] for token in toks: # 如果单词不包含这个 byte-pair 那么直接跳过 if not contain(token, pair): continue new_token = [] # 下面的循环是将 token 里的 byte-pair 合并起来 skip_next = False for i in range(len(token)): if skip_next: skip_next = False continue if i len(token) - 1 and (token[i], token[i + 1]) == pair: new_token.append(pair[0] + pair[1]) # 这里就是上面说的优化，只影响与 byte-pair 相交的 bytes # 两个 if 语句考虑的边界的情况 if i != 0: bp_cnt[(token[i - 1], token[i])] -= toks[token] bp_cnt[(token[i - 1], newbyte)] += toks[token] if i + 1 != len(token) - 1: bp_cnt[(token[i + 1], token[i + 2])] -= toks[token] bp_cnt[(newbyte, token[i + 2])] += toks[token] skip_next = True else: new_token.append(token[i]) new_token = tuple(new_token) affected.append((token, toks[token], new_token)) # 由于更新了单词的 bytes 表示，所以单词表也要同步更新 for old, cnt, new in affected: toks.pop(old) toks[new] = cnt return (vocabulary, merges) Serialization 注意事项 我们把 merges 和 vocabulary 写入文件时，如果直接写入，会遇到一个问题： 有一些 bytes 无法以 ASCII 的形式呈现，比如说 b'\\x80' 有一些空白字符比如空格，如果直接写入文件，日后再读取的时候解析就会比较困难。merges.txt 有时 面对这些情况我们有一些处理方法：我们考虑把所有无法以 ASCII 呈现的字符映射到 ≥256\\ge 256≥256 的字符上。可以使用 utility 工具 gpt2_bytes_to_unicode()。其原理就是先筛选出 256\\lt 256256 里 printable 的字符，然后对于剩下的字符，映射到 ≥256\\ge 256≥256 的字符上，返回一个字典。 def bytes2unicode_serializer() - Dict[int, str]: bytes_list = ( list(range(ord(!), ord(~) + 1)) + list(range(ord(¡), ord(¬) + 1)) + list(range(ord(®), ord(ÿ) + 1)) ) copy = bytes_list[:] n = 0 for b in range(256): if b in bytes_list: continue bytes_list.append(b) copy.append(256 + n) n += 1 d = dict(zip(bytes_list, [chr(x) for x in copy])) return d Tokenizer Encode/Decode 的流程和实现 regex.split() 的使用 regex.split(REGEX, STRING) 可以按 REGEX 分割字符串，但是默认不会保留匹配了 REGEX 的部分。可以通过在外面加一个圆括号 (REGEX) 让 regex 能够保留匹配的部分。 Encoding Tokenizer Encoding 的过程是接受一个字符串 str，然后输出 List[int]。 我们现在已有的是 What We Have Type Meaning vocabulary Dict[int, bytes] 记录了 bytes 对应的编码 merges List[Tuple[bytes, bytes]] 记录了 bytes 合并的先后顺序 这里，合并的先后顺序很重要，因为我们需要正确模拟出 training 过程中它是怎么被合并的。 不能使用贪心法进行合并！ 例如，考虑单词 abcde，我们在 training 时先合并 b c 然后再合并 a b，这意味着 b c 的数量比 a b 多。如果使用贪心法合并，我们会得到 ab c d e，而正确的是 a bc d e。和正确的 tokenization 会有出入。 Encoding 和 Train BPE 的时候类似，我们都先把文本划分成多个 chunks 以利用多线程优势，这里的 chunk 仍然需要以 special tokens 作为结尾来保证不会横跨某个 token. 不过 chunkify 其实是可选的，因为通常 encode 的文本相比训练文本短很多。 def _chunkify(self, text: str, num_chunks: int) - List[int]: chunk_size = len(text) // num_chunks boundaries = [i * chunk_size for i in range(num_chunks + 1)] boundaries[-1] = len(text) mini_chunk_size = 4096 for i in range(1, len(boundaries) - 1): init_pos = boundaries[i] while True: sub_chunk = text[init_pos : init_pos + mini_chunk_size] if sub_chunk == : boundaries[i] = len(text) break special_pos = [ sub_chunk.find(token) for token in self.special_tokens if sub_chunk.find(token) != -1 ] if len(special_pos) != 0: special_pos = min(special_pos) boundaries[i] = init_pos + special_pos break init_pos += mini_chunk_size return sorted(set(boundaries)) 接下来，我们还需要将文本转化为 pretokens，和 train BPE 时一致. 这里的一个优化是用 yield 返回迭代器，而不是直接返回一个 list，可以大大减少内存消耗 def _pretokenize(self, text: str) - Iterable[List[str]]: data = ( regex.splititer( f(|.join(regex.escape(s) for s in self.special_tokens)), text, ) if self.special_tokens else [text] ) # split into sentences, and special_tokens for each_sentence in data: if each_sentence in self.special_tokens: yield [each_sentence] else: yield regex.findall(self.pretoken_pattern, each_sentence) 然后我们需要把每一个 pretoken 转化成 a list of bytes，对 bytes 执行合并，最后转化成 a list of index，整段文本的 encoding 结果就是所有的 list of index 拼接起来. 下面的 _apply_merge() 是针对单个 pretoken 分解的结果。具体做法就是不断从前往后遍历 merge list（注意必须是这个顺序），能合并则合并。 _convert_to_index() 则接受迭代器，负责把 pretoken 的 index list 拼接在一起。 def _apply_merge(self, token: bytes) - List[int]: words = [bytes([i]) for i in token] for merge in self.merges: if len(words) == 1: break # no more merges possible new_word = [] skip_next = False for i in range(len(words)): if skip_next: skip_next = False continue if ( i != len(words) - 1 and words[i] == merge[0] and words[i + 1] == merge[1] ): new_word.append(merge[0] + merge[1]) skip_next = True else: new_word.append(words[i]) words = new_word return [self.inverse_vocab[word] for word in words]def _convert_to_index(self, pre_tokens: Iterable[str]) - List[int]: tokens_bytes = [tok.encode(utf-8) for tok in pre_tokens] result: List[int] = [] for token_byte in tokens_bytes: if token_byte in self.special_tokens_bytes: result.append(self.inverse_vocab[token_byte]) continue token_ids = self._apply_merge(token_byte) result.extend(token_ids) return result 最后，我们就可以实现 encode() 的逻辑了（实际上就是把上面的部分拼在一起）. def encode(self, text: str) - List[int]: pre_tokens_list = self._pretokenize(text) with Pool(processes=10) as pool: tokens = pool.map( self._convert_to_index, pre_tokens_list, ) tokens = [token for sublist in tokens for token in sublist] return tokens Decoding Decoding 就很简单了，因为 decoding 接受 a list of index，所以我们直接查词汇表找出对应的 bytes，拼接起来，然后转成字符串即可。 这个 _chunkify_token_list() 的思路和上文 _chunkify() 的思路其实差不多。decode 也不是特别需要利用多线程。 def decode(self, tokens: List[int]) - str: boundaries = self._chunkify_token_list(tokens, 25) with Pool(processes=10) as pool: results = pool.map( self._decode_chunk, [tokens[i:j] for i, j in zip(boundaries[:-1], boundaries[1:])], ) decoded_bytes = b.join(results) decoded_str = decoded_bytes.decode(utf-8, errors=replace) return decoded_str"},{"title":"南洋理工交换指北","path":"/topic/exchanging/ntu-exchange-guide/","content":"HKUWW 阶段 实际上这一个阶段"},{"title":"数学分析一 Ep.1 ：实数的公理化描述、四条公理","path":"/topic/analysis/math-analysis-1/","content":"实数 我们定义实数 R\\RR 是一个集合，上面有两个操作： 加法 +:R×R↦R,(x,y)↦x+y+:\\R\\times\\R\\mapsto\\R, (x,y)\\mapsto x+y+:R×R↦R,(x,y)↦x+y 乘法 ⋅:R×R↦R,(x,y)↦x⋅y\\cdot: \\R\\times\\R\\mapsto\\R, (x,y)\\mapsto x\\cdot y⋅:R×R↦R,(x,y)↦x⋅y 同时还有序关系 (Order Relation) ≤:x≤y\\le: x\\le y≤:x≤y. 域公理 (Axioms of Field) 加法交换律：x+y=y+xx+y=y+xx+y=y+x 加法结合律：x+(y+z)=(x+y)+zx+(y+z)=(x+y)+zx+(y+z)=(x+y)+z 加法单位元：存在 0∈R0\\in\\R0∈R 使得对 ∀x∈R\\forall x\\in\\R∀x∈R 有 0+x=x0+x=x0+x=x 成立 加法逆元的存在性、唯一性：对 ∀x∈R\\forall x\\in\\R∀x∈R，存在且只存在一个 −x∈R-x\\in\\R−x∈R，使得 x+(−x)=0x+(-x)=0x+(−x)=0 这里，我们还没有证明 (−1)⋅x=−x(-1)\\cdot x=-x(−1)⋅x=−x，−x-x−x 整体应该被当作一个记号，用于表示 xxx 的加法逆元。 乘法结合律：x⋅(y⋅z)=(x⋅y)⋅zx\\cdot (y\\cdot z)=(x\\cdot y)\\cdot zx⋅(y⋅z)=(x⋅y)⋅z 乘法交换律：x⋅y=y⋅xx\\cdot y=y\\cdot xx⋅y=y⋅x 乘法单位元：存在 1∈R,1≠01\\in\\R,1 e 01∈R,1=0，使得对任意 ∀x∈R\\forall x\\in\\R∀x∈R 都有 1⋅x=x1\\cdot x=x1⋅x=x 乘法逆元的存在性、唯一性：对任意 ∀x∈R−{ 0 }\\forall x\\in\\R-\\set{0}∀x∈R−{0}，存在 x−1∈Rx^{-1}\\in\\Rx−1∈R 使得 x⋅x−1=1x\\cdot x^{-1}=1x⋅x−1=1 乘法分配律：x⋅(y+z)=x⋅y+x⋅zx\\cdot (y+z)=x\\cdot y+x\\cdot zx⋅(y+z)=x⋅y+x⋅z 练习题 证明 对任意的 x,y∈R,b≠0x,y\\in\\R,b e0x,y∈R,b=0，有 x+a=y+a ⟹ x=yx⋅b=y⋅b ⟹ x=yx+a=y+a\\implies{x=y}\\\\x\\cdot{b}=y\\cdot{b}\\implies{x=y}x+a=y+a⟹x=yx⋅b=y⋅b⟹x=y 先证明第一个。我们在方程两边同时加上 aaa 的加法逆元 −a-a−a，则 x+a+(−a)=y+a+(−a)x+[a+(−a)]=y+[a+(−a)]x+0=y+0x=y \\begin{aligned} x+a+(-a)=y+a+(-a)\\\\ x+[a+(-a)]=y+[a+(-a)]\\\\ x+0=y+0\\\\ x=y \\end{aligned} x+a+(−a)x+[a+(−a)]x+0x​=y+a+(−a)=y+[a+(−a)]=y+0=y​ 再证明第二个。 证明 对任意 x,y,z,wx,y,z,wx,y,z,w，若 y,w≠0y,w e 0y,w=0，则有 xy+zw=xw+zyyw \\frac{x}{y}+\\frac{z}{w}=\\frac{xw+zy}{yw} yx​+wz​=ywxw+zy​ 序公理 (Axioms of Order) 序的传递性：x≤y, y≤z ⟹ x≤z.x \\le y,\\; y \\le z\\;\\Longrightarrow\\;x \\le z.x≤y,y≤z⟹x≤z. 序可以决定元素：x≤y, y≤x ⟹ x=yx\\le y,\\; y\\le x \\implies x=yx≤y,y≤x⟹x=y 全序关系：∀x,y∈R, x≤y\\forall x,y\\in\\R,\\; x\\le y∀x,y∈R,x≤y 或者 y≤xy\\le xy≤x 二者至少有一个正确 与加法相容：x≤y ⟹ x+z≤y+zx\\le y\\implies x+z\\le y+zx≤y⟹x+z≤y+z 与乘法相容：x≥0∧y≥0 ⟹ xy≥0x\\ge 0\\land y\\ge 0 \\implies xy\\ge 0x≥0∧y≥0⟹xy≥0 练习题 证明题 10 1\\gt 0 10 证明 证明 反证法：若 101\\lt 010，则 −10-1\\gt 0−10，由于和乘法相容，有 1⋅(−1)0⋅(−1)−10 1\\cdot (-1)\\lt 0\\cdot (-1)\\\\ -1\\lt 0 1⋅(−1)0⋅(−1)−10矛盾！故 101\\gt 010 证明： x≥0 ⟹ −x≤0 x\\ge 0 \\implies -x\\le 0 x≥0⟹−x≤0 证明 证明 因为和加法相容，则 x+(−x)≥0+(−x)x+(-x)\\ge 0+(-x)x+(−x)≥0+(−x)，故有 0≥−x ⟺ −x≤00\\ge -x \\iff -x\\le 00≥−x⟺−x≤0. 证明： y∈R,y0 ⟹ 0y−11 y\\in\\R,y\\gt 0 \\implies 0\\lt y^{-1}\\lt 1 y∈R,y0⟹0y−11 证明 证明 首先证明，y1 ⟹ y−10y\\gt 1\\implies y^{-1}\\gt 0y1⟹y−10. 反证法，假设 y−1=0y^{-1}= 0y−1=0，那么根据 Field Axioms，我们有 1=y−1⋅y≤0⋅y=0 1=y^{-1}\\cdot y\\le 0\\cdot y=0 1=y−1⋅y≤0⋅y=0 而这与 1≠01 e 01=0 的定义不符。 假设 y−10y^{-1}\\lt 0y−10，则根据上面证明的，有 −y−10-y^{-1}\\gt 0−y−10，由于和乘法相容，故 y⋅(−y−1)0+(−y−1)−1−(y−1)y−11 \\begin{aligned} y\\cdot (-y^{-1})\\gt 0+(-y^{-1})\\\\ -1\\gt -(y^{-1})\\\\ y^{-1}\\gt 1 \\end{aligned} y⋅(−y−1)−1y−1​0+(−y−1)−(y−1)1​再由 y10y\\gt 1\\gt 0y10，故 y−1⋅y1⋅y ⟺ 1yy^{-1}\\cdot y\\gt 1\\cdot y \\iff 1\\gt yy−1⋅y1⋅y⟺1y，矛盾！ 故必有 y−10y^{-1}\\gt 0y−10，再由于和乘法相容，有 y⋅y−11⋅y−11y−1 y\\cdot y^{-1}\\gt 1\\cdot y^{-1}\\\\ 1\\gt y^{-1} y⋅y−11⋅y−11y−1所以，0y−110\\lt y^{-1}\\lt 10y−11 Achimedes 公理 Achimedes Axioms 对任意 x0x\\gt 0x0 和 yyy，总存在正整数 nnn，使得 n⋅x≥yn\\cdot x\\ge yn⋅x≥y."},{"title":"Rust 原子操作与内存顺序","path":"/topic/rust-grammar/rust-atomic/","content":"AtomicT 内存顺序"},{"title":"Rust 同步：Semaphore, 条件变量, 互斥锁","path":"/topic/rust-grammar/rust-concurrency-condvar-sema-mutex/","content":"MutexData 数据被 MutexT 拥有，获得锁用 m.lock()（会阻塞线程，返回 Result），在作用域结束后锁会自动释放。 准确地说，mutex.lock() 返回一个智能指针 MutexGuardT。它实现了 Deref，会自动解引用，指向 MutexT 内的数据。还实现了 Drop，作用域结束之后自动释放锁。 mutex.try_lock() 则尝试获取锁，返回包含数据或者错误的 Result 读写锁 RwLockT 相比于 MutexT 同一时间允许多个 read，但只允许一个 write。读写不允许同时存在 多个 read 的时候不会阻塞 std::sync::CondVar 条件变量 tokio::sync::Semaphore 信号量"},{"title":"Rust 泛型","path":"/topic/rust-grammar/rust-generics/","content":"泛型（对标 C++ 模板） 函数、方法、结构体、枚举都可以使用泛型。 Type Annotation 明确指出泛型的类型需要满足什么条件。 函数泛型/函数模板 fn funcT(a: T) template typename Tvoid function(T a) 方法泛型/方法模板 可以包含其他的类型 implT PointT fn funcU(self) - T // 等价于 ...... template typename Tclass Point template typename U T func() const 泛型、模板参数 对应 C++ 中 template int N 这样的模板参数。 fn funcT, const N: usize(arr: [T; N]) template typename T, size_t Nvoid func(std::arrayT, N arr) 针对 const 泛型做检查 这个在 C++ 里应该需要使用 require 做检查，我还没有研究过。Rust 里使用 Assert: IsTrue 泛型限制即可。 fn somethingT(val: T)where Assert core::mem::size_of::T() 768 : IsTrue, // ^ 这里是一个 const 表达式，换成其它的 const 表达式也可以 // const fn 对应 C++ 的 constexpr，在编译期求值。"},{"title":"Rust 线程同步：消息传递","path":"/topic/rust-grammar/rust-concurrency-msg-passing/","content":"单发送者，单接受者 使用 std::sync::mpsc::channel() (multiple producer, single consumer) use std::sync::mpsc;use std::thread;fn main() // 创建一个消息通道, 返回一个元组：(发送者，接收者) let (tx, rx) = mpsc::channel(); // 创建线程，并发送消息 thread::spawn(move || // 发送一个数字1, send方法返回ResultT,E，通过unwrap进行快速错误处理 tx.send(1).unwrap(); // 下面代码将报错，因为编译器自动推导出通道传递的值是i32类型，那么Optioni32类型将产生不匹配错误 // tx.send(Some(1)).unwrap() ); // 在主线程中接收子线程发送的消息并输出 println!(receive , rx.recv().unwrap()); 接受数据的时候，recv() 是阻塞的；try_recv() 是不阻塞的，当通道中没有消息时，它会立刻返回一个错误 Result (Ok 或者 Err())。 多发送者 mpsc 支持多发送者，我们需要将 sender 进行 clone() 后，给每一个线程一个拷贝即可。 use std::sync::mpsc;use std::thread;fn main() let (tx, rx) = mpsc::channel(); let tx1 = tx.clone(); thread::spawn(move || tx.send(String::from(hi from raw tx)).unwrap(); ); thread::spawn(move || tx1.send(String::from(hi from cloned tx)).unwrap(); ); for received in rx println!(Got: , received); 需要注意的是： 需要所有的发送者都被 drop 掉后，接收者 rx 才会收到错误，进而跳出 for 循环，最终结束主线程 通道与所有权的转移 如果值的类型实现了 Copy，则直接复制该值，传输到 channel 里. 如果没有实现 Copy，则其所有权会转移到 channel，随后给 receiver. 同步通道、异步通道 异步通道：无论接收者是否正在接收消息，消息发送者在发送消息时都不会阻塞 同步通道：发送消息是阻塞的，只有在消息被接收后才解除阻塞 mpsc::sync_channel(N) 这里可以通过设置 N，使得发送者可以无阻塞地发送 N 条消息。当消息缓冲队列满了后，新发送的消息将被阻塞。 use std::sync::mpsc;use std::thread;use std::time::Duration;fn main() let (tx, rx)= mpsc::sync_channel(0); // 设置 0 条消息可以无阻塞发送 let handle = thread::spawn(move || println!(发送之前); tx.send(1).unwrap(); println!(发送之后); ); println!(睡眠之前); thread::sleep(Duration::from_secs(3)); println!(睡眠之后); println!(receive , rx.recv().unwrap()); handle.join().unwrap(); 关闭通道 所有发送者被 drop 或者所有接收者被 drop 后，通道会自动关闭。"},{"title":"Rust 多线程与并发","path":"/topic/rust-grammar/rust-concurrency/","content":"thread::spawn 可以使用 thread::spawn 创建线程： 线程内部的代码用闭包来执行 main 进程一旦结束，所有子线程也会立刻结束。所以需要先确保子线程都结束，再结束程序。 use std::thread;use std::time::Duration;fn main() thread::spawn(|| for i in 1..10 println!(hi number from the spawned thread!, i); thread::sleep(Duration::from_millis(1)); ); for i in 1..5 println!(hi number from the main thread!, i); thread::sleep(Duration::from_millis(1)); .join(): 等待线程结束 和其他语言里的 join() 作用一样： use std::thread;use std::time::Duration;fn main() let handle = thread::spawn(|| for i in 1..5 println!(hi number from the spawned thread!, i); thread::sleep(Duration::from_millis(1)); ); handle.join().unwrap(); for i in 1..5 println!(hi number from the main thread!, i); thread::sleep(Duration::from_millis(1)); 线程屏障 Barrier 是用于同步的机制. 类似于初始值非零的 semaphore. use std::sync::Arc, Barrier;use std::thread;fn main() let mut handles = Vec::with_capacity(6); let barrier = Arc::new(Barrier::new(6)); for _ in 0..6 let b = barrier.clone(); handles.push(thread::spawn(move|| println!(before wait); b.wait(); println!(after wait); )); for handle in handles handle.join().unwrap(); 线程局部变量 使用 thread_local! 宏初始化线程内部的局部变量，然后在线程内部使用该变量的 with 方法获取变量值。每个新的线程访问它时，都会使用它的初始值作为开始，各个线程中的值彼此互不干扰。 use std::cell::RefCell;use std::thread;thread_local!(static FOO: RefCellu32 = RefCell::new(1));FOO.with(|f| assert_eq!(*f.borrow(), 1); *f.borrow_mut() = 2;);// 每个线程开始时都会拿到线程局部变量的FOO的初始值let t = thread::spawn(move|| FOO.with(|f| assert_eq!(*f.borrow(), 1); *f.borrow_mut() = 3; ););// 等待线程完成t.join().unwrap();// 尽管子线程中修改为了3，我们在这里依然拥有main线程中的局部值：2FOO.with(|f| assert_eq!(*f.borrow(), 2);); 这里，线程对 FOO 的使用方式是借用。我们无法在每个线程里获取 FOO 的独立拷贝最后汇总。 只调用一次的函数（初始化全局变量） use std::thread;use std::sync::Once;static mut VAL: usize = 0;static INIT: Once = Once::new();fn main() let handle1 = thread::spawn(move || INIT.call_once(|| unsafe VAL = 1; ); ); let handle2 = thread::spawn(move || INIT.call_once(|| unsafe VAL = 2; ); ); handle1.join().unwrap(); handle2.join().unwrap(); println!(, unsafe VAL );"},{"title":"Rust Cell&lt;T&gt;: 在不可变引用的同时修改目标数据","path":"/topic/rust-grammar/rust-internal-changeable-cell-refcell/","content":"CellT Cell 和 RefCell 在功能上没有区别，区别在于 CellT 适用于 T 实现 Copy 的情况： use std::cell::Cell;fn main() let c = Cell::new(asdf); let one = c.get(); c.set(qwer); let two = c.get(); println!(,, one, two); 输出： asdf,qwer 因为实现了 Copy，所以 asdf 这个值被复制了一份，给了 one 变量. Cell 没有可变引用 use std::cell::Cell;fn main() // code snipet 1 let x = Cell::new(1); let y = x; let z = x; x.set(2); y.set(3); z.set(4); println!(, x.get()); 这里，y, z 都是 x 的不可变引用，调用 y.set() 的时候，会自动 dereference 为 (x).set() === x.set() 因此，y, z 调用 set() 都会修改 x 这个指针指向的数据，因此输出为 444 Cell::from_mut(), Cell::as_slice_of_cells() 使用技巧 Cell::from_mut()，该方法将 mut T 转为 CellT Cell::as_slice_of_cells()，该方法将 Cell[T] 转为 [CellT] // Wrongfn is_even(i: i32) - bool i % 2 == 0fn retain_even(nums: mut Veci32) let mut i = 0; for num in nums.iter().filter(|num| is_even(*num)) nums[i] = *num; i += 1; nums.truncate(i); // Trueuse std::cell::Cell;fn retain_even(nums: mut Veci32) let slice: [Celli32] = Cell::from_mut(mut nums[..]) .as_slice_of_cells(); let mut i = 0; for num in slice.iter().filter(|num| is_even(num.get())) slice[i].set(num.get()); i += 1; nums.truncate(i); 这个例子里，我们希望通过 iter().filter() 遍历元素，此时迭代器是对元素的不可变借用，因此无法通过不可变借用来修改元素值（因为会违反 Rust 的借用规则）。而 CellT 允许使用 .get() 和 .set() 方法，在不可变借用的上下文里修改值，从而绕过 Rust 的限制。所以我们需要的是 an immutable reference of a list of CellTs. RefCellT RefCellT 解决的问题更进一步：解决可变引用、不可变引用共存的问题。本质只是将编译阶段的共存检查，推迟到运行时阶段，即如果运行时阶段里还是出现了 mutable borrow 和 immutable borrow 同时存在的情况下，依然会报运行时错误 Panic. Rust 规则 智能指针带来的额外规则 一个数据只有一个所有者 RcT/ArcT 让一个数据可以拥有多个所有者 要么多个不可变借用，要么一个可变借用 RefCellT 实现编译期可变、不可变引用共存 违背规则导致编译错误 违背规则导致运行时 panic 什么是内部可变性？ 在某些场景中，一个值可以在其方法内部被修改，同时对于其它代码不可变。例如说考虑下面这个例子： // 定义在外部库中的特征pub trait Messenger fn send(self, msg: String);// --------------------------// 我们的代码中的数据结构和实现struct MsgQueue msg_cache: VecString,impl Messenger for MsgQueue fn send(self, msg: String) self.msg_cache.push(msg) 我们有个变量 x: MsgQueue，我们调用 x.send() 希望修改数据结构里的一部分，但是不希望 x 变量本身被修改。然而，send() 方法的签名是 self，而且 .push() 修改了 x 的数据，因此 self 会报错。 这个时候 RefCellT 就派上用场了。我们把 msg_cache 用 RefCellVecString 包裹，就可以实现保持 x 不可变的情况下修改内部的数据。 // 定义在外部库中的特征pub trait Messenger fn send(self, msg: String);// --------------------------// 我们的代码中的数据结构和实现struct MsgQueue msg_cache: VecString,impl Messenger for MsgQueue fn send(self, msg: String) self.msg_cache.push(msg) use std::cell::RefCell;pub trait Messenger fn send(self, msg: String);pub struct MsgQueue msg_cache: RefCellVecString,impl Messenger for MsgQueue fn send(self, msg: String) self.msg_cache.borrow_mut().push(msg) fn main() let mq = MsgQueue msg_cache: RefCell::new(Vec::new()), ; mq.send(hello, world.to_string());注意这里的 borrow_mut() Rc + RefCell 一个常见的用法是 RcRefCellT，这样，多个 x = RcRefCellT 可以共享底层的数据。 use std::cell::RefCell;use std::rc::Rc;fn main() let s = Rc::new(RefCell::new(我很善变，还拥有多个主人.to_string())); let s1 = s.clone(); let s2 = s.clone(); // let mut s2 = s.borrow_mut(); s2.borrow_mut().push_str(, oh yeah!); println!(:? :? :?, s, s1, s2); 输出： RefCell value: 我很善变，还拥有多个主人, oh yeah! RefCell value: 我很善变，还拥有多个主人, oh yeah! RefCell value: 我很善变，还拥有多个主人, oh yeah!"},{"title":"Rust Arc&lt;T&gt;：在多线程里共享对象","path":"/topic/rust-grammar/rust-multithread-arc/","content":"ArcT: Atomic RcT ArcT 和 RcT 具有完全相同的 API，和 RcT 的区别就在于原子化或者其它锁虽然可以带来的线程安全，但是都会伴随着性能损耗，而且这种性能损耗还不小。 use std::sync::Arc;use std::thread;fn main() let s = Arc::new(String::from(多线程漫游者)); for _ in 0..10 let s = Arc::clone(s); let handle = thread::spawn(move || println!(, s) );"},{"title":"Rust 中的计数指针","path":"/topic/rust-grammar/rust-shared-pointer-rc/","content":"RcT 希望在堆上分配一个对象供程序的多个部分使用且无法确定哪个部分最后一个结束时，就可以使用 Rc 成为数据值的所有者。使用 Rc::clone(ptr) 进行指针复制。 use std::rc::Rc;fn main() let a = Rc::new(String::from(hello, world)); let b = Rc::clone(a); assert_eq!(2, Rc::strong_count(a)); assert_eq!(Rc::strong_count(a), Rc::strong_count(b)) 这里的 Rc::clone()，仅仅复制了智能指针并增加了引用计数，并没有克隆底层数据，因此 a 和 b 是共享了底层的字符串 s. RcT 与 mutable reference 事实上，RcT 是指向底层数据的不可变的引用，因此你无法通过它来修改数据，这也符合 Rust 的借用规则：要么存在多个不可变借用，要么只能存在一个可变借用。 那么如何需要修改数据该怎么办呢？这就需要 RefCellT 或者互斥锁 MutexT 了。而在多线程里需要共享对象的话，需要 ArcT"},{"title":"智能指针：Deref, Drop","path":"/topic/rust-grammar/smart-pointer-deref-drop/","content":"Deref Deref 可以访问被分配的资源。 dereference 类似于 C++ 中的指针 * 操作符，并且只能在 ... 上或者实现了 Deref trait 的类型上使用。仅引用类型的实参才会触发自动解引用 隐式 Deref 转换 Rust 编译器在碰到实参与形参的类型对不上的时候，会考虑将实参自动 Deref 以匹配函数的形参类型。考虑这样的代码： fn main() let s = Box::new(String::from(hello world)); display(s)fn display(s: str) println!(, s); s 的类型是 BoxString，而函数的形参类型是 str，出现对不上号的情况。于是 Rust 编译器做了如下的自动 dereference: BoxString == String == str 连续的 Deref 转换（引用归一化） 先前提到，Rust 只能对 进行 dereference，当遇到多个 或者智能指针的时候，Rust 是怎么处理的呢？简而言之，可以概括为以下几点： 智能指针 BoxT 展开成 T（包括其他类型的智能指针，如 RcT, ArcT） 把多重 归一为单个 第二点在标准库里是这样实现的： impl T: ?Sized Deref for T type Target = T; fn deref(self) - T *self Deref 与 DerefMut dereference 还支持将 mutable reference 转换成 immutable reference，或者 immutable reference 转换成 mutable reference. 当 T: DerefTarget=U 时，T 可以转换成 U，immutable ref →\\to→ immutable ref 当 T: DerefTarget=U 时，mut T 可以转换成 U 当 T: DerefMutTarget=U 时，mut T 可以转换成 mut U 并且，从标准库实现上说，DerefMut 是继承了 Deref. Drop Drop 特征可以释放资源。在变量超出作用域的时候，执行一段特定的代码，最终编译器将帮助自动插入这段收尾代码。 当然也可以手动调用 xxx.drop() 进行手动回收。 Drop 的顺序 我们考察下面这段代码： struct HasDrop1;struct HasDrop2;impl Drop for HasDrop1 fn drop(mut self) println!(Dropping HasDrop1!); impl Drop for HasDrop2 fn drop(mut self) println!(Dropping HasDrop2!); struct HasTwoDrops one: HasDrop1, two: HasDrop2,impl Drop for HasTwoDrops fn drop(mut self) println!(Dropping HasTwoDrops!); struct Foo;impl Drop for Foo fn drop(mut self) println!(Dropping Foo!) fn main() let _x = HasTwoDrops two: HasDrop2, one: HasDrop1, ; let _foo = Foo; println!(Running!); 输出为 Running!Dropping Foo!Dropping HasTwoDrops!Dropping HasDrop1!Dropping HasDrop2! 由此可以得出 drop 的顺序： 变量级别，按照逆序的方式，_x 在 _foo 之前创建，因此 _x 在 _foo 之后被 drop 结构体内部，按照顺序的方式，结构体 _x 中的字段按照定义中的顺序依次 drop Copy 与 Drop 互斥 我们无法为一个类型同时实现 Copy 和 Drop 特征。因为实现了 Copy 特征的类型会被编译器隐式的复制，因此非常难以预测析构函数执行的时间和频率。因此这些实现了 Copy 的类型无法拥有析构函数。"},{"title":"Rust 中的智能指针","path":"/topic/rust-grammar/rust-smart-pointers/","content":"BoxT 基础智能指针 特意的将数据分配在堆上 数据较大时，虽然可以放在栈上，但是不想在转移所有权时进行数据拷贝（栈上的数据只会拷贝） 当栈上数据转移所有权时，实际上是把数据拷贝了一份，最终新旧变量各自拥有不同的数据，因此所有权并未转移。 而堆上则不然，底层数据并不会被拷贝，转移所有权仅仅是复制一份栈中的指针，再将新的指针赋予新的变量，然后让拥有旧指针的变量失效，最终完成了所有权的转移 类型的大小在编译期无法确定，但是我们又需要固定大小的类型时 特征对象，用于说明对象实现了一个特征，而不是某个特定的类型 Box::leak 可以消费掉 Box 并且强制目标值从内存中泄漏，使用场景：需要一个在运行期初始化的值，但是可以全局有效，也就是和整个程序活得一样久。Box::leak 可以完成动态初始化。"},{"title":"迭代器：消费、适配","path":"/topic/rust-grammar/iterator-consumer-adapter/","content":"Consume Iterator 只要迭代器上的某个方法 A 在其内部调用了 next 方法，那么 A 就被称为消费性适配器：因为 next 方法会消耗掉迭代器上的元素，所以方法 A 的调用也会消耗掉迭代器上的元素。 注意，这里会拿走的是迭代器的所有权，而非原来变量的所有权。例如，sum() 方法 fn main() let v1 = vec![1, 2, 3]; let v1_iter = v1.iter(); let total: i32 = v1_iter.sum(); assert_eq!(total, 6); // v1_iter 是借用了 v1，因此 v1 可以照常使用 println!(:?,v1); // 以下代码会报错，因为 `sum` 拿到了迭代器 `v1_iter` 的所有权 // println!(:?,v1_iter); Adapt Iterator 会返回一个新的迭代器。迭代器仍然是惰性的，不操作就不会进行求值。这也意味着我们需要一个 consumer 来消费这个迭代器，如 .collect() 方法。"},{"title":"Rust 迭代器","path":"/topic/rust-grammar/rust-iterator/","content":"转化为迭代器 .into_iter() 拿走所有权，并转化为迭代器 .iter() 对元素进行不可变借用的迭代器 .iter_mut() 对元素进行可变借用的迭代器，可以修改元素 Iterator Trait, IntoIterator Trait 两者稍有区别。前者定义了 next 方法使得可以访问元素，后者则定义了 into_iter(), iter(), iter_mut() 等迭代器转化方法 pub trait Iterator type Item; fn next(mut self) - OptionSelf::Item; // 省略其余有默认实现的方法implI: Iterator IntoIterator for I type Item = I::Item; type IntoIter = I; #[inline] fn into_iter(self) - I self"},{"title":"把 closure 作为函数的返回值","path":"/topic/rust-grammar/closure-as-return-value/","content":"闭包作为函数返回值 回想到我们如何将特征对象作为函数的返回类型，这里的 FnOnce, FnMut, Fn 也都是特征，那么我们可以用相同的方法进行处理： fn factory(x:i32) - Boxdyn Fn(i32) - i32 let num = 5; if x 1 Box::new(move |x| x + num) else Box::new(move |x| x - num)"},{"title":"Rust 与函数式编程：闭包","path":"/topic/rust-grammar/rust-closure/","content":"what is closure? Rust 里的 closure 是一种匿名函数，可以保存在变量里用于日后的调用，也可以作为参数传递给函数。而且相比于函数，closure 可以在其定义域内捕获变量。 closure 的类型推导 closure 不是泛型，因此当编译器推导出一种类型后，它就会一直使用该类型。 let example_closure = |x| x;let s = example_closure(String::from(hello));// example_closure 的类型为 Fn(String) - Stringlet n = example_closure(5); // 但这里希望以 Fn(i32) - i32 调用// 报错！ Rust 闭包可以用泛型吗？ struct CacherT, Ewhere T: Fn(E) - E, E: Clone, query: T, value: OptionE,implT, E CacherT, Ewhere T: Fn(E) - E, E: Clone, fn new(query: T) - CacherT, E Cacher query, value: None fn value(mut self, arg: E) - E match self.value Some(ref v) = v.clone(), None = let v = (self.query)(arg.clone()); self.value = Some(v.clone()); v.clone() fn main() let mut test = Cacher::new(|d: String| d + world); println!(first cache: , test.value(wtf.to_string())); println!(second cache: , test.value(hello.to_string())); closure 与内存 当闭包从环境中捕获一个值时，会分配内存去存储这些值。对于有些场景来说，这种额外的内存分配会成为一种负担。与之相比，函数就不会去捕获这些环境值，因此定义和使用函数不会拥有这种内存负担。 三种闭包特征 (Trait) 闭包捕获变量有三种途径，恰好对应函数参数的三种传入方式：转移所有权、可变借用、不可变借用，因此相应的 Fn 特征也有三种： 这三种的关系并不是说，我定义了闭包类型满足 FnOnce 那么这个闭包就只能调用一次，而是会根据捕获和使用方式，自动推导闭包属于哪一种 Trait FnOnceFnMutFn 可以移动变量所有权 只能调用一次（除非也实现 Copy trait） 来看这么一个例子 fn fn_onceF(func: F)where F: FnOnce(usize) - bool, println!(, func(3)); println!(, func(4)); // 报错fn main() let x = vec![1, 2, 3]; fn_once(|z| z == x.len()) 这里一个问题是，FnOnce 特征的 func 变量为什么只能被调用一次？怎么从所有权的角度进行解释？ 从捕获的变量的角度来说，闭包捕获了变量的所有权，根据 Rust 的语言设计，所有权只能在一个人手里，于是第二次再调用闭包就会无法拿到所有权。 那么闭包自身的所有权转移给谁了呢？闭包 func 的所有权会转移到调用闭包的代码上下文。此时，闭包可能释放其捕获的资源（如 x），或者将这些资源的所有权转移给其他逻辑（即使没有显式转移，闭包本身的调用也意味着其自身被“销毁”） 这里 func(4) 的报错其实与 fn_once(|z| z == x.len()) 这个匿名函数本身没关系，只是泛型函数自己做的类型检查。 使用变量的可变借用 (mut) 闭包变量本身也需要定义为 let mut，或者作为函数参数时以 mut 的方式借用 使用变量的不可变借用 move 与 Fn Trait 实际上使用了 move 的闭包依然可以使用 Fn 或 FnMut 特征 一个闭包实现了哪种 Fn 特征取决于该闭包如何使用被捕获的变量，而不是取决于闭包如何捕获它们 这个点怎么理解呢？考虑下面的代码 let f = move || println!(, s.len()); 这里 f 闭包同时实现了 FnOnce, FnMut, Fn，尽管 move 把所有权都转移走了（“闭包如何捕获他们”）但是 s.len() 仅仅只使用了不可变借用（“该闭包如何使用被捕获的变量”），因此还是 Fn，也因此可以作为 FnOnce, FnMut 泛型的参数。 更具体的，一个闭包并不仅仅实现某一种 Fn 特征，规则如下： 所有的闭包都自动实现了 FnOnce 特征，因此任何一个闭包都至少可以被调用一次 没有移出所捕获变量的所有权的闭包自动实现了 FnMut 特征 不需要对捕获变量进行改变的闭包自动实现了 Fn 特征 我们可以看成是一个继承的关系： FnOnce⟶FnMut⟶Fn \\boxed{\\texttt{FnOnce}}\\longrightarrow\\boxed{\\texttt{FnMut}}\\longrightarrow\\boxed{\\texttt{Fn}} FnOnce​⟶FnMut​⟶Fn​通过源码可以看得更清晰： pub trait FnOnceArgs: Tuple /// The returned type after the call operator is used. #[lang = fn_once_output] #[stable(feature = fn_once_output, since = 1.12.0)] type Output; /// Performs the call operation. #[unstable(feature = fn_traits, issue = 29625)] extern rust-call fn call_once(self, args: Args) - Self::Output;pub trait FnMutArgs: Tuple: FnOnceArgs /// Performs the call operation. #[unstable(feature = fn_traits, issue = 29625)] extern rust-call fn call_mut(mut self, args: Args) - Self::Output;pub trait FnArgs: Tuple: FnMutArgs /// Performs the call operation. #[unstable(feature = fn_traits, issue = 29625)] extern rust-call fn call(self, args: Args) - Self::Output;"},{"title":"具身智能（一）：数据集","path":"/topic/researchthoughts/rl-agent-dataset-01/","content":"6f592527dfcf14273982642650ba23d6edaaa319c53634fffd49af368ca2244ff121d6d4d6d9b5a0f205c8cfa1e33a595c27b300ef383bbfbb5f35d2cc1d01b520ae805a34d3e829467a1ed1ae7f0dcaff5733b6ca2fcb015558be80773daa867f670a8998b36f6496f92daea65a57d6e3845dc63c1295a691e3e903259826288db38a4f6e20dfd5b0d6293d55f80745809c83e9d6cafb311393ec8c86ced8260f62dc0872dddc041e62f7fc6591ef02f6e2fa6ffb194a70f8561863f1c55848b07469ebe114cf3445440105b9997cf3540569da3b5674038cf74efa9b94357d2f5fc9e1208f599289991d3aece7cb9278ab9cb1796f4f659eac4a9e405670d92fe9770822db56d036936c958c1c80db06349236ca3cc3a8e72f0c64ff35fa762e1f90fc935099da4d17e272b12fdc3276a0b506ab13a01ee799c4f48d2d09b977a2877514bc00b1eadbe45a91c7afc1d0fd9c5db430d2c7f2859bbf2d9da6924bf7e7e00afdf436f80191c97c82b5108b5c3cf9880a4a524d33f2235da6205b31a6bd230e7a52fff92e63fbbecf7eec2c853af0e616ef111cf8839658443aefc1c38c8aeb7343a20529a187b1c138861518ea6e7376bb9c18cdd68afb1b2c2e8af102673b0ea6678239f95e952be7f5c5635abf3e4e680315e329e3fe9d5d9ef30c6c00e0acc3fef090e453a33dcf8083217f530bf0092f1027249234fa28738ad83e911bd3cff56a4338b7d1191f7b11a50e4f1b4c935bbb13af17d3cc8d6e69082724ace558126efedc4cae8217c23c057b5f99c03da1b92980ccf8e081a511f89f377433ad47424d3970a5fc825c44937a24c3c13257401d073723d524be4b067e233037c7a929665586cbdd983f55a060f84ad539599e51ab0bc7028e533288030a37b66240a95e40095257bb3612654a8a2aad638602612b1a52ce1e9bc266669c8e17b12fa6cd34ec8e505c9d18cd7bc21b7fda77cbe8eba408de887d1cc09f5891c51271e250f33f8b380024a4f6fdb32279a9cb8913214a7b3af403d07d3f243bfaaef5741bfd370ba3f8bdd3d5bf269a8c986d00db6b470ecf96c43c5677bfed2b90414b5276de67fe2360121b904276f1c3ab2f3cf5a664b28e1437575642ead7a6badae64bbb92671e38e83a7f8ea3f61614a84b769c4e71e064ed69066b84819b0506f91d11a1b70b28 Password is needed."},{"title":"Sublime Text 4 Crack","path":"/sublime-text-4200-crack/","content":"Linux 下破解 一行命令即可： sudo perl -pi -e s/\\x0F\\xB6\\x51\\x05\\x83\\xF2\\x01/\\xC6\\x41\\x05\\x01\\xB2\\x00\\x90/ /opt/sublime_text/sublime_text"},{"title":"MoE (Mixture of Experts) 技术","path":"/about-moe/","content":"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 Password is needed."},{"title":"Model Context Protocol","path":"/topic/mcp/mcp-intro/","content":"MCP Introduction MCP 本质上像是一个转接口，一侧是由厂商提供的 Pretrained LLM，另一侧是自己实现的工具、文本等等，这两者通过实现 MCP 的接口实现无缝切换，这样个人开发者就不需要为每一个模型都适配接口了。 然而听说 CLine 的 MCP 的实现方式只是在 Prompt 里添加包含工具的信息…… 感觉 Token 用量会直接爆炸啊 以下使用 Python 的 Official MCP SDK. 日后应该会加上 LangChain 的 MCP Adapter MCP Server MCP Server 就是提供工具服务的一方。简单来说，创建一个 MCP Tool 只需要正常地写完功能函数后，用 @mcp.tool(name=, description=, annotation=) Decorator 包装一下即可。 Python SDKTypeScript SDKfrom mcp.server.fastmcp import FastMCPservice = FastMCP(ServiceProviderName)# 使用 decorator 注册 MCP 工具@service.tool(name=Tool Name, description=Description of Tool, annotation=Input/Output Claim) def some_function(): # 或者 async def 也可以 确保返回的字符串是结构化的，以便让 LLM 读懂. return Some retured (structured) data. 还不会 TS（ 当然最重要的，MCP 作为服务的提供方需要向外暴露自己的功能： service.run(transport=stdio) MCP Client","categories":["MCP","Agent"]},{"title":"RoPE","path":"/topic/papers/RoPE/","content":"旋转位置编码 旋转位置编码的核心诉求就是需要让 embedding 同时体现出相似度和相对位置这两个信息 ⟨RoPE(Q,i),RoPE(K,j)⟩=PosSim(QKT,j−i) \\lang\\text{RoPE}(Q,i), \\text{RoPE}(K,j)\\rang=\\text{PosSim}(QK^T,j-i) ⟨RoPE(Q,i),RoPE(K,j)⟩=PosSim(QKT,j−i) 我们先考虑如果 Q,KQ,KQ,K 都只是二维向量，即 Q=[ab],K=[cd] Q=\\begin{bmatrix}a\\\\ b\\end{bmatrix}, K=\\begin{bmatrix}c\\\\ d\\end{bmatrix} Q=[ab​],K=[cd​] 那么通常的 Scaled Dot Product Attention 的计算结果应为 QKT=ac+bdQK^T=ac+bdQKT=ac+bd，考虑写成复数的话，令 cq=a+bi,ck=c+dic_q=a+bi,c_k=c+dicq​=a+bi,ck​=c+di，且 ckc_kck​ 的共轭为 ck∗c_k^\\astck∗​，则有 ⟨q,k⟩=ℜ[cqck∗]\\lang q,k \\rang=\\Re[c_qc_k^\\ast]⟨q,k⟩=ℜ[cq​ck∗​]，其中 ℜ[c]\\Re[c]ℜ[c] (Latex Code: \\real) 表示 ccc 的实数部分，ℑ[c]\\Im[c]ℑ[c] 表示 ccc 的虚数部分 (Latex Code: \\image)。 我们假设 RoPE(x,i)\\text{RoPE}(x,i)RoPE(x,i) 可以表示为复数形式，即 RoPE(x,i)=ax+bxi=Rxeiθ(x,i)\\text{RoPE}(x,i)=a_x+b_xi=R_x e^{i\\theta(x,i)}RoPE(x,i)=ax​+bx​i=Rx​eiθ(x,i)，这里第二个等号是复数的指数表示法。 RoPE 虽然论文里是要求对 xm2ix_m^{2i}xm2i​ 与 xm2i+1x_m^{2i+1}xm2i+1​ 加上 cos⁡,sin⁡\\cos,\\sincos,sin 的 embedding，不过从数学的角度来说，这里的配对实际上不影响最终的结果，“不同的配对方式本质上不影响模型的表达能力，并且可以相互转化”。因此在工程上，考虑到内存连续性，我们通常不用这种相邻配对，而是 xix^{i}xi 与 xi+d/2x^{i+d/2}xi+d/2 进行配对计算。","categories":["位置编码"]},{"title":"学习 PyQT6 (1)——事件循环 (Event Loop)","path":"/topic/coding/qt/pyqt-001/","content":"Event Loop QT 的核心是 QApplication 类（每个应用程序只需要一个 QApplication 类），它维护了 Event Loop，负责管理用户和应用程序的交互：每一次互动（摁键盘、点鼠标）都会生成一个 event 放在 event queue 里. 然后每一次 iteration 中，我们检查一次 queue，查找是不是有正在 waiting 的 event，然后 QT 把 event 和 control 交给对应的 event handler 进行处理。处理完毕后，handler 再把 control 交还给 QApplication. Event Loop Extending Qt Classes 需要总是先 super().__init__()"},{"title":"Swin Transformer","path":"/topic/papers/swin-transformer/","content":"Swin Transformer 视觉 Transformer 面临两大核心挑战： 视觉实体尺度变化大 (Scale Variation)； 图像像素分辨率远高于文本单词 (High Resolution)。 而现有的解决方法 ViT 存在一下几个问题： 单一尺度特征图 全局自注意力计算复杂度随图像尺寸呈平方增长 (Quadratic Complexity) 难以直接应用于密集预测任务 (Dense Prediction)","categories":["Transformer 架构"]},{"title":"Agentic 模式","path":"/agentic-patterns/","content":"Prompt Chaining 链式调用 Chaining Prompt Chaining 的实现思路比较简单：上一个 LLM 的输出作为下一个 LLM 的输入。这样的 agentic 范式适合那些可以被分解成 sequential assembly line 的任务。 然而这样的方法也有局限性：这样的任务也依赖于人工对任务进行拆解。 Routing 路由转发 Routing 顾名思义，Routing Flow 下，一个小型的 LLM 负责解析用户的指令，判断指令所属的任务，然后转发给对应的 LLM 进行处理。 这样做的一个好处就是可以控制 API 调用成本。例如用小模型进行解析工作，如果指令比较简单，则转发给小模型进行指令处理；否则把复杂指令交给 API 调用费更贵的大模型进行进一步的处理。 Parallelization 并行处理 Parallel A task is broken down into independent subtasks that are processed simultaneously by multiple LLMs, with their outputs being aggregated. RAG with Query Decomposition 分析复杂文档 Reflection 生成并反思 Reflection Reflection 是让 LLM 重新评判自己生成的内容是否满足要求，如果不满足，则继续生成（修正）。这种范式已经出现在诸如 VSCode Copilot 等产品上 Tool Use 工具使用 在我看来，Tool Use 只是 Reflection 的一种形式：评判的过程换成了外部工具的调用。 Tool Use Planning Planning 中央 LLM 将任务拆解为多个子任务，然后转发给专门的 LLM 进行处理并汇总。和 Routing 很像，但区别在于 Routing 直接转发一整个任务，而 Planning 先拆分大任务，再将小人物转发处理。 Multi-Agent Manager 形式 Multi-Agent (Manager) Manager 形式实现的 MultiAgent 分多个角色：manager 和 worker. 每个 worker 具有特定领域的专业知识和特定工具的访问权限，manager 则可以分析任务进展，指定 worker 进行任务协作。 Swarm 形式 Multi-Agent (Swarm) Swarm 形式实现的 Multi Agent 相比于 Manager 形式来说，去掉了负责协调的 central manager，直接让 worker 之间自行合作。"},{"title":"LightRAG 论文","path":"/lightrag-paper/","content":"RAG 任务概述 我们可以把 RAG 任务用一个统一的框架来描述： M=(G,R=(I,S))M(q;D)=G(q,S(q;D^))D^=I(D) \\begin{aligned} M=(G, R=(I, S))\\\\ M(q;D)=G(q, S(q; \\hat D))\\\\ \\hat D=I(D) \\end{aligned} MM(q;D)D^​=(G,R=(I,S))=G(q,S(q;D^))=I(D)​这里的字母变量都是什么意思呢？有必要搞这么复杂的数学描述吗……原论文甚至还用上了希腊字母和花体字…… MMM 是整个 RAG 框架。 M(q;D)M(q;D)M(q;D) 代表一次查询：给定问题 qqq 和知识库 DDD 让模型给出回答。 这里的 GGG 是 Generation Module，即模型生成。 R=(I,S)R=(I,S)R=(I,S) 是 RAG 里的 Retrieval-Augment 部分 其中 III 用于处理知识库 DDD，将原始文档 DDD 转化为特定的数据结构 D^\\hat DD^，以期更好质量的 retrieval SSS 是 Search，其实更像 Augment 的过程，从处理好的数据库 D^\\hat DD^ 里获取相关的信息，传递给 Generation 模块 GGG 让模型进行输出。"},{"title":"自然语言处理（NLP）的发展与关键技术：从词义到大模型","path":"/nlp-development/","content":"词义表示的进化：从规则到分布式语义 WordNet 近义词表示 早期的NLP系统依赖人工构建的规则和词典，如 WordNet。这类工具通过同义词集合和上下位关系 (Hyponymy) 描述词义，例如将“proficient”标记为“good”的同义词。然而，这种方法存在明显局限： 静态性：无法捕捉新兴含义（如“wicked”在俚语中的“酷”意）；一些单词仅在特定情况下意义相近 主观性：词语意义需要人工界定 实时性：依赖人工标注，更新缓慢； 缺乏相似性计算 ：无法量化“hotel”和“motel”的语义相似度。 Localist: One-Hot 编码与词袋模型 2013 年前，NLP 普遍采用 One-Hot 编码 表示词汇，即用高维稀疏向量（如长度为 101010 万的向量中仅一个位置为 111）表示单词。这种“局部表示”导致： 维度灾难 ：101010 万词汇需 101010 万维空间； 零相似性 ：正交向量无法反映词义关联（如“hotel”与“motel”）。 Distributed Semantics 与 Word2Vec Word2Vec 是一种通过无监督学习生成词向量的工具，其核心思想是基于词的上下文共现规律，将词映射到低维稠密向量空间中，使得语义相似的词在向量空间中距离较近。主要有两种模型：一种是根据上下文推理中心词的 CBOW 模型，另一种是根据中心词推理上下文的 Skip-Gram 模型。 这里的 Word Vector 也被称为 Word Embedding 或者 Word Representations Skip Gram 给定上下文长度为 mmm，对于样本数据中心词为 wtw_twt​，其上下文为 wt−m,…,wt+mw_{t-m},\\dots,w_{t+m}wt−m​,…,wt+m​，Skip Gram 训练的目标就是最大化上下文词的条件概率 ∏j=−mmP(wt+j∣wt) \\prod_{j=-m}^m P(w_{t+j}|w_t) j=−m∏m​P(wt+j​∣wt​)对于全部语料库，那么就要最大化每一个这样的中心词和上下文的条件概率，把他们乘起来（假设 Skip Gram 的参数为 θ\\thetaθ） 这里的 θ\\thetaθ 其实是一个矩阵。假设我们把词汇编码为 ddd 维稠密向量，词汇表大小为 VVV，则 θ\\thetaθ 的大小应该为 R2dV\\mathbb R^{2dV}R2dV，我们这里直接把 θ\\thetaθ 当作词汇表来看。 其本质是一个将词汇映射到 embedding 空间的函数 f:{0,1}V↦Rdf:\\mathbb \\{0,1\\}^V\\mapsto R^{d}f:{0,1}V↦Rd。要取出 viv_ivi​ 向量，可以通过矩阵乘法，设置每一行为 111、其余全部置 000 来取出这一行的词向量，相当于 θ\\thetaθ 乘上了一个常数。 即 vw=θTCv(w)v_w=\\theta^TC_v(w)vw​=θTCv​(w)，uw=θTCu(w)u_w=\\theta^TC_u(w)uw​=θTCu​(w)，这里的 Cv,CuC_v,C_uCv​,Cu​ 可以是 www 的 One-Hot Vector. θ=arg max⁡θ∏t=1T∏−m≤j≤mj≠0Pθ(wt+j∣wt) \\theta=\\argmax_\\theta \\prod_{t=1}^T \\prod_{-m \\le j\\le m}^{j e 0} P_\\theta(w_{t+j}|w_t) θ=θargmax​t=1∏T​−m≤j≤m∏j=0​Pθ​(wt+j​∣wt​)这也等同于最小化其负对数 θ=arg min⁡θJ(θ)=arg min⁡θ−1T∑t=1T∑−m≤j≤mj≠0log⁡Pθ(wt+j∣wt) \\theta=\\argmin_\\theta J(\\theta)=\\argmin_\\theta -\\frac{1}{T}\\sum_{t=1}^T \\sum_{-m \\le j\\le m}^{j e 0} \\log P_\\theta(w_{t+j}|w_t) θ=θargmin​J(θ)=θargmin​−T1​t=1∑T​−m≤j≤m∑j=0​logPθ​(wt+j​∣wt​)这里概率的计算就是老套路 softmax 了，我们把整个词汇库的 context vector 和当前词的中心词向量点乘起来，做 softmax（这里用 ooo 表示上下文词，uwu_wuw​ 表示单词 www 作为上下文时的向量，vwv_wvw​ 表示 www 作为中心词时的向量） P(o∣c)=exp⁡(uoTvc)∑w∈Vexp⁡(uwTvc) P(o|c)=\\frac{\\exp(u_{o}^T v_c)}{\\sum_{w\\in \\mathbb V} \\exp(u_w^T v_c)} P(o∣c)=∑w∈V​exp(uwT​vc​)exp(uoT​vc​)​于是损失函数 J(θ)J(\\theta)J(θ) 变成了 J(θ)=−1T∑t=1T∑−m≤j≤mj≠0(uoTvc−log⁡∑w∈Vexp⁡(uwTvc)) J(\\theta)=-\\frac{1}{T}\\sum_{t=1}^{T} \\sum_{-m \\le j\\le m}^{j e 0} \\Big(u_o^Tv_c-\\log\\sum_{w\\in\\mathbb V}\\exp(u_w^Tv_c)\\Big) J(θ)=−T1​t=1∑T​−m≤j≤m∑j=0​(uoT​vc​−logw∈V∑​exp(uwT​vc​))"},{"title":"Intro to AI: Searching","path":"/ai-and-searching/","content":"Searching Searching Problem 的组成部分 state space successor function (包含 action 和 cost/reward，例如路径) start state 和 goal state 搜索问题的解决方法：a sequence of action which transforms start state into goal state 于是可以表示为图论问题 节点：代表 state 有向边：代表 state transformation 目标：从一个节点走到另一个节点 这张图是全局的，包含所有 state 的信息和转移。如果我们只关注从某一个 state (例如 start state) 出发的可能性（即局部的），则得到搜索树 ……但是子树结构大量重复 ……可能有环，导致树高无限高 解决方案：只保留正在考虑的部分子树，其余的全部扔掉 DFS 令一个状态可以拓展到 bbb 个状态，最多 mmm 层，则 树中节点数量：O(bm)O(b^m)O(bm) 原图无环时，DFS 搜索树是有穷的 不一定 Optimal，因为只往 leftmost 方向走 BFS 到达一个状态，所探明的节点数量为 O(bs)O(b^s)O(bs)，sss 为当前所处在的深度 必然是 complete 的 不一定 optimal，除非 cost 均为 111"},{"title":"LightRAG: query 处理查询的深入探究","path":"/lightrag-query-method/","content":"LightRAG.query() 支持四类查询： Local: 只注重局部信息 Global: 只注重全局信息 Hybrid Naive: 当作最传统的 RAG 来使用 Bypass: 不使用额外的知识库 Mix: Naive + Hybrid 下面，我们主要来看 GraphRAG 相关的 Hybrid 查询（其实就是一个 kg_query() 的 router） 启动异步查询"},{"title":"HKU GPU Farm 指北","path":"/hku-gpu-farm/","content":"进入 GPU Farm 用 ssh 链接 gpugate1.cs.hku.hk 然后输入密码即可。 ssh [your_username]@gpugate1.cs.hku.hk 进入 GPU 模式 注意 安装任何软件、仓库都必须先进 GPU Mode. gpu-interactive"},{"title":"LightRAG: 构建索引 insert() 方法深入探究","path":"/lightrag-insert-method/","content":"LightRAG LightRAG 是针对 GraphRAG 构建索引速度慢、消耗 Token 量大而诞生的解决方案，更多详细的方法论请移步 wiki，这一篇主要聚焦与代码层面的实现。 注意事项 由于 Python 的异步模块 asyncio 不支持嵌套，.insert() 方法在执行的时候会报错：This event loop has already been running。我们需要用 nest_asyncio 打个补丁 先安装 nest_asyncio 包 pip install nest-asyncio 再导入，并 patch 一下 import nest_asyncionest_asyncio.apply() 然后就 ok 了 启动异步索引构建 .insert() 这个函数是插入文档的入口，是一个同步函数（然而 readme 里写成了异步函数，well）。它的工作实际上就是调用了异步函数进行插入文档 .insert() def insert( self, input: str | list[str], split_by_character: str | None = None, split_by_character_only: bool = False, ids: str | list[str] | None = None, file_paths: str | list[str] | None = None,) - None: Sync Insert documents with checkpoint support Args: input: 文档字符串，或者包含很多字符串的列表。用于插入的文档 split_by_character: 是否按字符（而非 token）进行切割，如果把一个 token 切开了，把 token 放进来 split_by_character_only: 只以字符切割，忽略 token ids: 文档的标识符（应该不同），不提供则用 hash 计算 file_paths: 文档的路径，只用于 citation 目的 loop = always_get_an_event_loop() loop.run_until_complete( self.ainsert( input, split_by_character, split_by_character_only, ids, file_paths ) ) 异步进行插入 .ainsert() 将处理分为两个主要阶段，真是太有异步了（ 将文档加入队列 (.apipeline_enqueue_documents()) 处理队列中的文档 (.apipeline_process_enqueue_documents()) 完整代码 其实好像没必要开一个 Heading 4 ( .ainsert() async def ainsert( self, input: str | list[str], split_by_character: str | None = None, split_by_character_only: bool = False, ids: str | list[str] | None = None, file_paths: str | list[str] | None = None,) - None: Async Insert documents with checkpoint support Args: （基本同上） input: 文档字符串，或者包含很多字符串的列表。用于插入的文档 split_by_character: 是否按字符（而非 token）进行切割，如果把一个 token 切开了，把 token 放进来 split_by_character_only: 只以字符切割，忽略 token ids: 文档的标识符（应该不同），不提供则用 hash 计算 file_paths: 文档的路径，只用于 citation 目的 await self.apipeline_enqueue_documents(input, ids, file_paths) await self.apipeline_process_enqueue_documents( split_by_character, split_by_character_only ) 对文档进行预处理 docstring 比较明晰地写明了 .apipeline_enqueue_documents() 这个函数在干什么。这里截取出代码详细解释一遍。 1. Pre-Checks for file paths 首先是预检查：把单个 str 放进列表方便后续统一的操作。然后检查 file_path 的数量是不是和 str 的数量对得上，毕竟file_path 的作用是引用 Pre-check if isinstance(input, str): input = [input]if isinstance(ids, str): ids = [ids]if isinstance(file_paths, str): file_paths = [file_paths]# If file_paths is provided, ensure it matches the number of documentsif file_paths is not None: if isinstance(file_paths, str): file_paths = [file_paths] if len(file_paths) != len(input): raise ValueError( Number of file paths must match the number of documents ) 2. Pre-checks for Document IDs 检查文档的 ID。 如果提供了 ID，则 检查数量是否与文档的数量一致 检查 ID 是否重复 否则就生成 MD5 作为 ID 这个阶段还把 file_path 作为引用和文档内容打包在一起，形成 id: content: , file_path: 的 Object 格式 pack up information # 1. Validate ids if provided or generate MD5 hash IDsif ids is not None: # Check if the number of IDs matches the number of documents if len(ids) != len(input): raise ValueError(Number of IDs must match the number of documents) # Check if IDs are unique if len(ids) != len(set(ids)): raise ValueError(IDs must be unique) # Generate contents dict of IDs provided by user and documents contents = id_: content: doc, file_path: path for id_, doc, path in zip(ids, input, file_paths) else: # Clean input text and remove duplicates cleaned_input = [ (clean_text(doc), path) for doc, path in zip(input, file_paths) ] unique_content_with_paths = # Keep track of unique content and their paths for content, path in cleaned_input: if content not in unique_content_with_paths: unique_content_with_paths[content] = path # Generate contents dict of MD5 hash IDs and documents with paths contents = compute_mdhash_id(content, prefix=doc-): content: content, file_path: path, for content, path in unique_content_with_paths.items() 3. De-duplicate 紧接着在输入的文档内部进行去重，意思是说，去除输入里的重复文档 Code # 2. Remove duplicate contentsunique_contents = for id_, content_data in contents.items(): content = content_data[content] file_path = content_data[file_path] if content not in unique_contents: unique_contents[content] = (id_, file_path)# Reconstruct contents with unique contentcontents = id_: content: content, file_path: file_path for content, (id_, file_path) in unique_contents.items() 4. Register and Track Documents 为每一份文档建立一个状态，方便追踪（包括更新时间） 这里的 content_summary 并非 LLM 的总结，仅仅只是做了截取。 Code # 3. Generate document initial statusnew_docs: dict[str, Any] = id_: status: DocStatus.PENDING, content: content_data[content], content_summary: get_content_summary(content_data[content]), content_length: len(content_data[content]), created_at: datetime.now().isoformat(), updated_at: datetime.now().isoformat(), file_path: content_data[ file_path ], # Store file path in document status for id_, content_data in contents.items() def get_content_summary(content: str, max_length: int = 250) - str: Get summary of document content Args: content: Original document content max_length: Maximum length of summary Returns: Truncated content with ellipsis if needed content = content.strip() if len(content) = max_length: return content return content[:max_length] + ... 5. Filter based on Database 紧接着是根据已有的数据库过滤掉已经添加过的文档。 Code # 4. Filter out already processed documents# Get docs idsall_new_doc_ids = set(new_docs.keys())# Exclude IDs of documents that are already in progressunique_new_doc_ids = await self.doc_status.filter_keys(all_new_doc_ids)# Log ignored document IDsignored_ids = [ doc_id for doc_id in unique_new_doc_ids if doc_id not in new_docs]if ignored_ids: logger.warning( fIgnoring len(ignored_ids) document IDs not found in new_docs ) for doc_id in ignored_ids: logger.warning(fIgnored document ID: doc_id)# Filter new_docs to only include documents with unique IDsnew_docs = doc_id: new_docs[doc_id] for doc_id in unique_new_doc_ids if doc_id in new_docsif not new_docs: logger.info(No new unique documents were found.) return 6. Insert Filtered Documents 最后把过滤出来的文档插入文档数据库终于！（笑 Code # 5. Store status documentawait self.doc_status.upsert(new_docs)logger.info(fStored len(new_docs) new unique documents) 正式处理文档 .apipeline_process_enqueue_documents() 大体的结构分成 async with 部分和 try ... finally 部分，分别对应“获取所有待处理文档”和“处理文档”的逻辑。 获取待处理文档的逻辑比较直观：获取数据库的锁之后，把数据库里的要处理的文档拿出来。但是写的比较奇怪，先不去深挖细节了（挖个坑先 Code async with pipeline_status_lock:# Ensure only one worker is processing documentsif not pipeline_status.get(busy, False): processing_docs, failed_docs, pending_docs = await asyncio.gather( self.doc_status.get_docs_by_status(DocStatus.PROCESSING), self.doc_status.get_docs_by_status(DocStatus.FAILED), self.doc_status.get_docs_by_status(DocStatus.PENDING), ) to_process_docs: dict[str, DocProcessingStatus] = to_process_docs.update(processing_docs) to_process_docs.update(failed_docs) to_process_docs.update(pending_docs) if not to_process_docs: logger.info(No documents to process) return pipeline_status.update( busy: True, job_name: Default Job, job_start: datetime.now().isoformat(), docs: 0, batchs: 0, # Total number of files to be processed cur_batch: 0, # Number of files already processed request_pending: False, # Clear any previous request latest_message: , ) # Cleaning history_messages without breaking it as a shared list object del pipeline_status[history_messages][:]else: # Another process is busy, just set request flag and return pipeline_status[request_pending] = True logger.info( Another process is already processing the document queue. Request queued. ) return 注意 以下是 LightRAG 的核心部分，针对单篇文档提取 entity 和 relation，因此忽略了其他的一些操作，例如往 chunk database 里插入 chunks，插入 full doc 等等。包括错误处理、异步同步处理等等在内的很多细节也一并选择没有展开 那么肯定要考虑多篇文档的同时处理的。项目的处理也比较容易想到，也还是用 asyncio.create_task() 后用 asyncio.gather() 并行执行 把文档和 prompt 输入大模型 这一部分由 _process_single_content() 完成。首先 patch Prompt 输入，然后调用 use_llm_func_with_cache() 获得 LLM 输出并缓存下来。 接着开始解析输出，for ... in range(entity_extract_max_gleaning) 表示如果最多尝试提取关系 entity_extract_max_gleaning 次。 注意 以下的两个步骤是针对一块 chunk 做的。也就是说，如果文档太长而被切分成很多 chunk，那么以下两个步骤也会运行多次。 那么批量处理是如何进行的呢？ 项目源码这里采用多线程的方式批量处理 chunk. 具体做法是定义了一个 semaphore，然后将所有任务都用 asyncio.create_task() 包装后，由 asyncio.wait() 统一执行并阻塞直到任务全部完成。 这里我们先忽略错误处理，先关注后面的流程。 收集完 chunk_results 后，直接用 list 的 extend() 方法合并到 all_nodes, all_edges 里面 合并 chunk # Collect all nodes and edges from all chunksall_nodes = defaultdict(list)all_edges = defaultdict(list)for maybe_nodes, maybe_edges in chunk_results: # Collect nodes for entity_name, entities in maybe_nodes.items(): all_nodes[entity_name].extend(entities) # Collect edges with sorted keys for undirected graph for edge_key, edges in maybe_edges.items(): sorted_edge_key = tuple(sorted(edge_key)) all_edges[sorted_edge_key].extend(edges) 合并完这篇文档内的 entity 和 relation 之后，就要进入 graph insert 阶段了。我们把这部分放到后面再说。 解析大模型的输出 _process_extraction_result() 是 extract_entities() 中定义的一个内部辅助函数，主要负责处理来自大语言模型 (LLM) 的提取结果，将非结构化的文本响应转换为结构化的实体和关系数据 首先将 LLM 返回的结果依照配置好的分隔符切开为 Record/Completion，每一条 Record/Completion 可能包含实体或者关系。 async def _process_extraction_result( result: str, # 从 LLM 获取的提取结果文本字符串 chunk_key: str, # 文本块的唯一标识符，用于源跟踪 file_path: str = unknown_source, # 文件路径，用于引用来源（默认为unknown_source）):# 返回一个元组 (maybe_nodes, maybe_edges)，包含提取出的实体和关系 紧接着处理每一条记录（通过正则的匹配字符串可以发现每一条 Record 都包裹在一对圆括号内），在每一条 record 的内部，再使用 tuple delimiter 分割出 Entity 与 Attribute for record in records: record = re.search(r\\((.*)\\), record) if record is None: continue record = record.group(1) record_attributes = split_string_by_multi_markers( record, [context_base[tuple_delimiter]] ) 接着分别尝试将这条 record 解析为 Entity 或者 Relation. 根据 prompt.py 里记录的 prompt，可以看到我们要求大模型输出的格式为用 tuple delimiter 分割的元组。 Format each entity as (entitytuple_delimiterentity_nametuple_delimiterentity_typetuple_delimiterentity_description) # 提取为实体if_entities = await _handle_single_entity_extraction( record_attributes, chunk_key, file_path)if if_entities is not None: maybe_nodes[if_entities[entity_name]].append(if_entities) continue # 提取为关系if_relation = await _handle_single_relationship_extraction( record_attributes, chunk_key, file_path)if if_relation is not None: maybe_edges[(if_relation[src_id], if_relation[tgt_id])].append( if_relation ) 合并实体节点和关系边 这一块就是简单地用提取的实体名称和关系名称做合并 Code # Process gleaning result separately with file pathglean_nodes, glean_edges = await _process_extraction_result( glean_result, chunk_key, file_path)# Merge results - only add entities and edges with new namesfor entity_name, entities in glean_nodes.items(): if ( entity_name not in maybe_nodes ): # Only accetp entities with new name in gleaning stage maybe_nodes[entity_name].extend(entities)for edge_key, edges in glean_edges.items(): if ( edge_key not in maybe_edges ): # Only accetp edges with new name in gleaning stage maybe_edges[edge_key].extend(edges) 为了避免 LLM 遗漏 Entity，我们再额外用 LLM 判断是否有遗漏，用 prompt.py 里的 if_loop_prompt 作为输入，直到没有遗漏了就退出循环。 最后返回从这个 chunk 提取出来的 nodes 和 edges Code if_loop_result: str = await use_llm_func_with_cache( if_loop_prompt, use_llm_func, llm_response_cache=llm_response_cache, history_messages=history, cache_type=extract,)if_loop_result = if_loop_result.strip().strip().strip().lower()if if_loop_result != yes: break 更新知识图谱 由于是异步执行，我们需要先获取锁确保数据的完整性 async with graph_db_lock: 接着是根据已有的图谱过滤掉已经添加过的点和边，这一部分比较偏工程实现，这里不做展开 根据已有知识库进行过滤 # Centralized processing of all nodes and edgesentities_data = []relationships_data = []# Use graph database lock to ensure atomic merges and updatesasync with graph_db_lock: # Process and update all entities at once for entity_name, entities in all_nodes.items(): entity_data = await _merge_nodes_then_upsert( entity_name, entities, knowledge_graph_inst, global_config, pipeline_status, pipeline_status_lock, llm_response_cache, ) entities_data.append(entity_data) # Process and update all relationships at once for edge_key, edges in all_edges.items(): edge_data = await _merge_edges_then_upsert( edge_key[0], edge_key[1], edges, knowledge_graph_inst, global_config, pipeline_status, pipeline_status_lock, llm_response_cache, ) if edge_data is not None: relationships_data.append(edge_data) 然后是更新节点数据库 更新节点数据库 # Update vector databases with all collected dataif entity_vdb is not None and entities_data: data_for_vdb = compute_mdhash_id(dp[entity_name], prefix=ent-): entity_name: dp[entity_name], entity_type: dp[entity_type], content: fdp[entity_name] dp[description], source_id: dp[source_id], file_path: dp.get(file_path, unknown_source), for dp in entities_data await entity_vdb.upsert(data_for_vdb) ……和关系数据库 更新关系数据库 if relationships_vdb is not None and relationships_data: data_for_vdb = compute_mdhash_id(dp[src_id] + dp[tgt_id], prefix=rel-): src_id: dp[src_id], tgt_id: dp[tgt_id], keywords: dp[keywords], content: fdp[src_id]\\tdp[tgt_id] dp[keywords] dp[description], source_id: dp[source_id], file_path: dp.get(file_path, unknown_source), for dp in relationships_data await relationships_vdb.upsert(data_for_vdb)"},{"title":"异步编程","path":"/async-programming/","content":"Python: asyncio 与异步编程"},{"title":"OpenAI 流式传输与 StreamLit","path":"/openai-stream-mode-w-streamlit/","content":"流式传输 大模型的 API 通常都支持流式传输。所谓流式传输，就是指将大模型生成的文字拆分成一小块一小块发送过来，比如说每隔 555 秒就发送一次生成的文字，而不是等文字全部生成完毕才一次性全部发送。 这样做的好处在于 langchain langchain 的 ChatOpenAI 已经包装的十分完善了。"},{"title":"Python 并行库 joblib","path":"/python-joblib/","content":"joblib joblib 提供两个最核心的功能：caching 和 parallel computing. 使用 joblib.Parallel 进行并行计算 joblib.Parallel 的基础用法是通过 n_jobs 指定进程数（指定 n_jobs=-1 则表示能用多少用多少），初始化类后，用 joblib.delayed(function)(args) 指定每一个进程的工作 并行读取图片，保存为 NumPy Array import matplotlib.pyplot as plt # 读取图片from joblib import Parallel, delayed # 并行计算from rich.progress import track # 可视化进度条def read_img(path): return plt.imread(path)imgs = Parallel(n_jobs=-1)( delayed(read_img)(path) for path in track(csv[im_name], description=Loading images ... , transient=True) # transient=True 指定进度条在完成后隐藏) # parallel() 结束之后，imgs 是 List[np.ndarray]imgs = np.asarray(imgs) # 转化为 np.ndarray"},{"title":"使用 Socat 创建虚拟串口并指定名称","path":"/socat-usage/","content":"Preface 起因主要是社团……没有车的时候调试个 serial port 十分费劲，甚至根本调试不了写的对不对 所以只能用 socat 开虚拟串口模拟通讯了 socat 安装 安装比较容易，可以直接通过 apt 包管理器安装 sudo apt install socat socat 指定串口名称 指定名称时，用 link= 表示指定的串口位置，pty,raw,echo=0 表示串口的配置参数 sudo socat -d -d pty,raw,echo=0,link=/dev/ttyACM0 pty,raw,echo=0,link=/dev/ttyACM1 然后还需要给 /dev/ttyACM0, /dev/ttyACM1 这两个串口权限，方便起见，这里直接全部设为 rwx sudo chmod 777 /dev/ttyACM0sudo chmod 777 /dev/ttyACM1"},{"title":"Python 通过 URL 获取 Embedding","path":"/python-get-embedding-through-urls/","content":"requests 需求类似于不希望内部数据上传到其他网页，于是希望在本地同时部署 Embedding Model 和 LLM. 于是，我用 llama-server 同时 serve 了 BGE-m3 和 Deepseek-R1-Distill-Llama-8B，前者作为 Embedding 模型暴露在 http://localhost:8081，后者作为 LLM 暴露在 http://localhost:8080 然后就遇到了一个小问题，怎么通过 Python 去获取 Embedding 呢？我这里的解决方案是直接用 requests 库发送请求了。好在 llama.cpp 提供的 llama-server 能够兼容 OpenAI 的 API 接口。 import requestsembedding_url = http://localhost:8081/v1/embeddings# OpenAI compatible embeddingapi_key = not_used# 因为是本地部署，所有干脆没有设置 API Keydata = input: 要嵌入的文字, model: BGE-m3, # 这里就填本地部署的模型名称headers = Authorization: fBearer api_key, content-type: application/json,result = requests.post( embedding_url, data=str(data), # 这里必须将字典以字符串的格式传入 headers=headers, # 这个 headers 其实也可以不用)embedding = result.json()[data][0][embedding]"},{"title":"Linux Mint 22.1 升级 6.11 内核和升级英伟达 560 驱动","path":"/upgrade-to-linux-kernel611-and-nvidia/","content":"升级到 Linux Kernel 6.11.0-21.21 事情的原委很简单，Update Manager 疯狂地提醒我该升级 Kernel 了，正巧想玩玩 CUDA 升级一下 nvcc，于是想顺便升级一下 nvidia-driver. 但是很快就初见端倪，升级 Kernel 提示 installed linux-image-6.11.0-21-generic package post-installation script subprocess returned error exit status 11 啊？报错了？又往前翻了翻，发现 nvidia-fs/2.22.3 autoinstall failed due to missing dependencies: nvidia OK 破案了，原来又是英伟达驱动搞的鬼，那么先处理 nvidia-drvier 吧[1] nvidia-driver-560 我现在已经在用 nvidia-driver-550 了，但是为什么会报错缺少 nvidia 呢？这里没有多想，就顺着上面的帖子，重装驱动了。 我是直接在 Driver Manager 里的 GUI 操作，但是报错安装失败。 man! 怎么个事？重启一下系统，虽然可以开机，但是默认切换到 Intel 核显了，并且 nvidia-smi 也提示无法链接 GPU. 值得一提的是，系统居然是 6.11 内核的……我还以为没安装成功呢 hhhh 那看来只能进 recovery-mode 了 Recovery Mode 进入 Recovery Mode 后，尝试用命令行删除驱动，再重新安装驱动。先移动到 network 打开网络，然后移动到 root 回车进入命令行。先删除所有英伟达的驱动 apt purge ~nnvidia 删除倒是挺简单的，然后安装驱动，我直接选择了 nvidia-driver-560-open（这个版本是英伟达官方推荐 Ubuntu 24.04 系统使用的驱动版本，而我使用的 Linux Mint 也是基于 Ubuntu 24.04 制作的） apt install nvidia-driver-560-open 回车等待结果，然而，在 Building for Linux-kernel-6.11.0-21.21-generic 的时候，却出现了报错 nvidia-dkms-560 configuration failed# 大致差不多长这样，其实就是提示你有两个组件构建失败 不过在构建失败后，也给出了一个日志文件让我们去查看，日志目录是 /var/lib/dkms/nvidia/版本号/build/make.log，我们用 vim 进行查看 cc: unrecognized command line option -ftrivial-auto-var-init=zero# 大致是这么个意思 然后又检查一下 cc -v 版本信息，发现是 11.4.0，而 nvidia-driver-560-open 是用 gcc-13 构建的 所以问题很明晰了，nvidia-driver-560-open 用 gcc-13 进行构建，但是由于编译时，可执行文件用的是 cc，而在我的机器上，我的 cc 版本为 gcc-11，所以不支持这个命令行参数（即 ftrivial-auto-var-init=zero），因此构建失败，也导致了后续一系列的问题。 好消息是，我本机已经安装过 gcc-13，因此，我先删除了 gcc-11, g++-11, gcc-11-base 等包，然后用 symlink 将 cc 直接映射为 gcc-13 ln -s /usr/bin/gcc-13 /usr/bin/cc 再次安装 nvidia-driver-560-open，成功！安装 Linux Kernel 6.11，也是成功！ 还好没有心急让电脑 remake TAT Forum Answer ↩︎"},{"title":"Matplotlib 快速入门指南","path":"/matplotlib-fast-tutorial/","content":"网格布局 plt.subplots() 例如，我想将 121212 张 MNIST 图片排列成 333 行 444 列的样子。 使用 fig, axes = plt.subplot() 新建图片，并划分成网格 可以搭配 axes = axes.flatten() 进一步方便处理（将二维网格拍成一维，方便循环处理） 代码 fig, axes = plt.subplot(3, 4) # 可以额外指定 fig_size 指定图片大小axes = axes.flatten()for i in range(12): axes[i].imshow(......) axes[i].set_title(......) # 每一张小图像的标题 axes[i].axis(off) 如果想要给整张图添加总标题的话，是 plt.suptitle(......) 散点图 plt.scatter()"},{"title":"conda 与 pip 配置代理服务器","path":"/conda-pip-proxy/","content":"conda 配置代理服务器 conda 配置代理服务器需要在 ~/.condarc 这个文件里配置 channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro/ - defaultsshow_channel_urls: truecustom_channels: conda-forge: https://mirrors.bfsu.edu.cn/anaconda/cloud msys2: https://mirrors.bfsu.edu.cn/anaconda/cloud # xxx: yyyyyyssl_verify: falseproxy_server: http: # 这里配置公司内网的 proxy server https: # 例如：http://[Account]:[Passwrod]@proxyhk.huawei.com:8080 pip 配置代理服务器 pip 需要在 ~/.pip/pip.conf 这个文件里配置。Linux 的配置文件在 ~/.pip/pip.conf，Windows 为 AnaConda: C:/Users/[Username]/pip/pip.ini MiniConda: C:/Users/[Username]/AppData/Roaming/pip/pip.ini 添加： [global]index-url = # 代理服务器地址。例如 https://mirror.tools.huawei.com/pypi/simpletrusted-host = # 主机域名。例如 mirror.tools.huawei.comtimeout = 120 uv 包管理器添加 indexing 在 uv init 初始化本地目录后，修改 pyprojects.toml 添加下面的条目 [[tool.uv.index]]url = https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple/ # 这个是清华大学的 pip 软件包源default = true"},{"title":"Python LangChain 将图像当作 URL 传递","path":"/raw-image-pass-as-url/","content":"Pass Image as if URL 一个小 trick 可以将本地图片 encode 成 byte string 之后，放在 URL 栏里传递给多模态大模型。 import base64with open(path/to/image.png, rb) as image_file: b64_image = base64.b64encode(image_file.read()).decode(utf-8)def encode(path): with open(path, rb) as image_file: code = base64.b64encode(image_file.read()).decode(utf-8) return fdata:image;base64,code 然后就可以正常放在 URL 栏里了。"},{"title":"Django 快速开始","path":"/django-kickstart/","content":"Django 组织结构 Django 大体架构是一个 Project 管理若干个小 Application，每一个 Application 负责一个功能，跟 Application 平行的还有一个用于部署网站的 Config Folder（默认和 Project 同名）. UH├── CedarsCenter│ ├── admin.py│ ├── apps.py│ ├── __init__.py│ ├── migrations│ │ └── __init__.py│ ├── models.py│ ├── tests.py│ └── views.py├── manage.py├── StudentCenter│ ├── admin.py│ ├── apps.py│ ├── __init__.py│ ├── migrations│ │ └── __init__.py│ ├── models.py│ ├── tests.py│ └── views.py└── UH ├── asgi.py ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-311.pyc │ └── settings.cpython-311.pyc ├── settings.py ├── urls.py └── wsgi.py 管理某一个 Project 的时候，通过 manage.py 运行相应的指令。例如在当前 Project 下新增一个 Application poll，则运行 python manage.py startapp poll Django 中 Model 的作用 Model 的作用只是用来查询数据用的"},{"title":"Python 使用 C/C++ 接口","path":"/python-c-integration/","content":"Python 调用 C/C++ 代码 如何编写 C/C++ 代码？ 首先导入 Python.h 头文件，包含了必要的结构体、方法。（需要通过 sudo apt install python3-dev 提前安装好） #define PY_SSIZE_T_CLEAN#include python3.12/Python.h // 我这里需要额外指定一下路径 编译为动态库 g++ -fPIC [file_name] -shared -o [module_name].so 在 Python 里使用 直接通过这个 Module 的名字导入 import [module_name]# ......"},{"title":"MinerU Examples","path":"/MinerU-examples/","content":"uv 包管理器安装 MinerU 先用 uv 安装 setuptools wheel torch uv pip install setuptools wheel torch 然后再安装 detectron2 uv pip install --no-build-isolation git+https://github.com/facebookresearch/detectron2.git 最后安装 magic-pdf[full] uv pip install magic-pdf[full] --extra-index-url https://wheels.myhloli.com --prerelease=allow 最后检查 magic-pdf 的版本 =0.7.0，而不是 0.6.1 如果像使用 GPU 进行 PaddlePaddle OCR 的推理，继续安装 paddlepaddle-gpu uv pip install paddlepaddle-gpu MinerU Command Line MinerU API 使用指南 MinerU 的使用流程基本上是 将 PDF 加载为 magic_pdf.data.dataset.Dataset 执行 OCR 和 Layout Inference 这里还想更详细地记录一下 API，感觉 Documentation 里写的不是很全，得从 demo.py 里找。"},{"title":"Linux 下用 HKU 账号使用 HK eduroam WiFi","path":"/connect-to-eduroam-hku/","content":"eduroam 我是 Linux Mint 系统，所以 Ubuntu/Mint 应该比较通用。 Option Value Security WPA WPA2 Enterprise Authentication Protected EAP (PEAP) Anonymous Identity 留空 Domain hku.hk CA Certificate 选 (None)，勾选 No CA Certificate is required PEAP version Automatic Inner Authentication MSCHAPv2 Username UID@hku.hk，UID 是邮箱 @ 前的部分，这里后缀必须是 @hku.hk Password Portal 的登录密码"},{"title":"Docker Introduction","path":"/docker-intro/","content":"Dockerfile Docker Image 编写完 Dockerfile 后，我们可以用命令创建一个 docker 镜像 docker build -t [镜像名称] [Dockerfile 所在目录] 启动 container 还需要一个 container 才能运行起来 docker run -p [映射到主机的哪个端口]:[容器内的哪个端口] -d [镜像名称] -p：指定端口 -d：后台运行。想要查看终端输出的话需要到 docker desktop 里查看 用 Volume 保存 container 的数据 利用 -v 参数，把本地文件夹 ~/A 挂载到 container 里的 .../B，这样在 container 里读写 .../B 其实等于读写 ~/A. 我们先创建一个数据卷 docker volume create [数据卷位置] 然后在 docker run 的时候进行挂载 docker run -v [数据卷在本地的位置]:[数据卷在容器里的位置] -d [镜像名称] Docker Compose 多个容器共同协作：例如数据库和前端分离。 在 docker-compose.yml 里用 services 进行定义 # docker-compose.ymlservices: # 定义前端和数据库 front-end: # 前端 build: . # 镜像——从文件夹构建 ports: # 端口映射 - 80:5000 database: # 数据库 image: mysql # 镜像——从其他地方拉取 environment: # 可以定义环境变量 OPENAI_API_KEY: ... OPENAI_API_BASE: volumes: # 数据卷，等同于 -v 参数 - ~/A:.../B 定义完毕后，使用 docker compose up -d 来运行所有的 container docker compose down 来停止并删除所有的 container"},{"title":"本地部署 llama.cpp 大模型服务器并连接","path":"/local-deploy/","content":"llama.cpp 的安装、编译 alternative: llama-cpp-python 提供了 llama.cpp 的 Python 接口。通过 llama-cpp-python 也可以启动一个 LLM Server 启动一个 LLM server 直接在命令行里输入启动服务器 llama-server -m [模型路径] --port 8080 模型要保证必须是 .gguf 格式，可以使用 llama.cpp 项目根目录下的 convert_hf_to_gguf.py 进行转换。 convert_hf_to_gguf 食用方法 配置好虚拟环境后，命令行里输入 python convert_hf_to_gguf.py [模型.bin文件所在的目录] 这个目录末尾应该是哈希码，例如 ~/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/ad9f0ae0864d7fbcd1cd905e3c6c5b069cc8b562 连接 如果用 LangChain 进行连接，必须注意要输入 http://localhost:8080 的 http://（被坑了）"},{"path":"/progress.html","content":"Study Progress Math Topic Progress Optimization Theory Lecture 5.1 CS Topic Progress CS336 Lecture 3, 2/3 CS3110 OCaml 2.3"},{"title":"About Me","path":"/about/index.html","content":"About Me 挖坑，基本不会填坑（嗯…… 开发计划 开发 Wallpaper engine on Linux 嗯，现在啥都不会（ 开发 RM Simulator 希望是能够自带物理建模和模拟、参数调整、数据统计、与本机程序交互等等功能 但是现在啥都不会（ Archive.yazi (compress and extract) yazi.rs 插件，用按键解压、打包 学习计划 Stanford CS336 (0/inf) Princeton 强化学习背后的数学 不是很紧急 CUDA 编程 顺带也想学 Triton/TensorRT/OpenVINO 这类的 T.T 编写一个 LLM 倒是有抄过一遍代码，但是还是有点懵 以及 VLM 和 VLA（然而甚至都没试过 课程学习！ 数学方面的话，线性代数、数学分析……感觉学得不是很深入 然后 OS, 计算机网络 感觉也需要回顾一下…… 金融课程！感觉尤其是量化金融方面的 日语！ 长期目标，偷偷润日本自驾游（ 当下需要干的事 RAG 针对 PPT 做出改进 有点紧急，还是想在实习期间搞点成果的，但确实是不会，也没有头绪不太熟练 编写 LIBERO 的数据集 科研实习……如果能搞出 task 流水线就好了……"},{"title":"friends","path":"/friends/index.html","content":"Friends XXZ's Bloghttps://www.thisisxxz.com"},{"title":"OpenVLA 代码解析 (1)","path":"/wiki/agentai/OpenVLA-code-p1.html","content":"5d4944576327e6e1f59f35f07afdbc7917ffe61195e5ff6f7af12e15300ba733 Password is needed."},{"title":"OpenVLA-dataproc-pipeline","path":"/wiki/agentai/OpenVLA-dataproc-pipeline.html","content":"OpenVLA 数据处理过程 normalization diff embodyment 在环境下评测 model, performance, accuracy"},{"title":"Google Gemini Robotics","path":"/wiki/agentai/google-gemini-robotics.html","content":"AI Summary Background • Core Problem: The paper addresses the challenge of bridging the gap between state‐of‐the‐art multimodal AI models and practical robotics. Although digital AI models have achieved impressive capabilities in processing text, images, and other modalities, these models traditionally lack the embodied reasoning (i.e., the ability to understand and physically interact with the real world) required for autonomous robot control. • Current Limitations and Challenges: Existing vision–language models excel at perception and symbolic tasks, yet they fall short when it comes to physical grounding. The authors point out that current research struggles with robust spatial understanding, dexterous manipulation, safe human–robot interactions, and adapting to novel object configurations and environments. Moreover, there is an absence of standardized benchmarks that cover the nuanced set of reasoning skills needed for embodied tasks. • Real-world Needs and Theoretical Gaps: The paper underscores the necessity for general-purpose robots capable of smooth, safe, and reactive movements. There is a critical need to extend digital reasoning to handle real-world cues such as 3D geometry, object affordances, and trajectory planning. The authors identify a theoretical gap in unifying multimodal understanding (visual and language) with physical action planning. Motivation • Driving Factors Behind the New Method: The motivation comes from the desire to leverage the advanced multimodal reasoning of models like Gemini 2.0 and extend these capabilities to control physical robots. The authors aim to overcome the limitations of existing systems which are typically confined to digital domains by integrating a comprehensive embodied reasoning framework into robotics. • Shortcomings of Existing Approaches: Current practices often suffer from poor generalization when faced with unseen object types, variable settings, and complex manipulation tasks. Many existing methods lack the ability to plan sequential actions under safety constraints and cannot efficiently adapt to new robot embodiments or environments. Issues such as limited computational efficiency in planning and a narrow focus on perception without actionable outputs further drive the need for an improved, unified approach. Methodology • Core Innovation: The paper introduces the Gemini Robotics family of models––a suite of vision–language–action (VLA) systems that fuse the powerful multimodal understanding of Gemini 2.0 with a dedicated robotics layer. Two principal variants are proposed: Gemini Robotics-ER (Embodied Reasoning): Enhances the base model’s spatial and temporal reasoning to perform tasks such as object detection, 2D pointing, trajectory prediction, and top-down grasp estimation. Gemini Robotics: Extends these capabilities by incorporating robot action data to achieve high-frequency, dexterous control over physical robots. • Technical Steps and Key Components: Multimodal Input Fusion: Integrates visual inputs (images) and textual instructions, using open‐vocabulary queries to detect and localize objects in both 2D and 3D (e.g., 3D bounding boxes and multi-view correspondence). Embodied Reasoning Pipeline: Employs a chain-of-thought (CoT) prompting technique, which encourages the model to generate intermediate reasoning steps, thereby ensuring precise spatial grounding and sequential planning. (Chain-of-thought prompting is a method where the model is instructed to “think out loud” through intermediate steps before giving the final answer.) Robot Control Integration: A dedicated pipeline converts high-level planning into low-level actuation commands via a robot API, managing safe and reactive motions while adhering to physical constraints. Specialization and Adaptation: An optional fine-tuning stage enables the system to adapt to long-horizon tasks, enhance dexterity (e.g., folding or playing cards), and adjust to variations in robot embodiments such as bi-arm platforms or humanoids. • Fundamental Differences from Baseline Approaches: Unlike conventional methods that may treat perception and control as separate tasks, the Gemini Robotics models offer an end-to-end, integrated solution. By combining embodied reasoning with action data, these models generalize better to novel tasks, improve adaptation with few demonstrations, and facilitate safe control in dynamic environments. Experiments Results • Experimental Setup: The models are evaluated on several benchmarks including the newly introduced Embodied Reasoning Question Answering (ERQA) benchmark, RealworldQA, and BLINK. The experimental design encompasses tasks ranging from simple object detection to complex, dexterous manipulation tasks in both simulated and real-world scenarios. Datasets include images sourced from in-house collections and publicly available datasets like OXE, UMI Data, and MECCANO. Baseline comparisons are made against existing vision–language and diffusion-based robotics models, using performance metrics such as accuracy, success rate, and continuous progress scores. • Key Findings: Gemini 2.0 variants (Flash and Pro Experimental) achieve state-of-the-art performance across multiple benchmark evaluations, often with notable improvements when chain-of-thought prompting is used. Quantitative results show improvements in tasks like 2D pointing accuracy (measured by how well predicted points fall within ground-truth regions) and successful grasp predictions in real-world robot control. In dexterous tasks such as folding an origami fox or playing a card game, Gemini Robotics outperforms baseline models in both binary success metrics and nuanced progress scores. • Ablation and Sensitivity Analyses: The paper reports ablation studies that highlight the benefits of each component—from embodied reasoning enhancements to fine-tuning stages. Sensitivity analyses reveal that prompts such as chain-of-thought can significantly affect overall performance, and that the integrated approach provides robust improvements even under distribution shifts (e.g., novel visual variations or rephrased instructions). Effectiveness Analysis • Underlying Reasons for Success: The method’s effectiveness stems from its dual focus on robust multimodal understanding and practical action integration. By augmenting a strong foundation model with specialized embodied reasoning and robot-specific training, the system manages to bridge the digital–physical divide. In particular, the use of multi-view spatial understanding and sequential reasoning ensures continuity between perception and physical manipulation. • Critical Success Factors: Model Architecture and Training: Leveraging Gemini 2.0’s large-scale, pre-trained capabilities provides a strong foundation for embodied reasoning. Effective Data Fusion: Integrating diverse data types—including images, text, and robot sensor readings—improves generalization and adaptability. Prompting Strategies: Techniques like chain-of-thought prompting contribute to clear intermediate reasoning, enhancing the decision-making process. Safety and Adaptability Protocols: The incorporation of robot API interfaces and safety guidelines enables smooth interaction in regulated physical environments. • Potential Limitations and Applicability Boundaries: While the proposed models demonstrate impressive performance, their success is tightly coupled with the quality and diversity of training data. The system may face constraints when encountering drastically different physical environments or unanticipated object configurations. Furthermore, the reliance on comprehensive safety protocols suggests that additional work is required to ensure deployment in less structured or more variable real-world settings. Contributions • Theoretical Contributions: The paper provides a unified framework that connects high-level multimodal reasoning directly with low-level robot control. It introduces new benchmark tasks (e.g., ERQA) to evaluate embodied reasoning, setting a new standard for measuring the full spectrum of capabilities required for physical interaction. • Practical Contributions: The development of the Gemini Robotics family offers an end-to-end solution for controlling robots with diverse manipulation tasks, from dexterous object handling to adaptive task execution. Detailed model cards and open-source evaluation protocols are provided, facilitating reproducibility and further research in embodied AI. The work also contributes guidelines and best practices for ensuring safety and responsible development in robotics applications. • Implications for Future Research: This research marks a significant step toward general-purpose embodied AI. Future work can build on these findings to further enhance generalization, reduce reliance on extensive fine-tuning, and tackle more complex physical interactions. The integration of multimodal reasoning with action control paves the way for robots that can adapt to diverse, unstructured real-world environments while ensuring safe and reliable operation."},{"title":"内核性能分析工具","path":"/wiki/ai-infra/performance-analysis-tools.html","content":"内核优化的常见步骤 分析 Kernel 的执行时间 统计、查看各个 Kernel 的执行时间 定位性能瓶颈，确定需要优化的 Kernel 检查 GPU 的利用率等信息 例如检查是否占用过多的寄存器 （更细粒度）确定 Kernel 的性能瓶颈 优化 Kernel 性能 通过各种技术手段（软硬件、调度、内存优化）优化 Kernel PyTorch 内置 torch.profiler TensorBoard Holistic Trace Analysis NVIDIA Nsight 工具 Nsight System Nsight Compute Triton Proton"},{"title":"Pi 0, A Vision-Language-Action Flow Model for General Robot Control","path":"/wiki/agentai/pi-zero.html","content":"AI Summary Background The paper addresses the challenge of developing a unified robot foundation model that can handle a wide range of dexterous tasks across different robot platforms. Current robot learning approaches often suffer from limited generalization, data scarcity, and brittle performance when adapting to complex practical scenarios. Existing methods typically focus on narrow tasks or use discretized models that struggle with continuous, high-frequency control. The authors identify a real-world need for models that can integrate rich semantic understanding (gained from large-scale vision-language pre-training) with fine-grained physical control to perform intricate multi-stage tasks such as laundry folding or box assembly. Motivation The direct drive behind this work is the gap between the robust, versatile behavior seen in large language and vision-language models and the limitations of current robot control systems. Existing approaches fail to meet the demands of real-world robotic manipulation due to: Limited Generalization: Models are often trained on task-specific data and do not transfer well to diverse environments or novel tasks. Discrete Outputs: Many prior methods rely on autoregressive and token-based approaches, which are not well-suited for high-frequency continuous control. Data and Recovery Challenges: Training solely on curated, high-quality data results in brittle systems that cannot efficiently recover from errors or unexpected perturbations. This motivates the integration of internet-scale semantic pre-training with a continuous action generation method, thereby combining robust, flexible understanding with high-precision control. Methodology The core innovation is the design of a vision-language-action model (π0) that unifies pre-trained semantic knowledge with a novel mechanism for continuous action prediction. Key elements include: Pre-trained VLM Backbone: The model starts with a vision-language model (VLM) pre-trained on vast internet data (e.g., PaliGemma). This allows the system to inherit rich semantic representations and language understanding. Brief explanation: A VLM (Vision-Language Model) is trained on image-text pairs to extract visual and linguistic features simultaneously. Action Expert with Flow Matching: A separate module—termed the action expert—is added to predict continuous action sequences. Instead of relying on discrete token outputs, it uses conditional flow matching. Brief explanation: Flow matching is a variant of diffusion methods where a denoising vector field is learned to progressively transform noise into data, enabling precise continuous outputs. The model predicts action chunks (e.g., 50 future steps) at once, accommodating high-frequency control (up to 50 Hz). The design employs multi-stage training where the model is first exposed to diverse, lower-quality pre-training data and later fine-tuned on high-quality, task-specific data. Architectural Separation: The transformer backbone routes standard inputs (images and language) through the pre-trained VLM segment, while robot-specific proprioceptive inputs and continuous actions are handled by an additional expert. This mixture of experts approach allows leveraging pre-existing knowledge while adapting to robotics-specific demands. Two-Phase Training Recipe: Emphasizing the importance of both data diversity and quality, the training is split into a large-scale pre-training phase (covering 10,000 hours of data across 68 tasks and 7 robot configurations) and a post-training phase that refines the model for complex downstream tasks. Experiments Results The authors evaluate their model on a wide range of dexterous manipulation tasks spanning different robots and complexities: Experimental Setup: Tasks include shirt folding, table bussing (easy and hard versions), grocery bagging, removing toast from a toaster, laundry folding, box assembly, packing eggs, and more. Diverse robot configurations such as single-arm, dual-arm, and mobile manipulators are used to test cross-embodiment performance. Evaluation metrics involve task-specific score rubrics that measure progress and success (e.g., percentage of task completion or discrete scores per subtasks). Key Findings: The full π0 model, trained with both pre-training and fine-tuning, outperforms baseline approaches like OpenVLA and smaller models (π0-small) on both direct prompting (out-of-box performance) and after fine-tuning. Ablation studies show that incorporating continuous action generation via flow matching results in significant improvements in dexterity and robustness. The approach demonstrates the capability to perform complex, multi-stage tasks that were previously challenging or unsolvable using traditional methods. Effectiveness Analysis The method works due to several critical factors: Integration of Semantic and Physical Knowledge: The use of a pre-trained VLM imparts broad semantic understanding, enabling the model to interpret language commands effectively. This, when combined with flow matching, allows the system to translate high-level instructions into accurate continuous motions. Flow Matching for Continuous Control: By modeling the entire distribution of actions as a continuous process, the approach ensures high precision and the ability to recover from errors—a necessity in real-world dexterous tasks. Robust Training Recipe: The dual-phase training method (diverse pre-training followed by targeted fine-tuning) allows the model to learn both general competencies and task-specialized behaviors while mitigating issues of overfitting and brittleness. Critical Success Factors: The modular architecture separating vision-language and robotics-specific pathways facilitates efficient processing and speed. The ability to predict action chunks supports high-frequency control, crucial for tasks that require fine motor skills. Potential limitations include the reliance on massive amounts of pre-training data and the current lack of comprehensive understanding about optimal data composition. Moreover, while promising for robot manipulation, it remains to be seen how well the approach transfers to other domains such as autonomous driving or legged locomotion. Contributions The paper makes several novel contributions: Theoretical Contributions: It introduces a novel integration of a pre-trained vision-language model with flow matching to generate continuous action distributions—bridging the gap between discrete output methods and the requirements of dexterous robot control. The work advances our conceptual understanding of how multi-modal pre-training can be leveraged to build versatile and general-purpose robot foundation models. Practical Contributions: The π0 model is demonstrated across multiple robot embodiments and a wide array of tasks, showcasing its practicality and robustness in real-world settings. The proposed multi-stage training recipe—including cross-embodiment data integration and action chunking—sets a new benchmark for large-scale robot learning experiments (using approximately 10,000 hours of diverse data). The empirical results establish a state-of-the-art performance in complex, temporally extended tasks such as laundry folding and box assembly. Implications for Future Research: This work lays the groundwork for future exploration of universal robot foundation models that might extend beyond manipulation to domains like navigation and legged locomotion. The proposed framework encourages further studies on optimal data weighting, integration techniques, and extending these methods to even broader real-world scenarios. Background Large Scale Right Model Architecture Right Trainging Recipe"},{"title":"Triton Introduction","path":"/wiki/ai-infra/triton-intro.html","content":"PyTorch 关键组件 TorchDynamo 将所有复杂算子简化到 PrimTorch 中的 250 个算子 移除未使用的算子 确实需要存储、写入内存的中间算子，以及可融合的算子，从而减少开销 PrimTorch 定义了两个算子集合：Aten ops 和 Prim ops 将 PyTorch 程序的各种计算用这些算子集里的算子表示 简化后端需要编写的算子数量 AOTAutograd 提前获取反向传播 基于完整的 forward/backward 根据算子的依赖关系进行算子调度，对算子和层进行融合 TorchInductor 进行算子融合 自动生成低级 GPU 上的 Triton 代码（或者 CPU 上的 C++/OpenMP） 编译流程 我们用下面的例子介绍一下大致的编译流程，在运行时加入调试参数 TORCH_LOGS=... python example.py 查看中间的日志输出 import torch@torch.compiledef toy_example(x: torch.Tensor) - torch.Tensor: y = x.sin() z = y.cos() return zif __name__ == __main__: x = torch.randn(1000, device=cuda, requires_grad=True) # 开启反向传播 Step 1. TorchDynamo 运行 TORCH_LOGS=dynamo uv run example.py，我们先来看第一步 TorchDynamo 的输出。 日志输出 [torch/_dynamo/symbolic_convert.py:2706] [0/0] Step 1: torchdynamo start tracing toy_example [很长的路径]/example.py:5[torch/_dynamo/symbolic_convert.py:3028] [0/0] Step 1: torchdynamo done tracing toy_example[torch/_dynamo/output_graph.py:1458] [0/0] Step 2: calling compiler function inductor[torch/_dynamo/output_graph.py:1463] [0/0] Step 2: done compiler function inductor[torch/fx/experimental/symbolic_shapes.py:4547] [0/0] produce_guards[torch/_dynamo/pgo.py:636] [0/0] put_code_state: no cache key, skipping[torch/_dynamo/eval_frame.py:398] TorchDynamo attempted to trace the following frames [ toy_example [很长的路径]/example.py:5 ][torch/_dynamo/utils.py:446] TorchDynamo compilation metrics: Function Runtimes (s) ------------------------------------ -------------- _compile.compile_inner 0.5482 OutputGraph.call_user_compiler 0.4845 _recursive_pre_grad_passes 0.0018 create_aot_dispatcher_function 0.4817 _recursive_joint_graph_passes 0.0684 compile_fx.locals.fw_compiler_base 0.3442 compile_fx_inner 0.3437 inductor_codecache_torch_key 0.0523 TritonBundler.read_and_emit 0.0002 PyCodeCache.load_by_key_path 0.0122 async_compile.precompile 0.007 async_compile.wait 0.0001 从日志中可以看到，TorchDynamo 的框架流程就是 对要编译的模型进行追踪，然后编译并生成中间表示 (FX Graph IR) 调用 compiler.inductor 对模型进行化简 1.1 Dynamo 图捕获 Dynamo 首先进行图捕获。这里，__graph_code 将原始代码的 Dataflow 进行捕获，并输出捕获的 DAG，即 FX Graph IR. FX Graph IR # [torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code] def forward(self, L_x_: f32[1000]): l_x_ = L_x_ # File: example.py:7 in toy_example, code: y = x.sin() y: f32[1000] = l_x_.sin(); l_x_ = None # File: example.py:8 in toy_example, code: z = y.cos() z: f32[1000] = y.cos(); y = None return (z,)[torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code] def forward(self, L_x_: f32[1000][1]cuda:0): l_x_ = L_x_ # File: example.py:7 in toy_example, code: y = x.sin() y: f32[1000][1]cuda:0 = l_x_.sin(); l_x_ = None # File: example.py:8 in toy_example, code: z = y.cos() z: f32[1000][1]cuda:0 = y.cos(); y = None return (z,) 1.2 AOTAutograd Dynamo 的 AOTAutograd 阶段 生成正向传播图和反向传播图（也是表示为 FX Graph IR 的形式） 会将 FX Graph IR 中的算子替换为 ATen 算子库里的算子 基于完整的正向、反向传播图的视角，根据依赖关系，进行算子调度、对算子和层进行融合 将复杂的算子根据字典进一步分解为更底层的 Core ATen IR 算子或者 Prim IR 算子 AOTAutograd IR 生成的正向图与反向图 # 这个是正向图# ===== Forward graph 0 =====# torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module): def forward(self, primals_1: f32[1000][1]cuda:0): ## File: example.py:7 in toy_example, code: y = x.sin() sin: f32[1000][1]cuda:0 = torch.ops.aten.sin.default(primals_1) ## File: example.py:8 in toy_example, code: z = y.cos() cos: f32[1000][1]cuda:0 = torch.ops.aten.cos.default(sin); sin = None return (cos, primals_1)# 这个是反向图# [torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:603]# [0/0] [__aot_graphs]# # TRACED GRAPH# ===== Backward graph 0 =====eval_with_key.1 class GraphModule(torch.nn.Module): def forward(self, primals_1: f32[1000][1]cuda:0, tangents_1: f32[1000][1]cuda:0): # File: example.py:7 in toy_example, code: y = x.sin() sin: f32[1000][1]cuda:0 = torch.ops.aten.sin.default(primals_1) # File: example.py:8 in toy_example, code: z = y.cos() sin_1: f32[1000][1]cuda:0 = torch.ops.aten.sin.default(sin); sin = None neg: f32[1000][1]cuda:0 = torch.ops.aten.neg.default(sin_1); sin_1 = None mul: f32[1000][1]cuda:0 = torch.ops.aten.mul.Tensor(tangents_1, neg); tangents_1 = neg = None # File: example.py:7 in toy_example, code: y = x.sin() cos_1: f32[1000][1]cuda:0 = torch.ops.aten.cos.default(primals_1); primals_1 = None mul_1: f32[1000][1]cuda:0 = torch.ops.aten.mul.Tensor(mul, cos_1); mul = cos_1 = None return (mul_1,) 2. Inductor Triton 的核心： compile() model fullgraph dynamic"},{"title":"计算几何：双向连接边表 DCEL","path":"/wiki/algo/DCEL.html","content":"Doubly-Connected Edge List 半边 Half Edge Half Edge"},{"title":"OpenVLA 论文","path":"/wiki/agentai/OpenVLA.html","content":"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 Password is needed."},{"title":"KMP 算法","path":"/wiki/algo/KMP.html","content":"KMP 算法 核心是最长公共前后缀."},{"title":"博弈论：Minimax 与 Alpha-Beta 剪枝","path":"/wiki/algo/alpha-beta-minimax.html","content":"Minimax 博弈游戏可以简化成这样的局面：第 iii 轮我方行动，目的是最大化得分；下一轮对手行动，目的是最小化我方得分；再下一轮又是我方行动，目的是最大化得分……如此往复。 Minimax 搜索优化：Alpha-Beta 剪枝 记节点分数为 xxx，我们给每一个节点维护两个值 α,β\\alpha,\\betaα,β 满足 α≤x≤β\\alpha\\le x\\le\\betaα≤x≤β. 算法流程大致如下： 如果是我方行动"},{"title":"NP 理论：近似算法","path":"/wiki/algo/approx-algo.html","content":"近似算法 近似算法的目的在于：对 NP 难问题设计多项式时间算法，保证解的质量在一定比例内（近似比）"},{"title":"Boruvka 算法：特殊限制下的最小生成树算法","path":"/wiki/algo/boruvka.html","content":"Boruvka 最小生成树算法 Boruvka 在思想上算是 Kruskal 和 Prim 算法的结合，在 某个点出发的边的边权可以放在一起考虑 边权具有特殊性质 时具有较大优势。 算法流程 和 Kruskal 一样，我们维护若干个连通块。我们定义，连通块 cic_ici​ 的最小边表示这个连通块和其他连通块之间的边里最小的边，即 min⁡w(e){e:e=(u,v);u∈ci∧v∈cj∧i≠j}\\min_{w(e)}\\{ e:e=(u,v);u\\in c_i\\land v\\in c_j\\land i e j \\}minw(e)​{e:e=(u,v);u∈ci​∧v∈cj​∧i=j} 初始时，每一个点 vvv 都分配一个连通块 cvc_vcv​。 计算每一个点属于哪一个连通块，把这个连通块的最小边设为 None 遍历每一条边 e=(u,v)e=(u,v)e=(u,v)，如果 u,vu,vu,v 不在同一个连通块内，用 w(e)w(e)w(e) 更新这两个连通块 cu,cvc_u,c_vcu​,cv​ 的最小边 如果所有连通块的最小边都是 None，则结束；否则，将每个连通块的最小边加入答案，继续从 111 循环"},{"title":"无/有源汇 上下界 可行/最大/最小流","path":"/wiki/algo/bounded-flow.html","content":"无源汇 上下界可行流 每条边都存在下界 b(u,v),c(u,v)b(u,v),c(u,v)b(u,v),c(u,v) 分别表示这条边的流量至少、至多为多少。 我们先直接假设每条边已经流了 b(u,v)b(u,v)b(u,v) 的流量，设为初始流量 然后构造新图 HHH，其中的每一条边 eH(u,v)e_H(u,v)eH​(u,v) 满足其容量为 c(u,v)−b(u,v)c(u,v)-b(u,v)c(u,v)−b(u,v) 然后对 HHH 中的节点 iii 进行调整，假设 HHH 中两个额外的点 S,TS,TS,T 分别作为 HHH 中的源汇点 如果初始流量中 iii 的收支平衡，则不用添加边 如果 iii 的入流多于出流，差值为 ddd，则 SSS 向 iii 连边，容量为 ddd 如果 iii 的出流多于入流，差值为 ddd，则 iii 向 TTT 连边，容量为 ddd 然后以 SSS 为源点，TTT 为汇点跑最大流。 如果 SSS 出发的边都满流，则存在可行流；否则不存在 正确性证明 有源汇 上下界可行流 设源点为 SSS，汇点为 TTT，则我们连 T→ST\\to ST→S 的边，其下界为 000，上界为 ∞\\infin∞。于是问题转化为无源汇上下界可行流。 此时若有解，S→TS\\to TS→T 的可行流的流量就等于 T→ST\\to ST→S 的附加边的流量。 上下界最大流 上下界最小流"},{"title":"排列组合（一）","path":"/wiki/algo/combination.html","content":"排列组合加速技巧 C(n, a) P(n, a)，其中 a 较小时 可以在 O(a)O(a)O(a) 的时间内计算单个值。 mint res;for (int i = 1; i = res; i++)"},{"title":"算法设计：计数模型","path":"/wiki/algo/counting-techniques.html","content":"计数模型 计数模型 算法竞赛里的计数问题通常有几种类别： 给定区间，求这个区间里合法元素的个数"},{"title":"斜率优化 DP","path":"/wiki/algo/dp-convex-opt.html","content":"斜率优化 DP 斜率优化 DP 有个很套路化的形式： 这样的形式：dp[i]=max⁡j:conditiondp[j]+f(i)×g(j)dp[i] = \\max_{j:\\text{condition}} dp[j] + f(i)\\times g(j)dp[i]=j:conditionmax​dp[j]+f(i)×g(j)求 max 的话，通常是维护点集的上凸壳 或者……dp[i]=min⁡j:conditiondp[j]+f(i)×g(j)dp[i] = \\min_{j:\\text{condition}} dp[j] + f(i)\\times g(j)dp[i]=j:conditionmin​dp[j]+f(i)×g(j)求 min 的话，通常是维护点集的下凸壳 例题 Luogu P5504 首先我们需要想到的一点是：如果取的是 [l,r][l,r][l,r] 的贝壳，那么选择的 s0=sizel=sizers_0=size_l=size_rs0​=sizel​=sizer​ . 如果不这样做，考虑选取的是 [l,r][l,r][l,r] 而 lkr,sizek=sizerl\\lt k\\lt r, size_k=size_rlkr,sizek​=sizer​，那么，我们完全可以把这一段拆成 [l,k−1],[k,r][l,k-1],[k,r][l,k−1],[k,r] 两段，答案肯定更优。 然后，我们考虑 dp[]dp[]dp[] 式子，令 dp[]dp[]dp[] 就是我们想要的最大柠檬数，把它想象成一个总是从前缀转移的 dp，可以列出 c=sizeidpi=max⁡j≤i∧sizej=c(dpj−1+c×(sumc,i−sumc,j+1)2) c=size_i\\\\ dp_i=\\max_{j\\le i\\land size_j=c}\\Bigg( dp_{j-1} + c\\times (sum_{c,i}-sum_{c,j}+1)^2 \\Bigg) c=sizei​dpi​=j≤i∧sizej​=cmax​(dpj−1​+c×(sumc,i​−sumc,j​+1)2) 其中 sumc,isum_{c,i}sumc,i​ 表示 1∼i1\\sim i1∼i 里 size 为 ccc 的贝壳数量。 我们把式子展开成斜率优化的形式，首先拿掉 max⁡\\maxmax： dpi=dpj−1+c×(sumc,i2+(sumc,j−1)2−2⋅sumc,i⋅(sumc,j−1)) dp_i=dp_{j-1}+c\\times \\Big( sum_{c,i}^2+(sum_{c,j}-1)^2-2\\cdot sum_{c,i}\\cdot (sum_{c,j}-1) \\Big) dpi​=dpj−1​+c×(sumc,i2​+(sumc,j​−1)2−2⋅sumc,i​⋅(sumc,j​−1))由于 sizej=sizeisize_j=size_isizej​=sizei​，我们可以适当的替换 ccc 为 sizejsize_jsizej​，然后写成 b(i)=y(j)−k(i)⋅x(j)b(i)=y(j)-k(i)\\cdot x(j)b(i)=y(j)−k(i)⋅x(j) 的形式 dpi−c⋅sumc,i2⏟b(i)=dpj−1+c⋅(sumc,j−1)2⏟y(j)−(2⋅c⋅sumc,i)⏟k(i)⋅(sumc,j−1)⏟x(j) \\underbrace{dp_i-c\\cdot sum_{c,i}^2}_{b(i)}=\\underbrace{dp_{j-1}+c\\cdot (sum_{c,j}-1)^2}_{y(j)} - \\underbrace{(2\\cdot c\\cdot sum_{c,i})}_{k(i)}\\cdot\\underbrace{\\Big( sum_{c,j}-1 \\Big)}_{x(j)} b(i)dpi​−c⋅sumc,i2​​​=y(j)dpj−1​+c⋅(sumc,j​−1)2​​−k(i)(2⋅c⋅sumc,i​)​​⋅x(j)(sumc,j​−1)​​因为我们的 dpidp_idpi​ 要求的是 max⁡\\maxmax，而 sumc,j−1sum_{c,j}-1sumc,j​−1 是递增的，所以，我们需要维护的是上凸壳，插入的点的坐标是 (sumc,j−1,dpj−1+c⋅(sumc,j−1)2)\\Big( sum_{c,j}-1, dp_{j-1}+c\\cdot (sum_{c,j}-1)^2 \\Big)(sumc,j​−1,dpj−1​+c⋅(sumc,j​−1)2). Code 要注意的代码细节： 由于 jjj 其实可以选择 iii 自己（即只取长度为 111 的区间），所以我们需要先把 dpi−1dp_{i-1}dpi−1​ 塞进凸壳里，再在凸壳上二分斜率判断（代码 49 行） 二分判断斜率的时候，要注意越界问题（即如果 mid=r−1mid=r-1mid=r−1，则此时 mid+1mid+1mid+1 会造成数组越界）. 我这里的话，是在 convex hull 的末尾额外添加了一个点确保不会越界（绿色高亮和红色高亮），我这里的二分是 l + 1 r 的写法。 且 67 行的 r-- 也是为了避免越界。 此外如果用 mid 与 mid+1 的点之间的斜率，最后的转移点要取 mid+1mid+1mid+1，因为 mid ~ mid+1 的斜率 ≥k\\ge k≥k 而 mid+1 ~ mid+2 之间的斜率 k\\lt kk. JSOI2011 柠檬"},{"title":"动态规划例题 (1)","path":"/wiki/algo/dp-design.html","content":"动态规划例题 不能很清晰地分类……就全丢到这里了…… Codeforces 2107 F1 TP LinkSubmission我们考虑 apa_pap​，我们可以这样做：花费 i−pi-pi−p 把 apa_pap​ 换到自己前面花费 apa_pap​ 超车再花费 111 把 apa_pap​ 换到自己面前再花费 apa_pap​ 超车……于是我们可以利用 apa_pap​ 超车一个区间 ([l,r],l≤p≤r[l,r],l\\le p\\le r[l,r],l≤p≤r) 的车手，花费为（假设当前正在 rrr 的身后）(r−l+1)⋅apovertake+r−lmove ap from back to front+r−pmove ap from its original place to front\\begin{array}{rlll}(r-l+1)\\cdot a_p \\text{overtake}\\\\+r-l \\text{move \\(a_p\\) from back to front}\\\\+r-p \\text{move \\(a_p\\) from its original place to front}\\end{array}(r−l+1)⋅ap​+r−l+r−p​overtakemove ap​ from back to frontmove ap​ from its original place to front​当我们从 rrr 向 lll 扫描的时候，此时我们枚举的 ppp 递减，因此 r−pr-pr−p 递增，要使这个式子最小，apa_pap​ 必须为 [l,r][l,r][l,r] 内最小值（且最靠右），才有可能最小化这个式子。解决完这个区间之后，我们发现，整个 [1,n][1,n][1,n] 可以切分成多个这样的子结构（每个小区间里选出自己的 apa_pap​）。差不多就类似于走进另一个区间发现在另一个区间用另一个数当 apa_pap​ 更划算。现在我们来定义状态。设 dp[i]dp[i]dp[i] 表示从第 nnn 个人后面走到第 iii 个人后面、第 i+1i+1i+1 个人前面时所需要的最小费用。那么 dp[0]dp[0]dp[0] 就是我们需要的答案，初始化 dp[n]=0dp[n]=0dp[n]=0.随后，我们从后向前枚举 iii（现在在第 iii 个人后面），并且枚举 jjj，表明我们要从 iii 走到 jjj 位置，即 dp[i]→dp[j]dp[i]\\to dp[j]dp[i]→dp[j].根据上文的分析，我们需要知道 a[j…i]a[j\\dots i]a[j…i] 的最小值 a[p]a[p]a[p]（有多个的话取最右的，这个可以在枚举 jjj 时一起维护）dp[j−1]←min⁡dp[i]+a[p]×(i−j+1)⏟overtake+i−p⏟1st time moved to front+i−j⏟swap to front(j≤p≤i)dp[j-1]\\underset{\\min}{\\gets} dp[i]+\\underbrace{a[p]\\times(i-j+1)}_{\\text{overtake}}+\\underbrace{i-p}_{\\text{1st time moved to front}}+\\underbrace{i-j}_{\\text{swap to front}} (j\\le p\\le i)dp[j−1]min←​dp[i]+overtakea[p]×(i−j+1)​​+1st time moved to fronti−p​​+swap to fronti−j​​(j≤p≤i)因为 a[p]a[p]a[p] 考虑了 a[j]a[j]a[j]，所以 jjj 位置也会被超车，因此更新的是 dp[j−1]dp[j-1]dp[j−1]"},{"title":"网络流：EK 算法","path":"/wiki/algo/edmonds-karp.html","content":"EK 算法 EK 算法在 Residual Graph 里找增广路的时候，使用 BFS 算法求解出的增广路一定是 shortest (in terms of fewest edges). 算法证明 Distance Lemma 令 EF 算法第 iii 次在 GfG_fGf​ 上找到的增广路为 fif_ifi​，并且这些 fif_ifi​ 是最短的（经过最少的边）。记 GiG_{i}Gi​ 为 fif_ifi​ 对应的 residual graph，di(u,v)d_i(u,v)di​(u,v) 为 GiG_{i}Gi​ 上两点之间的最短距离（最小边数），那么有 di+1(s,v)≥di(s,v) \\boxed{d_{i+1}(s,v)\\ge d_i(s,v)} di+1​(s,v)≥di​(s,v)​ 证明 令 s,vs,vs,v 最短路径上的点按 distance 递增排列。我们用数学归纳法证明。 当 di+1(s,v)=0d_{i+1}(s,v)=0di+1​(s,v)=0 时，说明 s=vs=vs=v 两者是同一个点，因此在 fif_ifi​ 对应的图里，di(s,v)=0≤di+1(s,v)d_i(s,v)=0\\le d_{i+1}(s,v)di​(s,v)=0≤di+1​(s,v). 考虑任意长度 L0L\\gt 0L0，假设引理对 ∀v,di+1(s,v)L\\forall v, d_{i+1}(s,v)\\lt L∀v,di+1​(s,v)L 成立，考察 ∀v,di+1(s,v)=L\\forall v,d_{i+1}(s,v)=L∀v,di+1​(s,v)=L： 在 residual graph Gi+1G_{i+1}Gi+1​ 上找到 s,vs,vs,v 的最短路，令 xxx 为 vvv 的上一个节点，那么有 di+1(s,x)=L−1d_{i+1}(s,x)=L-1di+1​(s,x)=L−1 所以根据假设 di(s,x)≤L−1d_{i}(s,x)\\le L-1di​(s,x)≤L−1，我们再根据这个信息，推理 di(s,v)d_i(s,v)di​(s,v)。在 GiG_iGi​ 上有两种情况 GiG_iGi​ 中存在 (x,v)(x,v)(x,v) 这条边 这种情况下，我们只需要走 s→x→vs\\to x\\to vs→x→v，就一定可以保证 di(s,v)≤Ld_i(s,v)\\le Ldi​(s,v)≤L，所以 di(s,v)≤di+1(s,v)d_i(s,v)\\le d_{i+1}(s,v)di​(s,v)≤di+1​(s,v) GiG_iGi​ 中不存在 (x,v)(x,v)(x,v) 这条边 如果 GiG_iGi​ 不存在这条边，但 Gi+1G_{i+1}Gi+1​ 存在这条边，这就说明 (v,x)∈Gi(v,x)\\in G_i(v,x)∈Gi​ EK 算法找到了一条增广路，s→v→x→ts\\to v\\to x\\to ts→v→x→t 此时，由于 EF 算法找到的总是最短路，而 di(s,x)≤L−1d_i(s,x)\\le L-1di​(s,x)≤L−1 且经过 (v,x)(v,x)(v,x)，因此我们可以推导出 di(s,v)=di(s,x)−1≤L−2 d_i(s,v)=d_i(s,x)-1\\le L-2 di​(s,v)=di​(s,x)−1≤L−2所以 di(s,v)≤di+1(s,v)d_i(s,v)\\le d_{i+1}(s,v)di​(s,v)≤di+1​(s,v) 便自动成立了 Critical Edge Lemma"},{"title":"网络流：Ford-Fulkerson 增广路算法","path":"/wiki/algo/ford-fulkerson.html","content":"Ford Fulkerson 算法 对于边 (u,v)(u,v)(u,v)，我们定义 Residual Capacity cf(u,v)=c(u,v)−f(u,v)c_f(u,v)=c(u,v)-f(u,v)cf​(u,v)=c(u,v)−f(u,v)。把所有剩余容量 0\\gt 00 的边构成的子图定义为 Residual Graph GfG_fGf​。 Residual Graph 能够 work 的核心在于对 Backward Edge 的理解。Forward Edge 上的增广和 Backward Edge 上的增广可以抵消。 退流 Ford-Fulkerson 增广算法的时间复杂度是 O(nmf)O(nmf)O(nmf) 的，受容量最大的边的影响。 正确性证明"},{"title":"图论、网络流：最小割树 (Gomory-Hu Tree)","path":"/wiki/algo/gomory-hu-tree.html","content":"Gomory-Hu Tree 以下分析约定： 符号 含义 valSval_SvalS​ 令 SSS 是边集，那么 c(S)c(S)c(S) 就是其边权和 cutu,v={U,V−U}cut_{u,v}=\\{U,V-U\\}cutu,v​={U,V−U} 分割 u,vu,vu,v 的最小割，其中 u∈U,v∈V−Uu\\in U, v\\in V-Uu∈U,v∈V−U edgeUedge_UedgeU​ 对于一个割 U,V−UU,V-UU,V−U，其为割下的所有边，即 {(u,v):u∈U,v∈V−U}\\{(u,v):u\\in U,v\\in V-U\\}{(u,v):u∈U,v∈V−U} mincutu,vmincut_{u,v}mincutu,v​ (u,v)(u,v)(u,v) 的最小割（权值最小） minvalu,vminval_{u,v}minvalu,v​ =val(mincut(u,v))=val(mincut(u,v))=val(mincut(u,v)) 最小割树 T=(V,ET)T=(V,E_T)T=(V,ET​) 是这样一种树，对于所有的边 (s,t)∈ET(s,t)\\in E_T(s,t)∈ET​，从树上去掉这两条边之后剩下的两个连通块 S,TS,TS,T，恰好就是 s,ts,ts,t 的最小割 mincuts,tmincut_{s,t}mincuts,t​ 代码实现 Code // pass 例题"},{"title":"贪心算法","path":"/wiki/algo/greedy.html","content":"如何证明贪心算法的正确性？ 证明最佳的 solution 可以通过贪心算法在不增加 cost 的情况下求出来 证明每一步选择时，贪心算法的答案都不劣于其他算法，或者都是最优的 贪心模型 任务规划"},{"title":"区间 DP","path":"/wiki/algo/interval-dp.html","content":"区间 DP 常见建模技巧 将区间左右端点显式表现在 dp 状态定义里 例如说 dp[l][r]dp[l][r]dp[l][r] 表示区间 [l,r][l,r][l,r] 内的某某状态，且通常需要小区间拼接成大区间。为了保证正确性，这种转移通常先枚举区间长度，然后再枚举左右端点。 dp[l,r]←F( [l,a1],[a1,a2],…[am,r] ),l≤a1≤a2⋯≤am≤r dp[l,r]\\gets F\\Big(\\ [l,a_1],[a_1,a_2],\\dots[a_m,r] \\ \\Big),l\\le a_1\\le a_2\\dots\\le a_m\\le r dp[l,r]←F( [l,a1​],[a1​,a2​],…[am​,r] ),l≤a1​≤a2​⋯≤am​≤r有的时候，也会省略掉一个端点，只保留 lll (or rrr)。这种形式的 DP 通常是区间的特例，即 dp[i]dp[i]dp[i] 代表 [i,n][i,n][i,n] 后缀或者 [1,i][1,i][1,i] 前缀。状态转移差不多就是 dp[i]←dp[j]+f( [i,j) ) dp[i]\\gets dp[j]+f\\Big(\\ [i,j)\\ \\Big) dp[i]←dp[j]+f( [i,j) )"},{"title":"Manacher","path":"/wiki/algo/manacher.html","content":"Manacher 算法 Manacher 推广 如果我们的 pattern 拥有如下的一些性质，那么我们就可以利用 Manacher 进行加速： 具有类似回文的对称性。如果维护的回文 Box 为 [l,r][l,r][l,r]，那么这种对称性使得 R[i]R[i]R[i] 至少 ≥R[l+r−i]\\ge R[l+r-i]≥R[l+r−i]. 外推性：如果 s[l…r]s[l\\dots r]s[l…r] 满足性质，那么 s[l+1…r−1]s[l+1\\dots r-1]s[l+1…r−1] 也应该满足性质 例题 P3501 [POI 2010] ANT-Antisymmetry 题目所描述的 pattern 符合外推性：一个合法的 pattern 必须有 s[0]≠s[−1]s[0] e s[-1]s[0]=s[−1], 去掉之后的子串也必然满足反对称. 而且也具有对称性。于是可以使用 Manacher 算法，而且反对称的串长必为偶数，我们在数字中间 pad 字符，这样就可以向 Manacher 那样了。时间复杂度 O(n)O(n)O(n) Code #include iostream#include string#include vectorint main() int n; std::cin n; std::string s; std::cin s; std::string T = .; for (auto c : s) T += c, T += .; int L = T.length(); std::vectorint R(L + 1, 0); auto check = [](int p1, int p2) if (T[p1] == . T[p2] == .) return true; else if (T[p1] == 1 T[p2] == 0) return true; else if (T[p1] == 0 T[p2] == 1) return true; else return false; ; int C = 0, r = 0; long long ans = 0; for (int i = 0; i L; i += 2) int reflect = 2 * C - i; int k; if (i = C + r) k = std::min(R[reflect], C + r - i); else k = 0; while (i - k = 0 i + k L check(i - k, i + k)) k++; R[i] = k - 1; if (i + R[i] C + r) C = i; r = R[i]; ans += R[i] / 2; std::cout ans std::endl;"},{"title":"网络流模型","path":"/wiki/algo/network-flow-models.html","content":"二分图匹配模型"},{"title":"网络流：最大流、最小割","path":"/wiki/algo/network-flow.html","content":"Flow Network 网络流图 G=(V,E,c)G=(V,E,c)G=(V,E,c) 是一张有向图，其中每一条有向边 e=(u,v)e=(u,v)e=(u,v) 有容量 (capacity) c(u,v)≥0c(u,v)\\ge 0c(u,v)≥0. 除此之外，GGG 中还有两个特殊节点 source sss 和 sink ttt. Flow 在此基础上，我们定义流 (Flow). GGG 上的流 fff 给每一条边 e=(u,v)e=(u,v)e=(u,v) 都赋上一个实数 f(u,v)f(u,v)f(u,v) 且满足： 每一条边的流量都不会超过其容量 capacity. f(u,v)≤c(u,v)f(u,v)\\le c(u,v)f(u,v)≤c(u,v) 除了源点与汇点，其余每一点都满足：流入的流量和流出的流量相等。 ∀v∈V−{s,t},∑(x,v)∈Ef(x,v)=∑(v,y)∈Ef(v,y)\\forall v\\in V-\\{s,t\\}, \\sum_{(x,v)\\in E} f(x,v)=\\sum_{(v,y)\\in E} f(v,y)∀v∈V−{s,t},(x,v)∈E∑​f(x,v)=(v,y)∈E∑​f(v,y) 最大流求解算法 Ford-Fulkerson 算法 Cut 什么是割？ 图的一个“割” (Cut) 是指将图分成两个点集 A,BA,BA,B 且源点 s∈As\\in As∈A，汇点 t∈Bt\\in Bt∈B. 定义 Capacity of Cut cap(A,B)=∑e out of Ac(e)cap(A,B)=\\sum_{e\\text{ out of }A}c(e)cap(A,B)=∑e out of A​c(e) Flow Value Lemma 令 fff 为 GGG 上任意的流，(A,B)(A,B)(A,B) 为 GGG 上任意的割（s∈A,t∈Bs\\in A,t\\in Bs∈A,t∈B），则必有 value(f)=∑e out of Af(e)−∑e into Af(e) \\text{value}(f)=\\sum_{e \\text{ out of }A}f(e)-\\sum_{e\\text{ into }A} f(e) value(f)=e out of A∑​f(e)−e into A∑​f(e) 证明 考虑 AAA 中的每一个节点，除了源点 sss 以外，其余所有点都满足 outflow(v)=inflow(v)\\text{outflow}(v)=\\text{inflow}(v)outflow(v)=inflow(v)，而 sss 的 inflow(s)=0\\text{inflow}(s)=0inflow(s)=0，所以 value(f)=∑outflow(s)=∑v∈Aoutflow(v)−inflow(v) \\mathrm{value}(f)=\\sum \\mathrm{outflow}(s)=\\sum_{v\\in A} \\mathrm{outflow}(v)-\\mathrm{inflow}(v) value(f)=∑outflow(s)=v∈A∑​outflow(v)−inflow(v)我们再来考察 ∑v∈Aoutflow(v)\\sum_{v\\in A}\\mathrm{outflow}(v)∑v∈A​outflow(v)，检查所有边，可得 ∑v∈Aoutflow(v)=∑e inside Af(e)+∑e out of Af(e) \\sum_{v\\in A}\\mathrm{outflow}(v)=\\sum_{e\\text{ inside }A}f(e)+\\sum_{e\\text{ out of }A}f(e) v∈A∑​outflow(v)=e inside A∑​f(e)+e out of A∑​f(e)同理对 inflow 有 ∑v∈Ainflow(v)=∑e inside Af(e)+∑e into Af(e) \\sum_{v\\in A}\\mathrm{inflow}(v)=\\sum_{e\\text{ inside }A}f(e)+\\sum_{e\\text{ into }A}f(e) v∈A∑​inflow(v)=e inside A∑​f(e)+e into A∑​f(e)两式相减可得 value(f)=∑e out of Af(e)−∑e into Af(e) \\mathrm{value}(f)=\\sum_{e\\text{ out of }A}f(e)-\\sum_{e\\text{ into }A}f(e) value(f)=e out of A∑​f(e)−e into A∑​f(e) 由此也可以推得几个推论 Corollary 1 value(f)≤cap(A,B) \\mathrm{value}(f)\\le \\mathrm{cap}(A,B) value(f)≤cap(A,B) 证明 value(f)=∑e out of Af(e)−∑e into Af(e)≤∑e out of Af(e)≤∑e out of Acap(e)=cap(A,B) \\begin{aligned} \\mathrm{value}(f)=\\sum_{e\\text{ out of }A} f(e)-\\sum_{e\\text{ into }A} f(e)\\\\ \\le \\sum_{e\\text{ out of }A} f(e)\\\\ \\le\\sum_{e\\text{ out of }A} \\mathrm{cap}(e)\\\\ =\\mathrm{cap}(A,B) \\end{aligned} value(f)​=e out of A∑​f(e)−e into A∑​f(e)≤e out of A∑​f(e)≤e out of A∑​cap(e)=cap(A,B)​ Theorem 3 令 fff 为图上的流且使得 GfG_fGf​ 不存在增广路，那么存在一种 cut (A,B)(A,B)(A,B) 使得 value(f)=cap(A,B)\\mathrm{value}(f)=cap(A,B)value(f)=cap(A,B). 证明 既然 GfG_fGf​ 上已经不存在增广路，那么 GfG_fGf​ 天然的可以被划分为两个集合 A,BA,BA,B，其中 A={ v:s→v },B=V−AA=\\set{v:s\\to v},B=V-AA={v:s→v},B=V−A 这也就是说，原图 GGG 中，AAA 到 BBB 的有向边的 residual capacity 均为 000，根据 Flow Value Lemma，cut(A,B)\\mathrm{cut}(A,B)cut(A,B) 就是一种符合条件的割。 最大流最小割定理 Max Flow=Min Cut \\text{Max Flow}=\\text{Min Cut} Max Flow=Min Cut"},{"title":"【博弈论】Nim 游戏","path":"/wiki/algo/nim-game.html","content":"有向图游戏（博弈图） 博弈图是一张有向图，每一个节点表示游戏的状态（例如每个堆里石子的个数），有向边表示行动（取石子导致了状态的变化）。根据定义，博弈图是有向无环图 Nim Game Graph Nim 定理"},{"title":"NP 理论初探","path":"/wiki/algo/np-complete.html","content":"P 与 NP P: 存在多项式算法，可以解出一个 solution NP: 存在多项式算法，给定 solution 可以判定输入是否合法 NP-Complete: 所有 NP 问题都可以归约到 AAA，且 AAA 是 NP 问题，则 AAA 是 NP-Complete 问题 NP-Hard Polynomial-Time Reduce B≤pAB \\le_p AB≤p​A 或者 B→AB\\to AB→A，表示存在一个多项式算法 fff： 将 BBB 问题的输入 xxx 转化为 AAA 问题的输入 f(x)f(x)f(x)； xxx 是 BBB 问题的一组合法解，当且仅当 f(x)f(x)f(x) 是 AAA 问题的一组合法解。 NP 问题判定定理 111 若 B→AB\\to AB→A，且 AAA 是 P 问题，则 BBB 也是 P 问题。 【证明】 因为 AAA 问题内多项式时间 O(f(x))O(f(x))O(f(x)) 内可解，而又存在多项式算法 O(g(x))O(g(x))O(g(x)) 可以转化输入，则可以在 O(f(x)+g(x))O(f(x)+g(x))O(f(x)+g(x)) 的时间内解决 BBB. □\\square□ NP-Hard 优化问题 (Optimization Problem) NP Problem 是在寻找可行解，而其对应的优化问题则是在找最优解。 如果某个优化问题对应的判定问题是 NP Complete 的，则这个优化问题为 NP Hard 的。 考虑判定问题 XXX 和对应的优化问题 YYY，如果 XXX 是 NP Complete 问题 我们可以在多项式时间内解决 YYY 则我们可以在多项式时间内解决 XXX，从而 P=NPP=NPP=NP，那么 YYY 就是 NP-Hard 的。"},{"title":"经典 NP-Complete 问题与证明","path":"/wiki/algo/npc-problems.html","content":"SAT 形式：AND of OR clauses, Conjunctive Normal Form. 一个 SAT 是多个 clause 取 AND 的结果，每一个 clause 是多个变量取 OR 的结果。 (x1∨¬x2)∧(¬x1∨x3∨x4)∧(x2∨¬x3) (x_1\\lor \\lnot x_2)\\land (\\lnot x_1\\lor x_3\\lor x_4)\\land (x_2\\lor \\lnot x_3) (x1​∨¬x2​)∧(¬x1​∨x3​∨x4​)∧(x2​∨¬x3​) from SAT to 3SAT 3SAT: 在 SAT 的基础上，满足每一个 clause 只包含三个变量。 Clique 团 Clique Problem 是指: 给定一张图 $G=(V,E)$ 和一个整数 $k$，检查是否存在 $|V'|=k$ 的 clique. 证明 首先，clique problem 是 NP 问题。这个比较好证明。 其次，我们尝试把 3SAT 归约到 Clique. 对于每一个 clause ci=x1∨x2∨x3c_i=x_1\\lor x_2\\lor x_3ci​=x1​∨x2​∨x3​，建立三个节点 ni,1,ni,2,ni,3n_{i,1},n_{i,2},n_{i,3}ni,1​,ni,2​,ni,3​. 对于图中不属于同一个 clause 的两个节点 xi,xjx_i, x_jxi​,xj​，只要 xi≠¬xjx_i e \\lnot x_jxi​=¬xj​，就建立一条边。 我们令 k=mk=mk=m，检查图里是否存在大小为 kkk 的 clique. Subset Sum 给定 nnn 个元素 {a1,a2,…an}\\{ a_1,a_2,\\dots a_n \\}{a1​,a2​,…an​} 和一个数 TTT，是否存在能从这 nnn 个元素里找到一个子集，使得其和为 TTT？ 证明 我们证明 3SAT∝pSubset Sum\\text{3SAT} \\propto_p \\text{Subset Sum}3SAT∝p​Subset Sum 考虑 3SAT 有 nnn 个变量和 mmm 个 clause，对于每一个 3SAT 变量 xix_ixi​，在 subset sum 里创建两个变量 ti,fit_i,f_iti​,fi​ 我们令 set 里的变量均有 n+mn+mn+m 位，前 nnn 位记为变量部分，后 mmm 位记为子句部分。 tit_iti​ 变量部分第 iii 位设置为 111，子句部分若 clausej\\text{clause}_jclausej​ 包含 xix_ixi​ 则子句部分的第 jjj 位设置为 111 fif_ifi​ 变量部分第 iii 位设置为 111，子句部分若 clausej\\text{clause}_jclausej​ 包含 ¬xi\\lnot x_i¬xi​ 则子句部分的第 jjj 位设置为 111 同时，对每一个子句 clausei\\text{clause}_iclausei​，创建变量 cic_ici​，其变量部分为全 000，子句部分仅第 iii 位为 111. 考虑如何构造 TTT，令 TTT 的变量部分的每一位都是 111，子句部分的每一位都是 333. 3SAT ⟹ \\implies⟹ Subset Sum 如果 3SAT 存在一种合法的方案 SSS，对于 xi∈Sx_i\\in Sxi​∈S，如果 xi=truex_i=\\texttt{true}xi​=true，则往 subset 里添加 tit_iti​，否则添加 fif_ifi​. 又因为对每一个子句而言，至少有一个 xix_ixi​ 或者 ¬xi\\lnot x_i¬xi​ 为真，因此最多选两个 cjc_jcj​ 一定可以让 TTT 子句部分的第 jjj 位为 333. 3SAT ⟸ \\impliedby⟸ Subset Sum 考虑某个子集的元素： 如果包含 tit_iti​，则令 xi=truex_i=\\texttt{true}xi​=true 如果包含 fif_ifi​，则令 xi=falsex_i=\\texttt{false}xi​=false 因此对于每一个 clausej\\text{clause}_jclausej​ 而言，因为 cjc_jcj​ 最多被选 222 次，而 TTT 对应位置为 333，所有至少有一个 xi∈clausej=truex_i\\in \\text{clause}_j=\\texttt{true}xi​∈clausej​=true，即 clause 为真，从而 3SAT 满足。 Equal Sum Partition 给定数集 {a1,a2,…,an}\\{a_1,a_2,\\dots,a_n\\}{a1​,a2​,…,an​}，能否选出子集，使得子集的和恰好为总和的一半？ 证明 我们证明 Subset Sum∝pEqual Sum Partition\\text{Subset Sum}\\propto_p \\text{Equal Sum Partition}Subset Sum∝p​Equal Sum Partition 令 s=∑ais=\\sum a_is=∑ai​， 如果 t=s2t=\\frac s2t=2s​，那么问题不变 如果 ts2t\\gt \\frac s2t2s​，那么添加一个数 an+1=2t−sa_{n+1}=2t-san+1​=2t−s 否则 ts2t\\lt \\frac s2t2s​，那么添加一个数 an+1=s−2ta_{n+1}=s-2tan+1​=s−2t General Knapsack 背包问题 存在 nnn 个物品，每一个都有重量 wiw_iwi​ 和价值 viv_ivi​。给出重量限制 WWW 和价值目标 VVV，能否选出一些物品 SSS，使得 ∑i∈Swi≤W\\sum_{i\\in S} w_i\\le W∑i∈S​wi​≤W 并且 ∑i∈Svi≥V\\sum_{i\\in S} v_i\\ge V∑i∈S​vi​≥V. 证明 思路：我们证明 Subset Sum∝pKnapsack\\text{Subset Sum}\\propto_p \\text{Knapsack}Subset Sum∝p​Knapsack 【归约】令物品 wi=vi=aiw_i=v_i=a_iwi​=vi​=ai​，且 W=V=TW=V=TW=V=T. Hamiltonian Path 哈密顿路径 有向图下的哈密顿路径 证明 我们证明 3SAT∝pH-Cycle\\text{3SAT}\\propto_p \\text{H-Cycle}3SAT∝p​H-Cycle 【归约】 令 3SAT 有 nnn 个变量和 mmm 个子句。对于一个变量 xix_ixi​： 让其对应一行节点 rowirow_irowi​，包含 3m+33m+33m+3 个节点，这一行的相邻节点之间连双向边 在这一行节点中，从第二个节点开始，每三个节点代表一个 clause cjc_jcj​，颜色顺序为黄、绿、绿 额外有一个黄色节点，分别指向这一行的开头结尾两个节点 规定 xi=truex_i=\\texttt{true}xi​=true 则往左走，否则往右走 一行的开头结尾两个节点又分别指向 xi+1x_{i+1}xi+1​ 的额外黄色节点 对应 xix_ixi​ 对于每一个子句 cjc_jcj​，找到其所包含的三个变量的行 rowirow_irowi​ 中，对应 cjc_jcj​ 的两个绿色节点 rowi,1,rowi,2row_{i,1},row_{i,2}rowi,1​,rowi,2​，分别连不同方向的有向边。 如果是 xix_ixi​，则连边方向为：靠左的绿色节点 对应 cjc_jcj​ 3SAT ⟹ \\implies⟹ H-Path 如果一个合法的 3SAT 方案存在 最长路径 旅行商问题"},{"title":"有约束的数字满足性问题","path":"/wiki/algo/number-constraint-model.html","content":"题面描述 这类题型有几个比较明显的特征： 通常询问的是 ≤n\\le n≤n 内满足某个条件的数有多少个 通常数据范围很大，如 n≤1018n\\le 10^{18}n≤1018 解法一：素因子分解法 这一种方法的视角下，条件通常可以在素因子分解后，转化成指数上的约束，大大减少了可能的数得到范围。 【例】Number Reduction 考虑一个数 xxx，如果 xxx 的十进制表示中含有 kkk (2≤k≤92\\le k\\le 92≤k≤9) 且 k∣xk|xk∣x，则将 x←xkx\\gets \\frac{x}{k}x←kx​. 如果 xxx 可以经过以上任意次操作变成 111，则 xxx 是 Good 的。给定一个整数 n≤1018n\\le 10^{18}n≤1018，问有多少个数 ≤n\\le n≤n 是 Good 的？ 根据题面，显然我们每次只能除以 2∼92\\sim 92∼9，既然最后可以变成 111，这就说明 xxx 只能含有因子 2,3,5,72,3,5,72,3,5,7，即 x=2a3b5c7d x=2^a3^b5^c7^d x=2a3b5c7d那么 ≤1018\\le 10^{18}≤1018 有多少数满足这样的形式呢？答案是 log⁡2(1e18)×log⁡3(1e18)×log⁡5(1e18)×log⁡7(1e18)≈1.2×106\\log_2(1e18)\\times\\log_3(1e18)\\times\\log_5(1e18)\\times\\log_7(1e18)\\approx 1.2\\times 10^6log2​(1e18)×log3​(1e18)×log5​(1e18)×log7​(1e18)≈1.2×106. 可以看到量级最大大概只有 10610^6106. 确定了数不多后，我们考虑把所有这样的数都提取出来，然后检查数位和整除的条件. 既然最终可以变成 111，这样一个变化的过程即 a0=1→×m0a1→×m1a2→×m2…→×mkak+1 a_0=1\\underset{\\times m_0}{\\to}a_1\\underset{\\times m_1}{\\to}a_2\\underset{\\times m_2}{\\to}\\dots\\underset{\\times m_k}{\\to}a_{k+1} a0​=1×m0​→​a1​×m1​→​a2​×m2​→​…×mk​→​ak+1​这里 mi∈[2,9]m_i\\in [2,9]mi​∈[2,9]，所以其实相当于在有向无环图，以 111 为起点能到达的所有点。我们只需要对每一个 x=2a3b5c7dx=2^a3^b5^c7^dx=2a3b5c7d，枚举数位并判断整除，然后在图上添加有向边，最后从 111 开始跑 DFS 即可。 【时间复杂度】枚举 2a3b5c7d2^a3^b5^c7^d2a3b5c7d 需要 O(log⁡4n)O(\\log^4 n)O(log4n) 的时间，再算上枚举数字和其数位，需要 O(log⁡5n)O(\\log^5 n)O(log5n). Code #include algorithm#include cmath#include iostream#include unordered_map#include vectorusing i64 = long long;using vi = std::vectorint;constexpr int N = 1e7 + 5;vi G[N];i64 n;std::vectori64 v;std::unordered_mapi64, int mp;int mark[10];i64 get_num(i64 a, i64 b, i64 c, i64 d) return std::pow(2ll, a) * std::pow(3ll, b) * std::pow(5ll, c) * std::pow(7ll, d);int main() std::cin n; for (i64 a = 1; a = n; a *= 2) for (i64 b = 1; b = n; b *= 3) for (i64 c = 1; c = n; c *= 5) for (i64 d = 1; d = n; d *= 7) i64 x = a * b * c * d; if (x = n) v.push_back(x); for (int i = 0; i v.size(); i++) mp[v[i]] = i; for (int i = 0; i 10; i++) mark[i] = -1; for (int i = 0; i v.size(); i++) i64 x = v[i]; while (x 0) mark[x % 10] = i; x /= 10; for (int j = 2; j = 9; j++) if (mark[j] != i || v[i] % j != 0 || !mp.count(v[i] / j)) continue; G[mp[v[i] / j]].push_back(mp[v[i]]); int ans = 0; vi vis(v.size(), false); auto dfs = [](auto F, int u) - void vis[u] = true; ans++; for (int v : G[u]) if (!vis[v]) F(F, v); ; dfs(dfs, mp[1]); std::cout ans std::endl; 解法二：数位 DP 算是数数字的正统方法。记忆化搜索的写法比迭代的写法更简单易懂。"},{"title":"回文树","path":"/wiki/algo/palindromic-tree.html","content":"我们希望能够快速找到回文子串，回顾 Manacher 算法的思路，我们发现 Manacher 的基本思路就是从 S1,S2,…,Si−1S_1,S_2,\\dots,S_{i-1}S1​,S2​,…,Si−1​ 递推到 SiS_iSi​. 但是相比 Manacher 只能按回文中心查找回文串，我们能否按末尾字符来查找回文串呢？ 那么假设 S1∼i−1S_{1\\sim i-1}S1∼i−1​ 的答案已经计算好了，我们该如何为 SiS_iSi​ 计算答案（即找出以 SiS_iSi​ 为结尾的回文串）呢？ Key Observations 如果 SiS_iSi​ 可以产生新的回文子串，那么一定会包含 S1∼i−1S_{1\\sim i-1}S1∼i−1​ 的后缀 例如，在 fabcb 后面插入 a，是与 fabcb 的后缀 abcb 一起构成回文子串 因为需要产生的子串也得是回文的，所以，找到的那个后缀也应该有一个相同的字符来和 SiS_iSi​ 匹配 例如，在 fabcb 后面插入 a，fabcb 的后缀 abcb 中的 a 就是 SiS_iSi​ 对应匹配的相同字符 在这两个相同字母之间，应该也是回文串。 这个很好理解，回文串去掉头尾各一个字符，中间部分肯定也是回文的 这启发我们以另一个视角来看待回文串：回文串 P′PP′ 是回文串 PPP 的首尾各添加一个字符 XXX 得到的！即 P′=X+P+XP=X+P+XP′=X+P+X. 这也很符合我们想要递推的做法：令 PPP 是某个以 Si−1S_{i-1}Si−1​ 为结尾的回文串，令 X=SiX=S_iX=Si​，那么 P′=Si+P+SiP=S_i+P+S_iP′=Si​+P+Si​ 就是以 SiS_iSi​ 字符为结尾的最长回文串了。 但注意，这里仍然只是最长回文串，如果我们想求出次长、次次长呢？一样的，我们只需要改变 PPP 就可以，让 PPP 长度变得更短，但仍然满足是以 Si−1S_{i-1}Si−1​ 作结的回文串。 代码实现 接下来我们用数据结构实现我们的 PAM. 根据我们先前的分析，我们的数据结构需要能够表示： 回文串添加字符得到另一个回文串这个可以利用添加有向边的方式完成，例如 P→XP′P\\underset{X}{\\to} PPX→​P′ 就表示在 PPP 的前后加上 XXX 字符就可以得到 P′PP′ 既然如此，那我们便不能令节点 nodeinode_inodei​ 表示结束位置在 iii 的回文串了，毕竟有可能 回文树的构建 回文树依赖若干个数据结构 树上的每一个节点实际上代表了一个以 iii 为结尾的最长回文串（从根到 s[i]s[i]s[i]） 对于树上的实边 next[] 指针，next[i][c] = j 说明在以 iii 为结尾的最长回文串的两边各添加字符 c（其实也就是 s[j]s[j]s[j]）之后，就变成了以 jjj 为结尾的最长回文串。 对于树上的虚边 fail[] 指针，它实际上维护的是以 iii 为结尾的次长回文串。如果 fail[i] = j，则次长的回文串是 node[j]，其中 node[j] 表示树上节点 jjj 代表的最长回文串 没错，也就是说此时以 iii 为结尾的次长回文串和以 jjj 为结尾的最长回文串是一样的。但是我们又必须以 iii 为结尾，所以我们只需要知道其长度信息即可。 要注意的是，节点建模的是“回文串”而不是“以 iii 为结尾的最长回文串”，因为以 iii 为结尾可能有很多回文串，我们就是要通过 fail 指针快速查找以 iii 为结尾的回文串的长度，所以回文串以 iii 结尾还是 jjj 结尾（对于长度这个信息而言）无关紧要。 我们先来定义数据结构 using indexing = int;constexpr int N = 5e5 + 5;constexpr int E = 26;struct Node std::arrayindexing, E next; // 实边 indexing fail0; // 虚边 fail 指针 int len0; // 这个节点对应的回文串 的长度 int num0; // 多少回文串以 节点i代表的最长回文串最末尾的字符 为结尾 int count0; // 这个节点对应的回文串 的数量 void init(int len) this-len = len; this-count = 0; fail = 0; next.fill(0); ;std::arrayNode, N T;std::arrayint, N S;indexing last = 0, pnode = -1, strend = -1;// 构造一个新节点，放在末尾indexing construct(int len) T.at(++pnode).init(len); return pnode;// 初始化，添加偶根和奇根节点// 在最开头插入一个不可能出现的字符，减少后面的特判void init() // 0: 偶根，1: 奇根 construct(0), construct(-1); T[0].fail = 1; S[++strend] = -1; last = 0; 跳 fail 指针 然后我们先来看如何跳 fail 指针。跳 fail 指针的核心在于，对于当前字符 S[i]S[i]S[i] 找到一个位置 jjj 使得 S[i]=S[j]S[i]=S[j]S[i]=S[j]。 如果指针当前指向的 vvv、其代表的回文串为 p(v)p(v)p(v)，则 j=i−p(v).len−1j=i-p(v).\\texttt{len}-1j=i−p(v).len−1。如果无法匹配，我们就跳 fail 指针 j←j.failj\\gets j.\\texttt{fail}j←j.fail，即我在满足 S[j…i−1]S[j\\dots i-1]S[j…i−1] 是回文串的条件下缩小这个后缀回文串的长度，然后判断 iii 能组成的后缀回文串的最大长度。 // 返回 pos 使得// T[pos].len 最大，且 S[strend - T[pos].len - 1] ~ S[strend] 构成回文串indexing match(indexing pos) while (S[strend] != S[strend - T[pos].len - 1]) pos = T[pos].fail; return pos; 在字符串末尾插入字符 然后我们执行插入字符操作。我们需要做两件事： 维护 next[] 指针（实边） 维护 fail 指针（虚边） 对于实边而言，我们找到一个第一个 pos 使得 S[i-T[pos].len-1] ~ S[i] 构成回文串，那么根据定义，iii 应该成为 pos 的儿子节点，边权为 S[i]. 对于虚边而言，我们需要找到下一个 jjj 使得 j≠posS[j…i] is palindrome. j e pos \\\\ S[j\\dots i] \\text{ is palindrome.} j=posS[j…i] is palindrome.既然 S[j…i]S[j\\dots i]S[j…i] 是回文串，那么 S[j+1…i−1]S[j+1\\dots i-1]S[j+1…i−1] 也应该是回文串，且 S[j]=S[i]S[j]=S[i]S[j]=S[i]。这不就是要找 S[i−1]S[i-1]S[i−1] 为结尾的回文串嘛！不过由于是 fail 指针，所以这里的“回文串”其实不是指最长回文串，也就是说，我们需要从 pos.fail 开始跳 fail 指针，以免用最长回文串来更新 fail. void insert(int c) S[++strend] = c; indexing fa = match(last); indexing son = T[fa].next[c]; if (!son) son = construct(T[fa].len + 2); T[son].fail = T[match(T[fa].fail)].next[c]; T[fa].next[c] = son; T[son].num = T[T[son].fail].num + 1; // 注意这里必须先维护 fail 再更新子节点 // 不然可能出现当 fa 为偶根和奇根的时候 T[son].fail = son last = son; T[son].count++; 在字符串的开头插入字符 在末尾插入比较好理解，考虑当前在维护第 iii 个字符，last 刚好可以代表以 S[i−1]S[i-1]S[i−1] 为结尾的最长回文串。那如果需要在字符串开头插入字符呢？ 类似的，我们想，是不是需要用 last_front 表示以 S[0]S[0]S[0] 为开头的最长回文串，然后跳 fail 呢？但是我们的节点维护的都是以某个字符为结尾的回文串，应该怎么转化呢？ 这里，我们就需要以“节点对应的是回文串”视角来看待 last 和 last_front 指针了（而非“节点对应的是以 i−1i-1i−1 为结尾的回文串”）。last_front 指针保存的回文串，是从 S.front() 为开头的最长回文串。 last_front 指针 我们考虑对 last_front 跳 fail 指针，得到的是什么。根据跳 fail 指针的定义，我们会有 last_front 的跳 fail 考虑 last_front 对应回文串的回文性，于是： 利用回文性和 fail 指针的长度信息 于是我们就可以使用和“在末尾插入字符”同样的思路，维护“在开头插入字符”的操作。 线性时间复杂度证明 证明线性复杂度，我们考虑节点在 fail 树上的深度。 如何维护一些常用信息 例题"},{"title":"Prufer 序列","path":"/wiki/algo/prufer-seq.html","content":"Prufer 序列 常用引理与性质 引理一 具有 nnn 个节点的无根树有 nn−2n^{n-2}nn−2 棵。 引理二 假设有 kkk 个连通块，每一块连通块有 sis_isi​ 个节点。添加 k−1k-1k−1 条边将这 kkk 个连通块连成一棵树的方案数为 nk−2⋅∏iksin^{k-2}\\cdot\\prod_i^k s_ink−2⋅i∏k​si​ 性质一 例题 洛谷 P11039"},{"title":"网络流：预流推进算法","path":"/wiki/algo/push-relabel.html","content":"代码参考 template typename Tclass FlowGraph public: struct Edge std::size_t to; T cap; // flow, capacity ; std::vectorint dist; std::vectorstd::size_t cur; std::vectorstd::vectorstd::size_t adj; std::vectorEdge edges; std::size_t s, t; std::size_t vtot, etot; // ! ===== functionality ===== FlowGraph() : vtot(0), etot(0) void init(std::size_t s, std::size_t t, std::size_t vtot) this-s = s, this-t = t, this-vtot = vtot; adj.resize(vtot), dist.resize(vtot), cur.resize(vtot); void add(std::size_t u, std::size_t v, T f) adj.at(u).push_back(edges.size()); edges.emplace_back(v, f); adj.at(v).push_back(edges.size()); edges.emplace_back(u, 0); ;template typename Tclass PushRelabel : public FlowGraphT public: std::vectorT height; std::vectorT excess; std::vectorstd::size_t gap; std::vectorstd::vectorstd::size_t bucket; T level0; static constexpr T inf = std::numeric_limitsT::max(); bool push(std::size_t u) bool init = u == this-s; for (auto e : this-adj.at(u)) auto [v, cap] = this-edges.at(e); if (cap == 0 or this-height.at(v) == inf) continue; if (!init and this-height.at(u) != this-height.at(v) + 1) continue; T k = init ? cap : std::min(cap, this-excess.at(u)); if (v != this-s and v != this-t and this-excess.at(v) == 0) this-bucket.at(this-height.at(v)).push_back(v); this-level = std::max(this-level, this-height.at(v)); // push this-excess.at(u) -= k; this-excess.at(v) += k; this-edges.at(e).cap -= k; this-edges.at(e ^ 1).cap += k; if (this-excess.at(u) == 0) return false; // finish pushing return true; void relabel(std::size_t u) this-height.at(u) = inf; for (auto e : this-adj.at(u)) auto [v, cap] = this-edges.at(e); if (cap 0) this-height.at(u) = std::min(this-height.at(u), this-height.at(v)); this-height.at(u)++; if (this-height.at(u) static_castT(this-vtot)) this-bucket.at(this-height.at(u)).push_back(u); level = std::max(level, this-height.at(u)); ++this-gap.at(this-height.at(u)); bool bfs_init() this-height.assign(this-vtot, inf); std::queuestd::size_t q; q.push(this-t); this-height.at(this-t) = 0; while (!q.empty()) auto u = q.front(); q.pop(); for (auto e : this-adj.at(u)) auto [v, cap] = this-edges.at(e); auto [rv, rcap] = this-edges.at(e ^ 1); if (rcap 0 and this-height.at(v) this-height.at(u) + 1) this-height.at(v) = this-height.at(u) + 1; q.push(v); return this-height.at(this-s) != inf; std::size_t select() while (this-level -1 and this-bucket.at(this-level).size() == 0) this-level--; return this-level == -1 ? 114514 : this-bucket.at(this-level).back(); public: void init(std::size_t s, std::size_t t, std::size_t n) FlowGraphT::init(s, t, n); this-height.assign(n, inf); this-excess.assign(n, 0); this-gap.assign(n + 1, 0); this-bucket.assign(n + 1, ); this-level = 0; T max_flow() if (not this-bfs_init()) return 0; this-gap.assign(this-vtot, 0); for (std::size_t i = 0; i this-vtot; i++) if (this-height.at(i) != inf) this-gap.at(this-height.at(i))++; this-height.at(this-s) = this-vtot; this-push(this-s); for (std::size_t u = select(); u != 114514; u = select()) this-bucket.at(this-level).pop_back(); if (this-push(u)) if (not --this-gap.at(this-height.at(u))) for (std::size_t i = 0; i this-vtot; i++) if (i == this-s) continue; if (this-height.at(i) = static_castT(this-vtot + 1)) continue; if (this-height.at(i) = this-height.at(u)) continue; this-height.at(i) = static_castT(this-vtot + 1); this-relabel(u); return this-excess.at(this-t); ;"},{"title":"线段树进阶（二）","path":"/wiki/algo/segtree-advanced.html","content":"势能线段树 势能线段树的时间复杂度基本都是基于“对节点的更新次数总共不会多于 O(f(n))O(f(n))O(f(n)) 次，而每次更新的复杂度是 O(1)O(1)O(1) 的，所以总的更新复杂度不会超过 O(f(n))O(f(n))O(f(n))”，通常 f(n)f(n)f(n) 都是 O(nlog⁡n)O(n\\log n)O(nlogn) 量级的。 如何判断是否该使用势能对线段树的操作进行分析呢？通常来说，我们需要观察到某个量一定会越来越小、不可能通过操作越来越大、且到达最小之后不需要更新。比如说： 开根号操作 x←⌊x⌋x\\gets \\lfloor\\sqrt x\\rfloorx←⌊x​⌋（上取整和下取整都可以）。我们发现 x≤x\\sqrt x\\le xx​≤x，当 x∈[1,3]x\\in [1,3]x∈[1,3] 的时候，这个根式 对元素取最小值 x←min⁡(x,v)x\\gets \\min(x, v)x←min(x,v)"},{"title":"Segment Tree Beats 维护区间最值、历史最值","path":"/wiki/algo/segtree-beats.html","content":"区间最值 线段树维护区间最值是需要支持如下的操作：对于一个数组 a[]a[]a[] query_sum(l, r) 求出 ∑i=lrai\\sum_{i=l}^r a_i∑i=lr​ai​ query_min(l, r) 求出 min⁡i∈[l,r]ai\\min_{i\\in[l,r]} a_imini∈[l,r]​ai​ query_max(l, r) 求出 max⁡i∈[l,r]ai\\max_{i\\in[l,r]} a_imaxi∈[l,r]​ai​ make_min(l, r, x) 修改 ai←min⁡(x,ai)a_i\\gets \\min(x, a_i)ai​←min(x,ai​) make_max(l, r, x) 修改 ai←max⁡(x,ai)a_i\\gets \\max(x, a_i)ai​←max(x,ai​) add(l, r, x) 修改 ai←ai+xa_i\\gets a_i+xai​←ai​+x"},{"title":"线段树分治","path":"/wiki/algo/segtree-decomp.html","content":"线段树分治"},{"title":"线段树进阶：维护各种奇怪的操作","path":"/wiki/algo/segtree-how.html","content":"Preface 本文采用的线段树模板是 AtCoder 的线段树模板，可以在 AtCoder Library 处下载并使用。这里简单提一下相关的 API，AtCoder 的线段树是通过 push tag 的方式实现的。 AtCoder 区间修改线段树 lazy_segtree API 模板参数填什么 模板参数 含义 S 线段树节点维护的信息 F 线段树节点上的懒标记 S op(S left, S right) 子节点节点合并到父节点，即 pull_up() 的过程 S e() 线段树节点初始化 S mapping(F, S) 把父节点上的懒标记下放到子节点上，即 push_down() F compose(F new, F old) 新的懒标记和旧的懒标记结合。注意这个参数顺序非常重要！！！先新标记，再旧标记！！ F id() 标记的初始化 区间加减、区间取 max，单点查询"},{"title":"数列模型","path":"/wiki/algo/seq-model.html","content":"数列变换模型 数列变换模型 有 A,BA,BA,B 两个数列，每次操作可以对 AAA 数列进行操作，求问能否通过操作使得 A=BA=BA=B？ 解题思路 转化成 ai←ai−1,aj←aj+1a_i\\gets a_i-1, a_j\\gets a_j+1ai​←ai​−1,aj​←aj​+1 的模型。"},{"title":"【总结】序列变换题型","path":"/wiki/algo/sequence-transformation.html","content":"序列变换模型 序列变换指的是这样一类题目：给定初始序列 { ai }\\set{a_i}{ai​} 和目标序列 { bi }\\set{b_i}{bi​}，然后给出可以使用的操作，判断 { ai }\\set{a_i}{ai​} 是否可以变化成 { bi }\\set{b_i}{bi​}，或是用最少的步数变化，或是给出可行方案。 如何破题 例题 ARC203B - Swap If Equal Sum"},{"title":"Sum over Subset DP","path":"/wiki/algo/sosdp.html","content":"Intro 若 xxx 有 kkk 个 bits 为 000，则 xxx 会被 2k2^k2k 个 mask 访问."},{"title":"分块、根号分治","path":"/wiki/algo/sqrt-decomposition.html","content":"分块 分块是个很有意思的维护数据的思想。通常可以用于维护线段树等数据结构难以维护的信息。 典型的分块思路 我们把整个序列分成大小为 BBB 的 n/Bn/Bn/B 个块。 对于一个修改操作 [l,r][l,r][l,r]： 如果 [l,r][l,r][l,r] 在同一个块内，我们直接遍历 [l,r][l,r][l,r] 进行暴力修改。单次的复杂度是 O(B)O(B)O(B) 的； 如果跨越多个块，我们把 [l,r][l,r][l,r] 分解成若干个整块和最多两个散块，修改整块时，我们只 O(1)O(1)O(1) 修改整块上的 tag，而修改散块时则直接暴力。单次的复杂度是 O(n/B+B)O(n/B+B)O(n/B+B) 的 对于查询 [l,r][l,r][l,r]： 如果在同一个块内，我们先下放这个块上的 tag 并更新序列，然后直接遍历求解，复杂度是 O(B)O(B)O(B) 的 如果跨越了多个块，那么我们还是先拆成多个整块和最多两个散块，对于散块，我们还是先下放 tag 然后暴力，这一部分是 O(B)O(B)O(B) 的；对于整块，我们不下放 tag，而是带着 tag 在块上进行查询（这一步通常需要维护额外的信息，这些信息会需要在修改散块的时候趁机更新），一般的话，可以做到 O(1)O(1)O(1) 查询一个整块，那么单次的时间复杂度就是 O(n/B+B)O(n/B+B)O(n/B+B) 分块例题 CF121E. Lucky Array 我们发现要“维护 4,7,44,…4,7,44,\\dots4,7,44,… 等特殊数字的个数”这个任务很难用线段树操作，所以转而尝试分块。假设块长为 BBB，分了 n/Bn/Bn/B 块。 因为 ai≤104a_i\\le 10^4ai​≤104 且这样的 lucky number 的数量只有 303030 多个，所以我们在块上开 10410^4104 个桶记录值域，即 bucket[v]=∣{i:ai=v}∣\\text{bucket}[v]=|\\{ i: a_i=v \\}|bucket[v]=∣{i:ai​=v}∣ 这里还有区间加的操作，我们在块上维护 tag 表示区间累计加了多少，查询 lucky number 个数的时候相应减去即可。时间复杂度 O(nn),B=nO(n\\sqrt n),B=\\sqrt nO(nn​),B=n​ Code 带着 tag 查询 lucky number 的时候需要注意 value = tag 这个问题，不然数组会越界。 #include bits/extc++.h#include bits/stdc++.hconstexpr int N = 1e5 + 10;constexpr int V = 1e4 + 10;constexpr int B = 400;std::vectorint P;int ck[V];int n, m;int a[N];int L[B], R[B], belong[N], bsize, bnum;int map[B][V], tag[B];void init_ck() for (int b = 1; b = 4; b++) for (int i = 0; i (1 b); i++) int t = 0; for (int j = 0; j b; j++) if (i (1 j)) t = t * 10 + 4; else t = t * 10 + 7; if (t V) ck[t] = 1; P.push_back(t); int check(int x) return ck[x]; void rebuild(int l, int r, int v) int id = belong[l]; if (tag[id] == 0) if (v == 0) return; for (int i = l; i = r; i++) map[id][a[i]]--; a[i] += v; map[id][a[i]]++; else for (int i = L[id]; i = R[id]; i++) map[id][a[i]]--; a[i] += tag[id]; if (l = i i = r) a[i] += v; map[id][a[i]]++; tag[id] = 0; void add(int l, int r, int v) int s = belong[l], e = belong[r]; if (s == e) rebuild(l, r, v); else rebuild(l, R[s], v); rebuild(L[e], r, v); for (int i = s + 1; i e; i++) tag[i] += v; int query(int l, int r) int s = belong[l], e = belong[r]; int ans = 0; if (s == e) rebuild(l, r, 0); for (int i = l; i = r; i++) ans += ck[a[i]]; else rebuild(l, R[s], 0); rebuild(L[e], r, 0); for (int i = l; i = R[s]; i++) ans += ck[a[i]]; for (int i = L[e]; i = r; i++) ans += ck[a[i]]; for (int i = s + 1; i e; i++) for (int p : P) if (p = tag[i]) ans += map[i][p - tag[i]]; return ans;int main() std::cin.tie(nullptr)-sync_with_stdio(false); std::cout.tie(nullptr); init_ck(); std::cin n m; bsize = std::sqrt(P.size() * n); bnum = (n + bsize - 1) / bsize; for (int i = 1; i = bnum; i++) L[i] = (i - 1) * bsize + 1; R[i] = std::min(i * bsize, n); for (int j = L[i]; j = R[i]; j++) belong[j] = i; for (int i = 1; i = n; i++) std::cin a[i]; map[belong[i]][a[i]]++; std::string op; int l, r, v; while (m--) std::cin op l r; if (op == add) std::cin v; add(l, r, v); else std::cout query(l, r) ; 根号分治"},{"title":"Ascend C 介绍与 NPU 芯片","path":"/wiki/ascendc/intro.html","content":"d0144f89de14a1fc384914230d36d0ec0054caf00183a222885cc88f31848a18aab140cd588bb1a31357d2dfedf6c8adfb592b49da5961aaa495123a3ccb3fa9a9f6c36e1bc55e6ff161822b769668a2076f5cfab45f196e1c53f0376e4956630c8a0923bfd91ca220c8d6a51d78be71e5b15538793255c1fcf3fbc64b80ef22508f3168feccbdaffac7faff9ec41aa90bd14105244569a56fb953d126552fc91d226fb4aa73c720ffee5e27a6e6c44331325d651b1f74c98b14d272388278d75502dfd76dee11cf4ae7c17bae192f36d0fc6097e31014e778d3ce734f793cc91d6a0cd8deee525e0a2b5df1feb19690d6cb0ed574e528834c7497615774b37a7d6a622e4a391fda0f6d046ee84fabcc6866a2fa790d0da49f405d25262fe0b227284612f2bbedafb19c35fddce7dbee9ea231f1128845e2cb4d3acb14a798c9445debfe7149b4028652944b99b85e6edd553e3657d323986791286e06611026a0b53564e9d42cb8d5930b665780169dd1f70bcc60ea6a108e7508f6b4bb904766b050f2d77edb2d92da12da5e5c4e226216b6aebe1366030bb436121c7a9f88e091b50f1553e5c3fbf3e08989b7c17a3a39de4e90112e198843ee98d7250d6cd7aa6775e73d8e3f2c10e535f76a20fb37d456d87f6666f252b1711bb852ba8ebdb2569e786dc13560eadb4ae4c42e8b5d14abef39d4ba6c5316bef5bce30910aa0dbe3bd454a7fdf5dbd7c47c034aa4008e8976343eca32e87ba156dc597cb588f4606b79ed83933468023f298250221b45caa3fbc371929526c7172ba395c9ece27b8ece80706195af6c0e7022c6f17e2b0cdd6b68406ce32e43d5bd7bc28fba6d76fe865bc8d0418149ca493a51097a7de299e67e71170344738fcfd519ce277e2a286cff0c58e2a44ea50dddf92fa061b6360d6804ef591820ebe7ead7532e594b1b2328f6deb4d87aa44f3b2c206b04e743f4062a0f3e97c1b4864b8c6d7906763512c3f64396b213ab89caf45d38c0f50f1f5f93fd38816ec6bebbca3364766f2a26cda6379849f8a480114accbaeb749a9dd5b9f6cbee7c31fcb51ff6fcf80c879e9eaa5f05c522fbc29d55095152d5c8e5f7745956c602d18cc1637d57c82ea950aad8409b557346ad1245f228a4492cdfd9274376eadf8dc24f39ecd4594177677dd558ea8a857c7beb585e8d2d780bb29be3591827a4768e7f03b285aa795a4f294b5af5c6474303a17193a9be731f2d6453374ecc65ce9bbeb3cb7eccac9708994ea7be1632ecf6be3094 Password is needed."},{"title":"2023 ICPC World Final Luxor","path":"/wiki/algo_contests/2023-icpc-wf-luxor.html","content":"A. D. Carl’s Vacation 可以联想到将军饮马模型。我们把三维的金字塔侧面展平到二维上，那么答案的最短路径就可以表达为 tip1→foot1→foot2→top2 tip_1\\to foot_1\\to foot_2\\to top_2 tip1​→foot1​→foot2​→top2​于是，我们可以枚举每个金字塔的四个侧面，共 4×4=164\\times 4=164×4=16 种情况，在每种情况里求最短路径即可。 接下来考虑如何求这个最短路径。我们肯定需要找到两个 footfootfoot 的坐标。考虑用向量的模长表示线段长度，以及将两个 footfootfoot 的定比分点作为变量的话，那么路径长度 f(k1,k2)f(k_1,k_2)f(k1​,k2​) 分别关于 k1,k2k_1,k_2k1​,k2​ 是单峰函数，所以可以三分套三分。 小细节：浮点数三分或者二分的话，可以指定二分次数，而非 l,rl,rl,r 相差 eps\\texttt{eps}eps，后者容易出现浮点误差。 Code #include headers/geometry.hpp#include iostreamusing namespace Geo2D;using namespace std;Point p1[4], p2[4], tip1, tip2;Decimal h1, h2, d1, d2, len1, len2;Decimal phi = 0.618, cphi = -phi + 1;int main() cin p1[0] p1[1] h1; cin p2[0] p2[1] h2; for (int i = 2; i 4; i++) p1[i] = p1[i - 1] + (p1[i - 1] - p1[i - 2]).Perp(); p2[i] = p2[i - 1] + (p2[i - 1] - p2[i - 2]).Perp(); tip1 = (p1[0] + p1[2]) / 2; tip2 = (p2[0] + p2[2]) / 2; len1 = p1[0].Distance(p1[1]); len2 = p2[0].Distance(p2[1]); d1 = (len1.sqr() / 4 + h1.sqr()).sqrt(); d2 = (len2.sqr() / 4 + h2.sqr()).sqrt(); Decimal ans = 2e18; for (int i = 0; i 4; i++) Vector v1 = p1[(i + 1) % 4] - p1[i]; Point midp1 = (p1[i] + p1[(i + 1) % 4]) / 2; Point pt1 = midp1 + v1.Normal() * d1; for (int j = 0; j 4; j++) Vector v2 = p2[(j + 1) % 4] - p2[j]; Point midp2 = (p2[j] + p2[(j + 1) % 4]) / 2; Point pt2 = midp2 + v2.Normal() * d2; Decimal precent_l1 = 0; Decimal precent_r1 = 1; auto findfoot1 = [](Decimal precent_mid1) - Decimal Point foot1 = p1[i] + v1 * precent_mid1; Decimal precent_l2 = 0; Decimal precent_r2 = 1; auto findfoot2 = [](Decimal precent_mid2) - Decimal Point foot2 = p2[j] + v2 * precent_mid2; return foot1.Distance(foot2) + foot1.Distance(pt1) + foot2.Distance(pt2); ; for (int __ = 1; __ = 100; __++) Decimal l2 = precent_l2 * phi + precent_r2 * cphi; Decimal r2 = precent_l2 * cphi + precent_r2 * phi; if (findfoot2(l2) findfoot2(r2)) precent_l2 = l2; else precent_r2 = r2; return findfoot2(precent_l2); ; for (int _ = 1; _ = 100; _++) Decimal l1 = precent_l1 * phi + precent_r1 * cphi; Decimal r1 = precent_l1 * cphi + precent_r1 * phi; if (findfoot1(l1) findfoot1(r1)) precent_l1 = l1; else precent_r1 = r1; ans = min(ans, findfoot1(precent_l1)); std::cout ans ;"},{"title":"2024 ICPC EC 第二场网络预选赛","path":"/wiki/algo_contests/2024-icpc-ec-online-2.html","content":"这一场其实是队友带飞的，但是还是来补一下题。 A. Gambling on Choosing Regionals 最核心的点就是，对于某只队伍 tit_iti​ 最坏情况下其他学校都派最强的队伍来和这支队伍竞争，那么可想而知，如果学校越多，那么很有可能其他学校的强队全都来了，那么自己的机会就越来越小了，所以核心的点就是一定选择最小的赛站。 因为 tit_iti​ 总是有优先权，因此对于 tit_iti​ 来说，在他之上的队伍由两部分组成（假设最小的赛站，每个学校可以出 ccc 支队伍） 其他学校的所有强队，数量 ≤c\\le c≤c 本学校的强队，数量 ≤c−1\\le c-1≤c−1。因为包含 tit_iti​ 因此，我们直接对每只队伍的实力排序，先每个学校取前 ccc 个，然后如果某个队伍不在其学校的前 ccc 名，那么踢掉最后一名，把这支队伍换上来。总时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn). G. Game 首先我们要看出来平局其实没有用（因为不会产生任何影响）。我们直接令 p1=a0a0+a1,p2=a1a0+a1p_1=\\frac{a_0}{a_0+a_1},p_2=\\frac{a_1}{a_0+a_1}p1​=a0​+a1​a0​​,p2​=a0​+a1​a1​​. 然后分别考虑两人的筹码数和获胜条件，设 Alice 有 aaa 个筹码，Bob 有 bbb 个 如果 a=ba=ba=b，令 a=kb+r,r∈[0,b)a=kb+r,r\\in[0,b)a=kb+r,r∈[0,b) 此时，Alice 只要在这 kkk 次里赢一次，游戏就结束了。 这里的分布是“有 p1p_1p1​ 概率获胜， continue until first win”，其获胜概率为 ∑i=0kp1p2i=1−p2k1−p2×p1\\sum_{i=0}^k p_1 p_2^i=\\frac{1-p_2^k}{1-p_2}\\times p_1∑i=0k​p1​p2i​=1−p2​1−p2k​​×p1​ 然而，如果全输了，那么 rbr\\lt brb 会进入 2) 分支。综合一下，即为 win(a,b)=1−p2k1−p2p1+p2k×win(r,b)win(a,b)=\\frac{1-p_2^k}{1-p_2}p_1+p_2^k\\times win(r, b)win(a,b)=1−p2​1−p2k​​p1​+p2k​×win(r,b) 如果 ababab，令 b=ka+r,r∈[0,a)b=ka+r,r\\in[0, a)b=ka+r,r∈[0,a) 此时局面刚好反过来，Alice 要想获胜，必须保证这 kkk 次不能输（否则游戏结束） 当这 kkk 全赢了之后，局面回到 1)。因此 win(a,b)=p1k×win(a,r)win(a,b)=p_1^k\\times win(a, r)win(a,b)=p1k​×win(a,r) 而回顾 win(x,y)win(x,y)win(x,y) 的参数变化，这不就是辗转相除法吗！因此整体时间复杂度为 O(log⁡n)O(\\log n)O(logn)，考虑到逆元、快速幂的计算（p1,p2,kp_1,p_2,kp1​,p2​,k 与 nnn 差不多量级），其实差不多是 O(log⁡2n)O(\\log^2n)O(log2n) Code 注意最好用 int 做逆元相关的题，速度会比 uint64_t 之类的快很多。 #include iostreamconstexpr int M = 998244353;inline int smul(int a, int b) return (1ll * a * b M ? a * b : 1ll * a * b % M); inline int sadd(int a, int b) return (a + b = M ? a + b - M : a + b); int fpow(int base, int power = M - 2) int res = 1; for (; power; power = 1) if (power 1) res = smul(res, base); base = smul(base, base); return res;int answer(int a, int b, int win, int lose) if (a == 0 || b == 0) return a != 0; if (a = b) int k = a / b, r = a % b; int c = fpow(lose, k); int numerator = sadd(1, M - fpow(lose, k)); // 1 - lose^k (mod M) int denominator = sadd(1, M - lose); // 1 - lose (mod M) int tmp = smul(win, smul(numerator, fpow(denominator))); // Using modular inverse return sadd(smul(c, answer(r, b, win, lose)), tmp); else int k = b / a, r = b % a; int c = fpow(win, k); return smul(c, answer(a, r, win, lose)); void run() int a, b; int p1, p2, P; std::cin a b p1 p2 P; P = p1 + p2; p1 = smul(p1, fpow(P)), p2 = smul(p2, fpow(P)); std::cout answer(a, b, p1, p2) ;int main() std::cin.tie(0)-sync_with_stdio(0); int T; std::cin T; while (T--) run(); L. 502 Bad Gateway 当当前时刻为 1,2,3,…1,2,3,\\dots1,2,3,… 的时候，按按钮重置时间反而得不偿失；相反，如果当前时刻比较大，那么重置时间更有可能缩短用时。 于是基于这一个观察，我们可以猜测：存在一个阈值 ccc，当时刻 c\\lt cc 我们就慢慢等；反之我们就一直按按钮，直到 c\\lt cc 为止。 根据我们的猜测，每摁一次按钮，重置到 c\\lt cc 的概率为 p=ctp=\\frac{c}{t}p=tc​. “不断按成功概率为 ppp 的按钮，直到第一次成功停止”，诶，这不就是几何分布吗？"},{"title":"2024 ICPC 区域赛（西欧北欧 NWERC）","path":"/wiki/algo_contests/2024-icpc-nwerc.html","content":"A. Alphabetical Aristocrats very ez."},{"title":"2024 ICPC 区域赛：香港","path":"/wiki/algo_contests/2024-icpc-regional-hk.html","content":"E. Concave Hull 算法流程 先算一次凹包，把所有的点分成“在凸包上”和“不在凸包上”的点 SSS。 枚举 SSS 中的每一个点作为凹点 p0p_0p0​，然后对其他所有点 pip_ipi​ 计算出向量 vi=pip0→v_i=\\overrightarrow{p_ip_0}vi​=pi​p0​​ 并按极角排序。 极角排序完了之后，向量必定是 on, /, /, /, on, /, /, on, on, /, /, on ...... 这样排列（两个在凸包上的点 c1,c2c_1,c_2c1​,c2​ 中间夹着一些不在凸包上的点 djd_jdj​，记这些点的集合为 F={F0=c1,F1=d1,d2,…,Fm=dm,Fm+1=c2}F=\\{F_0=c_1,F_1=d_1,d_2,\\dots,F_m=d_m,F_{m+1}=c_2\\}F={F0​=c1​,F1​=d1​,d2​,…,Fm​=dm​,Fm+1​=c2​}）。我们尝试计算以 p0p_0p0​ 作为凹点，Fi,Fi+1F_i,F_{i+1}Fi​,Fi+1​ 作为优角的两边的凹包面积。 这里的话，如果跑暴力算法，时间复杂度会来到 O(n3)O(n^3)O(n3)。考虑到这个凹包的面积其实是凸包面积去掉一部分面积，我们可以利用这一点加速计算。 两个三角形 我们把凹包凹进去的部分分成左右两半凸壳（图中黄色和绿色部分），做两次 Andrew 凸包扫描算法（从左往右，然后从右往左）。例如 Andrew 算法从左往右扫描，只要扫描算法扫描经过这些点 FFF，那么我们就能算出由 c1→Fic_1\\to F_ic1​→Fi​ 这些点构成（且包括了 FiF_iFi​ 的）的左半凸壳的面积。同理也可以计算出右半凸壳的面积。因此以 p0p_0p0​ 为凹点、Fi,Fi+1F_i,F_{i+1}Fi​,Fi+1​ 为优角的凹包面积也就可以算出来了（大凸包面积，减掉这一个凹角对应的凸包上三角形的面积 ΔADG\\Delta ADGΔADG，再加上左右两个凸壳的面积） Code Code #include algorithm#include cassert#include cmath#include iostream#include ranges#include utility#include set#include vectorusing i64 = long long;constexpr i64 M = 1e9 + 7;struct pvec int x, y; friend std::istream operator(std::istream is, pvec a) return is a.x a.y; friend std::ostream operator(std::ostream os, const pvec a) return os a.x a.y; bool operator==(const pvec a) const return x == a.x y == a.y; pvec operator-(const pvec a) const return x - a.x, y - a.y; bool operator(const pvec a) const return x == a.x ? y a.y : x a.x; ;int sign(i64 val) return val 0 ? -1 : (val 0 ? 1 : 0); i64 cross(const pvec a, const pvec b) return 1ll * a.x * b.y - 1ll * a.y * b.x; i64 cross(const pvec a, const pvec b, const pvec c) return cross(b - a, c - a); i64 dot(const pvec a, const pvec b) return 1ll * a.x * b.x + 1ll * a.y * b.y; int scross(const pvec a, const pvec b, const pvec c) return sign(cross(a, b, c)); i64 sqrlen(const pvec a) return dot(a, a); auto convex_hull(const std::vectorpvec x) std::vectorpvec v(x); std::vectorpvec used, unused; std::sort(v.begin(), v.end()); int m = v.size(), tp = -1; for (int i = 0; i m; i++) while (tp 0 cross(used[tp-1], used[tp], v[i]) = 0) tp--, used.pop_back(); used.push_back(v[i]), tp++; int t = tp; for (int i = m - 1; i = 0; i--) while (tp t cross(used[tp-1], used[tp], v[i]) = 0) tp--; used.pop_back(); used.push_back(v[i]), tp++; used.pop_back(); std::setpvec s; for(auto d: used) s.insert(d); for(auto d: v) if (!s.contains(d)) unused.push_back(d); return std::pairused, unused;bool comp(const pvec a, const pvec b) bool upA = a.y 0 || (a.y == 0 a.x = 0); bool upB = b.y 0 || (b.y == 0 b.x = 0); if (upA != upB) return upA; auto val = cross(a, b); return val 0;i64 area(const std::vectorpvec p) i64 res = 0; for (int i = 0, m = p.size(); i m; i++) res += cross(p[i], p[(i + 1) % m]); return res;void run() int n; std::cin n; std::vectorpvec p(n); for (auto x : p) std::cin x; auto [used, un] = convex_hull(p); int sz = used.size(); i64 ans = 0; for (const auto x : un) // enum concave point. std::vectorstd::pairpvec, int al; std::vectorpvec cur(sz); std::vectori64 val(sz, 0); // area of triangle formed by on-convex points i64 sum = 0; for (const auto y : un) // compute vectors if (y == x) continue; al.push_back(y - x, -1); for (int i = 0; i sz; i++) cur[i] = used[i] - x; al.push_back(cur[i], i); // sort by angle std::sort(al.begin(), al.end(), [](const auto a, const auto b) return comp(a.first, b.first); ); // rotate to satisfy pattern: // [on-convex, not, not, ..., not, on-convex, not, not .... , not, on-convex] for (int i = 0; i al.size(); i++) if (al[i].second == -1) continue; std::rotate(al.begin(), al.begin() + i, al.end()); break; // compute convex area for (int i = 0; i sz; i++) val[i] = cross(cur[i], cur[(i + 1) % sz]); sum += val[i]; // enum all points between 2 on-convex points for (int l = 0, r = 0, al_size = al.size(); l al_size; l = r) r = l + 1; while (r al_size al[r].second == -1) r++; // (l, r) is the range of not-on-convex points // l, r are on-convex points int pos = al[l].second; std::vectori64 T(r - l, 0); assert((pos + 1) % sz == al[r % al_size].second); // left convex [al, T, l, r] std::vectorpvec pts; int top = -1; i64 ssum = 0; for (int fix = l; fix r; fix++) const auto q = al[fix].first; while (top = 1 cross(q - pts[top - 1], pts[top] - pts[top - 1]) = 0) ssum -= cross(pts[top - 1], pts[top]); top--, pts.pop_back(); pts.push_back(q), top++; if (top = 1) ssum += cross(pts[top - 1], pts[top]); T[fix - l] += ssum; (); // right convex [al, T, l, r, al_size] std::vectorpvec pts; int top = -1; i64 ssum = 0; for (int fix = r; fix l; fix--) const auto q = al[fix % al_size].first; while (top = 1 cross(pts[top - 1], pts[top], q) = 0) ssum -= cross(pts[top], pts[top - 1]); top--, pts.pop_back(); pts.push_back(q); top++; if (top = 1) ssum += cross(pts[top], pts[top - 1]); T[fix - l - 1] += ssum; (); for (int i = 0; i r - l; i++) i64 st = sum - val[pos] + T[i]; (ans += std::abs(st)) %= M; std::cout ans ;int main() std::ios::sync_with_stdio(false); std::cin.tie(nullptr); std::cout.tie(nullptr); int T = 1; // std::cin T; while (T--) run(); return 0; H. Mah-Jong 算法流程 我们先预处理出所有可能的顺子的情况 SSS，每个顺子最多出现 222 次（否则可以视为 333 个碰），最少出现 000 次，因此最多 729729729 种情况。 于是每一个合法的区间都可以视为，SSS 中的某一个顺子搭配 sss 加上 若干个碰，因此对于区间 [l,r][l, r][l,r] 而言，令这个区间有 did_idi​ 个数字为 iii 的麻将牌，而顺子组合 sss 要求 bib_ibi​ 个数字为 iii 的牌，那么区间合法这个条件等同于 di≡bi(mod3)di≥bi \\begin{aligned} d_i\\equiv b_i \\pmod 3\\\\ d_i\\ge b_i \\end{aligned} di​di​​≡bi​(mod3)≥bi​​ 考虑如何维护 di≥bid_i\\ge b_idi​≥bi​ 这个条件： 如果我们固定右端点 rrr，那么我们只需要让 l:r→1l:r\\to 1l:r→1 扫描，直到 [l,r][l,r][l,r] 的 did_idi​ 开始满足 di≥bid_i\\ge b_idi​≥bi​，那么对于所有 plp\\lt lpl，都一定会有 [p,r]:di≥bi[p,r]:d_i\\ge b_i[p,r]:di​≥bi​（因为 plp\\lt lpl，因此只会往这个区间里添加新的数，因此 did_idi​ 不可能变小） 考虑如何维护同余 di≡bi(mod3)d_i\\equiv b_i\\pmod {3}di​≡bi​(mod3) 用桶维护即可。可以开 888 维数组或者用三进制表示 时间复杂度 O(36n)O(3^6n)O(36n) Code #include iostream#include ranges#include vectorusing i64 = long long;void run() int n; std::cin n; std::vector cnt(n + 1, std::vectorint(8, 0)); std::vectorint mask(n + 1, 0); std::vectorint a(n + 1, 0); auto encode = [](int d) int s = 0; for (auto i : std::views::iota(0, 8)) s = s * 3 + cnt.at(d).at(i) % 3; return s; ; auto decode_chow = [](int pat) std::vectorint p(8, 0); for (int i : std::views::iota(0, 6)) int u = pat % 3; pat /= 3; p.at(i) += u, p.at(i + 1) += u, p.at(i + 2) += u; return p; ; for (int i = 1; i = n; i++) std::cin a.at(i); a.at(i)--; cnt.at(i) = cnt.at(i - 1); cnt.at(i).at(a.at(i))++; mask.at(i) = encode(i); i64 ans = 0; std::vectorint bucket(8000, 0); for (auto pattern : std::views::iota(0, 729)) auto pat = decode_chow(pattern); for (int i = 0; i = n; i++) bucket.at(mask.at(i))++; int r = 0; for (int l = 1; l = n; l++) for (; r = l; r++) bucket.at(mask.at(r))--; // 先去除不合法的区间（即 右端点小于左端点的区间） int target = 0; for (int t : std::views::iota(0, 8)) // 用双指针去除不满足偏序关系的 while (r = n cnt.at(r).at(t) cnt.at(l - 1).at(t) + pat.at(t)) bucket.at(mask.at(r))--; r++; target = target * 3 + (cnt.at(l - 1).at(t) + pat.at(t)) % 3; if (r n) break; ans += bucket[target]; // 加上满足同余的区间的贡献 std::cout ans ;int main() std::ios::sync_with_stdio(false); std::cin.tie(nullptr); std::cout.tie(nullptr); int T = 1; std::cin T; while (T--) run(); return 0;"},{"title":"2025 杭电多校春季赛 4","path":"/wiki/algo_contests/2025-hdu-spring-04.html","content":"战斗爽 根据题意我们可以直接模拟： 我们用 priority_queue 维护所有敌人的攻击力 atk 以及尚且存活的敌人的血量 alive，用数组 dead 记录敌人是否活着 直接在 while 循环里模拟每一轮： 从 alive 里根据规则取出一个敌人，结算伤害，更新其存活状态 接下来结算收到的伤害。从 atk 堆顶取出攻击力最大的，如果堆顶的敌人已经死亡，则弹出取下一个元素，直到堆顶的敌人是存活的。 Code #include algorithm#include iostream#include queue#include tuple#include vectorusing i64 = long long;using M = std::tupleint, int, int, int; // hp, atk, id, timesusing N = std::tupleint, int; // atk, idvoid run() int n, u, k, hq; std::cin n u k hq; std::priority_queueM, std::vectorM, std::greaterM pq; std::priority_queueN atks; i64 atk = 0; for (i64 i = 0, a, b; i n; i++) std::cin a b; pq.push(b, a, i, 0); atks.push(a, i); atk = std::max(atk, a); int cnt = 0; std::vectorint dead(n, false); while (!pq.empty() and hq = 0) auto [hp, a, id, t] = pq.top(); pq.pop(); // attack it if (t k) if (t == 0) hp -= u; else hp -= u / 2; t++; if (hp = 0) cnt++, dead.at(id) = true; else pq.push(hp, a, id, t); while (!atks.empty()) if (dead.at(std::get1(atks.top()))) atks.pop(); else break; atk = std::get0(atks.top()); hq -= atk; std::cout cnt ;int main() std::ios::sync_with_stdio(false); std::cin.tie(nullptr); std::cout.tie(nullptr); int T = 1; std::cin T; while (T--) run(); return 0; 充实 手推注意到如果只有两个数 xyxyxy 的话，有解当且仅当 y−x=2ky-x=2^ky−x=2k. 然后考虑多个数的情况，假设 a1a2a3a4…ama_1\\lt a_2\\lt a_3\\lt a_4 \\dots a_ma1​a2​a3​a4​…am​，其差分为 di=ai+1−aid_i=a_{i+1}-a_idi​=ai+1​−ai​. 只要 gcd⁡(di)=2k\\gcd(d_i)=2^kgcd(di​)=2k 就有解 Code 持家 考虑打 aaa 折，减 bbb 元，原价 PPP 元，则有 (P−b)×a=Pa−baPa−b (P-b)\\times a=Pa-ba \\lt Pa-b (P−b)×a=Pa−baPa−b所以我们应该总是先用打折券，再用减价券。时间复杂度为排序的 O(nlog⁡n)O(n\\log n)O(nlogn) Code #include algorithm#include iomanip#include iostream#include vectorusing i64 = long long;void run() int P, n, k; std::cin P n k; std::vectordouble a, b; for (int i = 0, c, t; i n; i++) std::cin t c; if (t == 0) a.push_back(c * 1.0 / 10); else b.push_back(c); std::sort(a.begin(), a.end()), a.insert(a.begin(), 1); std::sort(b.begin(), b.end(), std::greater()), b.insert(b.begin(), 0); for (int i = 1; i a.size(); i++) a[i] *= a[i - 1]; for (int i = 1; i b.size(); i++) b[i] += b[i - 1]; double ans = P; for (int i = 0; i = k; i++) if (i a.size()) ans = std::min(ans, a[i] * P - b[std::min(k - i, int(b.size()) - 1)]); std::cout std::fixed std::setprecision(2) std::max(ans, 0.0) ;int main() std::ios::sync_with_stdio(false); std::cin.tie(nullptr); std::cout.tie(nullptr); int T = 1; std::cin T; while (T--) run(); return 0;"},{"title":"香港中文大学（深圳）2025 校赛暨粤港澳国际编程比赛","path":"/wiki/algo_contests/20250419-CUHKSZ-contest.html","content":"A. Milky Loong 大致题意：给你一个字符串，提取一部分文字出来和另一部分拼一起。 实际也非常简单，用 Python 随便水一水就可以了。毕竟是照顾初学者的签到题 B. 约瑟夫问题 大致题意： 有 n≤105n\\le 10^5n≤105 个人围成一个圆，给定一个 2≤k≤92\\le k\\le 92≤k≤9，每个人轮流报数： 如果这个数是 kkk 的倍数、或者其十进制表示带有 kkk 这个数字，那么这个人就被杀死 报数的时候会跳过已经死掉的人 问最后活下来的是谁。 也毕竟简单。考虑到最多每 kkk 个数字就会干掉一个人，因此最多 nknknk 轮就会结束。 C. F. 试飞 大致题意： nnn 个人里面有 mmm 个人具有飞行经验，你的目标是选出两个有飞行经验的人试飞。 你每次可以选择任意 222 个人让其试飞，如果这两个人都具有飞行经验，则任务立刻结束；否则试飞失败。 你只能用至多 ⌊n2m⌋\\lfloor \\frac{n^2}{m}\\rfloor⌊mn2​⌋ 次试飞完成目标。 非常有意思的一道题目。考察鸽巢原理。具体做法是，把这 nnn 个人平分到 m−1m-1m−1 个组里，那么根据鸽巢原理，必然有一个组里有 222 个人具有飞行经验。 于是，我们直接对每一个组暴力枚举 pair. 由于是平分，每个组差不多 nm−1\\frac{n}{m-1}m−1n​ 人，因此一个组内的枚举次数为 12×(nm−1−1)×nm−1\\frac{1}{2}\\times(\\frac{n}{m-1}-1)\\times\\frac{n}{m-1}21​×(m−1n​−1)×m−1n​，而有 m−1m-1m−1 组，因此总枚举次数为 (m−1)×12×(nm−1−1)×nm−1=n22(m−1)−n≤n2m \\begin{aligned} (m-1)\\times\\frac{1}{2}\\times(\\frac{n}{m-1}-1)\\times\\frac{n}{m-1}\\\\ =\\frac{n^2}{2(m-1)}-n\\\\ \\le \\frac{n^2}{m} \\end{aligned} =≤​(m−1)×21​×(m−1n​−1)×m−1n​2(m−1)n2​−nmn2​​"},{"title":"AtCoder Regular Contest 203 (Div. 2)","path":"/wiki/algo_contests/ARC203.html","content":"A - All Winners 考虑第 iii 组有 aia_iai​ 个全胜者，那么考虑任意两组之间的 PK，应该有 ai+aj≤M,i,j∈[1,n] a_i+a_j\\le M, i,j\\in[1,n] ai​+aj​≤M,i,j∈[1,n]我们要在这个约束下，求出 max⁡∑iai\\max\\sum_i a_imax∑i​ai​. 考虑 a1a_1a1​ 的取值和 n−1n-1n−1 个与之有关的不等式，可以发现 ∑iai\\sum_i a_i∑i​ai​ 和 aia_iai​ 也需要满足不等式： ∑iai≤a1+(n−1)(M−a1)=(n−1)M−(n−2)a12(M−a1)≤M \\sum_i a_i\\le a_1+(n-1)(M-a_1)=(n-1)M-(n-2)a_1\\\\ 2(M-a_1)\\le M i∑​ai​≤a1​+(n−1)(M−a1​)=(n−1)M−(n−2)a1​2(M−a1​)≤M所以 a1a_1a1​ 应该取最小值，即 a1=⌈M2⌉a_1=\\lceil\\frac{M}{2}\\rceila1​=⌈2M​⌉，于是 ai=⌊M2⌋,i1a_i=\\lfloor\\frac{M}{2}\\rfloor, i\\gt 1ai​=⌊2M​⌋,i1 AC Code #include bits/stdc++.hvoid run() long long n, m; std::cin n m; if (m % 2 == 0) std::cout m / 2 * n ; else std::cout (m + 1) / 2 + (m - 1) / 2 * (n - 1) ;int main() int t; std::cin t; while (t--) run(); B - Swap If Equal Sum C - Destruction of Walls 因为从左上到右下至少需要经过 H+W−1H+W-1H+W−1 个格子，那么就需要至少开 H+W−2H+W-2H+W−2 堵墙，所以当 KH+W−2K\\lt H+W-2KH+W−2 时答案为 000。 当 K=H+W−2K=H+W-2K=H+W−2 的时候，刚好够左上到右下，因此开墙的方案数等于从左上到右下的路径数量。因为 hhh 方向上要走 H−1H-1H−1 步，而总共能走 H+W−2H+W-2H+W−2 步，所以从 H+W−2H+W-2H+W−2 里选出任意的 H−1H-1H−1 步向右走就是方案数，此时答案为 (H+W−2H−1)\\binom{H+W-2}{H-1}(H−1H+W−2​). 当 K=H+W−1K=H+W-1K=H+W−1 时，虽然能多开一堵墙，但是新开的这堵墙却不能形成任何新的路径，因此答案就是上一种情况下的每条路径再选另外的一堵墙开。选完路径后还剩 H(W−1)+(H−1)W−(H+W−2)H(W-1)+(H-1)W-(H+W-2)H(W−1)+(H−1)W−(H+W−2) 堵墙可选，选一堵墙即可。答案为 (H+W−2H−1)⋅(H(W−1)+(H−1)W−(H+W−2))\\binom{H+W-2}{H-1}\\cdot \\Big( H(W-1)+(H-1)W-(H+W-2) \\Big)(H−1H+W−2​)⋅(H(W−1)+(H−1)W−(H+W−2)) 当 K=H+WK=H+WK=H+W 时情况就变得有点复杂了。按照相同的思路，在剩下的墙里任选两堵墙，但是新开的两堵墙却有可能产生新的路径，从而导致重复/漏数. 可能重复的情况是“开墙后，有两条长度为 H+W−1H+W-1H+W−1 的路径”，漏数的情况是“开墙后，产生一条长度为 H+W+1H+W+1H+W+1 的路径” 有多少种情况会有两条长度为 H+W−1H+W-1H+W−1 的路径呢？有两条路径，就说明在某个格子 (i,j)(i,j)(i,j)，从它出发有两条路径到达 (i+1,j+1)(i+1,j+1)(i+1,j+1)，而且恰好形成 2×22\\times 22×2 的区域。对于这个 2×22\\times 22×2 的区域来说，路径必须从 (i,j)(i,j)(i,j) 的上方或左侧进入 (i,j)(i,j)(i,j)，也必须从 (i+1,j+1)(i+1,j+1)(i+1,j+1) 的下方或右侧离开。所以，我们考虑先构造一条长度为 H+W−3H+W-3H+W−3 的路径，然后，我们把这个路径上的一个格子换成这样的 2×22\\times 22×2 区域然后和剩下的部分接通，就能得到符合条件的、有两条长度为 H+W−1H+W-1H+W−1 路径的方案了。由于要保证替换完的路径需要是 H×WH\\times WH×W 的对角线，而替换则相当于构造的路径在横向、纵向都多了一格，因此，需要保证构造的路径的横向格子数和纵向格子数为 W−1,H−1W-1,H-1W−1,H−1，相当于在 (H−1)×(W−1)(H-1)\\times (W-1)(H−1)×(W−1) 的网格里。这样的方案数是 (H+W−4H−2)⋅(H+W−3) \\binom{H+W-4}{H-2}\\cdot (H+W-3) (H−2H+W−4​)⋅(H+W−3)接着考虑漏数的情况。这种情况产生的路径可以这样看：考虑最短路径长度为 H+WH + WH+W 的情况。在垂直或水平方向上仅一次远离 (H,W)(H,W)(H,W) 的移动，且其前后的所有操作应该都是向右或向下。 考虑枚举远离目标的这一步，假设为向上，则其前后的一个操作必须都为水平（向右）。 前后的一步动作不能是向下，不然就说明从这个格子走回去/走回来了。 这个向上导致在它之后需要多出一个向下的动作，而向右的动作数量仍然不变，相当于现在需要在 H+WH+WH+W 的位置上放置 111 个向上、HHH 个向下、W−1W-1W−1 个向右，并且向上的动作后面至少要有 111 个向下（不然会是从第 H+1H+1H+1 行走到第 HHH 行，不合法）、前面至少有一个向下（不然会从第 111 行走到第 000 行）、其前后一步都是向右。 所以考虑使用打包法，先把“向右、向上、向右”打包为 XXX，这样就是 111 个 “X”、HHH 个向下、W−3W-3W−3 个向右放一起排列组合。我们先把 “X” 也看成向下，进行排列组合，然后再将第 2∼H2\\sim H2∼H 个向下换成 “X”（注意不能是第 111 个和第 H+1H+1H+1 个，原因见上一段）。所以方案数是 (H+W−2H+1)⋅(H−1) \\binom{H+W-2}{H+1}\\cdot(H-1) (H+1H+W−2​)⋅(H−1)当远离的那一步是向左的时候，也是同理，方案数是 (H+W−2W+1)⋅(W−1)\\binom{H+W-2}{W+1}\\cdot(W-1)(W+1H+W−2​)⋅(W−1) AC Code #include bits/stdc++.husing i64 = long long;constexpr i64 P = 998244353;constexpr int N = 2e5 + 10;i64 fpow(i64 b, i64 p) i64 res = 1; while (p) if (p 1) res = res * b % P; b = b * b % P; p = 1; return res;template int M = Pstruct mint int v; mint(i64 x = 0) : vint((x % P + P) % P) mint operator+(mint rhs) const return v + rhs.v = P ? v + rhs.v - P : v + rhs.v; mint operator-(mint rhs) const return v = rhs.v ? v - rhs.v : v - rhs.v + P; mint operator*(mint rhs) const return 1ll * v * rhs.v % P; mint operator/(mint rhs) const return 1ll * v * fpow(rhs.v, P - 2) % P; mint operator+=(mint rhs) return *this = *this + rhs; mint operator-=(mint rhs) return *this = *this - rhs; mint operator*=(mint rhs) return *this = *this * rhs; mint operator/=(mint rhs) return *this = *this / rhs; mint inv() const return fpow(v, P - 2); friend std::ostream operator(std::ostream os, mint rhs) return os rhs.v; ;using Z = mintP;Z F[N * 2], iF[N * 2];void init() F[0] = 1; for (int i = 1; i N * 2; i++) F[i] = F[i - 1] * i; iF[N * 2 - 1] = F[N * 2 - 1].inv(); for (int i = N * 2 - 2; i = 0; i--) iF[i] = iF[i + 1] * (i + 1);Z C(i64 n, i64 m) if (m 0 || m n) return 0; if (m == 1) return n; if (m == 2) return iF[2] * n * (n - 1); return F[n] * iF[m] * iF[n - m];Z A(i64 n, i64 m) if (m 0 || m n) return 0; return F[n] * iF[n - m];void run() i64 h, w, k; std::cin h w k; if (k h + w - 2) return std::cout 0 , void(); i64 tot = (h - 1) * w + h * (w - 1), rest = tot - (h + w - 2); k -= (h + w - 2); if (k == 0) std::cout C(h + w - 2, h - 1) ; else if (k == 1) std::cout C(h + w - 2, h - 1) * rest ; else mint path = C(h + w - 2, h - 1) * C(rest, 2); path -= C(h + w - 4, h - 2) * (h + w - 3); path += C(h + w - 2, h + 1) * (h - 1) + C(h + w - 2, w + 1) * (w - 1); std::cout path ; int main() #ifdef ONLINE_JUDGE std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0);#endif init(); int t; std::cin t; while (t--) run(); D - Insert XOR 重要观察： B=(0,0)B=(0,0)B=(0,0) 可以生成全零序列 B=(1,0)/(0,1)B=(1,0)/(0,1)B=(1,0)/(0,1) 可以生成的序列满足： A1=1,Am=0A_1=1,A_m=0A1​=1,Am​=0 或者 A1=0,Am=1A_1=0,A_m=1A1​=0,Am​=1 中间不能出现连续的 000 B=(1,1)B=(1,1)B=(1,1) 可以生成的序列满足 A1=1,Am=1A_1=1,A_m=1A1​=1,Am​=1 中间不能出现连续的 000 所以我们观察到只要一段序列没有连续的 000，我们就可以把这段序列压缩成 222 个数。这提示我们需要记录所有长度 ≥2\\ge 2≥2 全 000 的连续段才能计算答案。 先考虑怎么计算答案，对应代码里的 compute() 函数。这些全 000 的连续段（记为 SiS_iSi​）将整个序列划分成几个 minor segments，这几个 minor segment 根据定义一定至少包含 111 个 111. 所以，对于这些夹在 Si,Si+1S_i,S_{i+1}Si​,Si+1​ 之间的 minor segments，我们只需要要用 111 个 111，表示这条线段；再用 (0,0)(0,0)(0,0) 表示 SiS_iSi​ 那么这些 minor segments 一定可以被 (1,0)(1,0)(1,0) 或者 (0,1)(0,1)(0,1) 生成出来。 接着考虑边界情况. 有可能在 S1S_1S1​ 的左侧还有若干个数字连续段，那么这些数字连续段必然要么是连续的 111，要么是单个 000. 我们只关心从前往后数第一个 111 的左侧是否还有 000 即可. 如果有的话，那么就说明必然是 a1=0,a2=1a_1=0,a_2=1a1​=0,a2​=1，就必须要 b1=0,b2=1b_1=0,b_2=1b1​=0,b2​=1 才能表示出 [a2,S1][a_2,S_1][a2​,S1​] 这一段；若没有，则必然 a1=1a_1=1a1​=1，这个时候只需要令 b1=1b_1=1b1​=1 即可生成 [a1,S1][a_1, S_1][a1​,S1​] 这段区间. 位于 SlastS_{last}Slast​ 右侧的也是同理 一个特殊情况是，没有 ≥2\\ge 2≥2 的全零连续段。根据我们刚刚分析的，序列必然是要么单个 000，要么是连续的 111，所以只要头尾不是 0,00,00,0 就只需要两个数，否则需要三个数 (0,1,0)(0,1,0)(0,1,0) 另一个特殊情况是全 111，这个时候答案就是 nnn. 接下考虑怎么维护这样的全 000 连续段。我们可以开一个 std::map[l] = r：若 ai=0a_i=0ai​=0，则尝试分裂区间；若 ai=1a_i=1ai​=1，则需要考虑是不是需要和两侧的 000 连续段合并起来。 时间复杂度 O(n+qlog⁡n)O(n+q\\log n)O(n+qlogn) AC Code #include bits/stdc++.h#define sz(x) int(x.size())constexpr int N = 2e5 + 10;int n, q;int a[N];std::mapint, int map; // [l, r]int single0;int compute() int ge2 = sz(map) - single; int ans = 0; if (ge2 0) ans += 2 * ge2; // 0s to represent 00...00 ans += ge2 - 1; // 1, to represent blocks between 2 len=2 zero blocks. auto b = map.begin(); if (a[1] == 0 a[2] == 1) ans += 2; // extra 0, extra 1 else if (b-first != 1) ans++; // extra 1 auto e = map.rbegin(); if (a[n] == 0 a[n - 1] == 1) ans += 2; else if (e-second != n) ans++; else if (single 0) // must contain 1; a[1] and a[2] cannot both be 0. ans = 2; ans += (a[1] == 0 a[n] == 0); else ans = n; // all 1s return ans;int main() #ifdef ONLINE_JUDGE std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0);#endif std::cin n; for (int i = 1; i = n; i++) std::cin a[i]; for (int i = 1, j; i = n;) j = i; while (j = n a[j] == a[i]) j++; if (a[i] == 0) map[i] = j - 1; if (j - i == 1) single++; i = j; std::cin q; for (int idx; q; q--) std::cin idx; // maintain if (a[idx] == 0) auto it = std::prev(map.upper_bound(idx)); auto [l, r] = *it; if (l == r) map.erase(it), single--; else if (l = idx - 1) map[l] = idx - 1, single += (l == idx - 1); else map.erase(l); if (idx + 1 = r) map[idx + 1] = r, single += (idx + 1 == r); else // 1 - 0 auto it = map.lower_bound(idx); if (idx != 1 a[idx - 1] == 0 idx != n a[idx + 1] == 0) auto R = map[idx + 1]; auto it = std::prev(map.upper_bound(idx - 1)); assert(it-second == idx - 1); map.erase(idx + 1); if (R == idx + 1) single--; if (it-first == idx - 1) single--; it-second = R; else if (idx != 1 a[idx - 1] == 0) // merge to left auto it = std::prev(map.upper_bound(idx)); if (it-second == it-first) single--; it-second = idx; else if (idx != n a[idx + 1] == 0) // merge to right auto it = map.lower_bound(idx); assert(it-first == idx + 1); if (it-second == it-first) single--; auto R = it-second; map.erase(it); map.insert(idx, R); else map.insert(idx, idx); single++; a[idx] = 1 - a[idx]; // compute std::cout compute() ;"},{"title":"上海 2015 OI 省选","path":"/wiki/algo_contests/SHOI2015.html","content":"[SHOI2015] 自动刷题机 一个比较重要的性质：n=tn=tn=t 时可以完成的题目数量，一定不少于 n=t+1n=t+1n=t+1 时能够完成的题目数量。令 f(x)=yf(x)=yf(x)=y 表示如果自动刷题机的 n=xn=xn=x，则一共可以完成 yyy 道题，则 f(x)f(x)f(x) 单调递减。 所以，我们需要找到 f(x)=kf(x)=kf(x)=k 的 xxx 的取值区间。所以我们可以使用二分找出区间的左右端点。 Code #include algorithm#include iostream#include vectorusing i64 = long long;int main() int n, k; std::cin n k; std::vectori64 all(n); i64 max = 0; for (int i = 0; i n; i++) std::cin all[i]; max = std::max(max, all[i]); auto check = [](i64 L) int cnt = 0; i64 lines = 0; for (auto v : all) lines += v; if (lines 0) lines = 0; if (lines = L) cnt++, lines = 0; return cnt; ; // find =k, =k-1 boundary as max L i64 l, r; i64 ans_r, ans_l; l = 0, r = 1e18 + 1; while (l + 1 r) i64 mid = l + ((r - l) 1); int res = check(mid); if (res = k) l = mid; else r = mid; ans_r = l; l = 0, r = 1e18 + 1; while (l + 1 r) i64 mid = l + ((r - l) 1); int res = check(mid); if (res k) l = mid; else r = mid; ans_l = r; if (check(ans_l) k || check(ans_r) k || check(ans_r) k || check(ans_l) k) std::cout -1 ; else std::cout ans_l ans_r ; [SHOI2015] 脑洞治疗仪 区间修改？区间最大子段？这不就是线段树吗！ 我们考虑线段树怎么做：为了维护区间最大子段（最长 000 子段），我们需要维护前缀 000 的长度、后缀 000 的长度、最长的中缀 000 的长度。区间修改的话，我们还需要懒标记。 接下来，我们来考察怎么进行 fill 操作。我们首先得知 [l0,r0][l_0,r_0][l0​,r0​] 区间有多少个 111 可以用于治疗（例如说有 ttt 个），然后删掉，接着再在 [l1,r1][l_1,r_1][l1​,r1​] 找到一个位置 ppp，使得 [l1,p][l_1,p][l1​,p] 之间恰好有 ttt 个位置可以填充。找 ppp 的过程可以通过二分结合线段树的区间查询做到 O(log⁡2n)O(\\log^2 n)O(log2n). 于是，操作 0/20/20/2 的单次复杂度为 O(log⁡n)O(\\log n)O(logn)，操作 111 的单次复杂度为 O(log⁡n)O(\\log n)O(logn)，整体时间复杂度为 O(mlog⁡2n)O(m\\log^2 n)O(mlog2n). Code 代码里使用 reset = 0/1 分别标记这个线段树节点所代表的区间应该全部被设置为 0/10/10/1. #include algorithm#include cstdio#include iostreamconstexpr int N = 2e5 + 5;struct Node // for queries int prefix0; int suffix0; int segment0; // for filling int length; int count1; // label; int reset; Node() : prefix0(0), suffix0(0), segment0(0), length(0), count1(0), reset(-1) T[N 2];int n, m;void pull_up(int rt) int ls = rt 1, rs = rt 1 | 1; T[rt].length = T[ls].length + T[rs].length; T[rt].count1 = T[ls].count1 + T[rs].count1; // maintain queries T[rt].prefix0 = T[ls].prefix0; if (T[ls].prefix0 == T[ls].length) T[rt].prefix0 += T[rs].prefix0; T[rt].suffix0 = T[rs].suffix0; if (T[rs].suffix0 == T[rs].length) T[rt].suffix0 += T[ls].suffix0; T[rt].segment0 = std::max(T[ls].segment0, T[rs].segment0, T[ls].suffix0 + T[rs].prefix0);void mark_zero(int root) T[root].reset = 0; T[root].count1 = 0; T[root].prefix0 = T[root].suffix0 = T[root].segment0 = T[root].length;void mark_one(int root) T[root].reset = 1; T[root].count1 = T[root].length; T[root].prefix0 = T[root].suffix0 = T[root].segment0 = 0;void push_down(int rt) if (T[rt].reset == -1) return; int ls = rt 1, rs = rt 1 | 1; if (T[rt].reset == 0) mark_zero(ls), mark_zero(rs); else if (T[rt].reset == 1) mark_one(ls), mark_one(rs); T[rt].reset = -1;// initialize to all 1svoid build(int rt = 1, int l = 1, int r = n) if (l == r) T[rt].length = 1, T[rt].count1 = 1; T[rt].segment0 = 0, T[rt].prefix0 = 0, T[rt].suffix0 = 0; T[rt].reset = -1; return; int mid = l + ((r - l) 1); build(rt 1, l, mid); build(rt 1 | 1, mid + 1, r); pull_up(rt);// make [L, R] all 0svoid remove(int L, int R, int rt = 1, int l = 1, int r = n) if (L r or R l) return; // no overlap if (L = l and r = R) mark_zero(rt); return; int mid = l + ((r - l) 1); push_down(rt); remove(L, R, rt 1, l, mid); remove(L, R, rt 1 | 1, mid + 1, r); pull_up(rt);void set_one(int L, int R, int rt = 1, int l = 1, int r = n) if (L r or R l) return; // no overlap if (L = l and r = R) mark_one(rt); return; int mid = l + ((r - l) 1); push_down(rt); set_one(L, R, rt 1, l, mid); set_one(L, R, rt 1 | 1, mid + 1, r); pull_up(rt);int query_sum(int L, int R, int rt = 1, int l = 1, int r = n) if (L r or R l) return 0; if (L = l and r = R) return T[rt].count1; int mid = l + ((r - l) 1); push_down(rt); return query_sum(L, R, rt 1, l, mid) + query_sum(L, R, rt 1 | 1, mid + 1, r);int query_empty(int L, int R, int rt = 1, int l = 1, int r = n) if (L r or R l) return 0; if (L = l and r = R) return T[rt].length - T[rt].count1; int mid = l + ((r - l) 1); push_down(rt); return query_empty(L, R, rt 1, l, mid) + query_empty(L, R, rt 1 | 1, mid + 1, r);Node query_segment(int L, int R, int rt = 1, int l = 1, int r = n) if (L r or R l) return Node(); if (L = l and r = R) return T[rt]; int mid = l + ((r - l) 1); push_down(rt); auto left = query_segment(L, R, rt 1, l, mid); auto right = query_segment(L, R, rt 1 | 1, mid + 1, r); Node res; res.length = left.length + right.length; res.count1 = left.count1 + right.count1; res.prefix0 = left.prefix0; if (left.prefix0 == left.length) res.prefix0 += right.prefix0; res.suffix0 = right.suffix0; if (right.suffix0 == right.length) res.suffix0 += left.suffix0; res.segment0 = std::max(left.segment0, right.segment0, left.suffix0 + right.prefix0); return res;int main() std::cin n m; build(); int op, l0, r0, l1, r1; while (m--) std::cin op; if (op == 0) std::cin l0 r0; remove(l0, r0); else if (op == 1) std::cin l0 r0 l1 r1; int sum = query_sum(l0, r0); remove(l0, r0); int empty = query_empty(l1, r1); int target = std::min(empty, sum); // if (target = 0) continue; int l = l1 - 1, r = r1 + 1; while (l + 1 r) int mid = l + ((r - l) 1); if (query_empty(l1, mid) = target) l = mid; else r = mid; if (l = l1) set_one(l1, l); else std::cin l0 r0; auto seg = query_segment(l0, r0); std::cout seg.segment0 ; 这道题应该还有线段树上二分的做法，就是替换掉二分的过程。改日再研究一下~ [SHOI2015] 超能粒子炮·改 看到模数非常小就想到了卢卡斯定理，此题的模数 233323332333 还是素数，我们只需要用朴素的卢卡斯定理即可，即 下面的除号都是下取整的意思。 (nk)≡(n/pk/p)(n%pk%p)(modp) \\binom{n}{k}\\equiv\\binom{n/p}{k/p}\\binom{n\\%p}{k\\%p}\\pmod p (kn​)≡(k/pn/p​)(k%pn%p​)(modp)然而我们要求的却是一个和式 ∑i=0k(ni)(modp)\\sum_{i=0}^k \\binom{n}{i} \\pmod p∑i=0k​(in​)(modp). 我们考虑怎么拆开这个式子，一个核心的想法就是尽量往卢卡斯定理的形式上靠。 （以下的推导略去 (modp)\\pmod p(modp)） ∑i=0k(ni)=∑i=0k(n/pi/p)(n%pi%p) \\sum_{i=0}^k \\binom{n}{i}=\\sum_{i=0}^k \\binom{n/p}{i/p}\\binom{n\\%p}{i\\%p} i=0∑k​(in​)=i=0∑k​(i/pn/p​)(i%pn%p​)然后我们发现：诶！好像 i%pi\\% pi%p 的取值只有 ppp 个，而且 n%pn\\% pn%p 还是定值！所以我们考虑把 (n/pi/p)\\binom{n/p}{i/p}(i/pn/p​) 按照 i%pi\\% pi%p 进行分类： ∑i=0k(n/pi/p)(n%pi%p)=∑r=0p−1((n%pr)⋅∑i=jp+r(n/pi/p))+(n/pk/p)∑i=0k%p(n%pi) \\begin{aligned} \\sum_{i=0}^k \\binom{n/p}{i/p}\\binom{n\\%p}{i\\%p}\\\\ =\\sum_{r=0}^{p-1} \\Bigg(\\binom{n\\% p}{r}\\cdot\\sum_{i=jp+r}\\binom{n/p}{i/p}\\Bigg)+\\binom{n/p}{k/p}\\sum_{i=0}^{k\\% p}\\binom{n\\% p}{i} \\end{aligned} =​i=0∑k​(i/pn/p​)(i%pn%p​)r=0∑p−1​((rn%p​)⋅i=jp+r∑​(i/pn/p​))+(k/pn/p​)i=0∑k%p​(in%p​)​在这个式子里，我们把 kkk 个组合数分成两个部分: 一个是 [0,k/p⋅p−1][0, k/p \\cdot p-1][0,k/p⋅p−1]，这一部分我们可以完整地分成 ppp 组，每组 k/pk/pk/p 个。 我们继续考察前半部分这个和式。我们可以写成两个和式的乘积： (∑r=0p−1(n%pr))⋅(∑i=jp+r(n/pi/p)) \\Bigg(\\sum_{r=0}^{p-1} \\binom{n\\% p}{r}\\Bigg)\\cdot\\Bigg( \\sum_{i=jp+r}\\binom{n/p}{i/p} \\Bigg) (r=0∑p−1​(rn%p​))⋅(i=jp+r∑​(i/pn/p​)) 接着考察后半个和式 i/pi/pi/p 的取值，我们发现，其实 jjj 的取值就是 [0,k/p−1][0, k/p-1][0,k/p−1]. 另一部分则是剩余的组合数 k/p⋅p∼kk/p\\cdot p\\sim kk/p⋅p∼k，他们无法按余数分成 ppp 组，所以单独计算。 但是虽然无法按余数分组，但是他们的除以 ppp 的商是一样的，所以我们在这里按商 (n/pk/p)\\binom{n/p}{k/p}(k/pn/p​) 合并. 所以，上面这个式子可以写作： ∑i=0k(ni)=(∑r=0p−1(n%pr))⋅(∑j=0k/p−1(n/pj))+(n/pk/p)∑i=0k%p(n%pi) \\sum_{i=0}^k \\binom{n}{i}=\\Bigg(\\sum_{r=0}^{p-1} \\binom{n\\% p}{r}\\Bigg)\\cdot\\Bigg( \\sum_{j=0}^{k/p-1}\\binom{n/p}{j} \\Bigg)+\\binom{n/p}{k/p}\\sum_{i=0}^{k\\% p}\\binom{n\\% p}{i} i=0∑k​(in​)=(r=0∑p−1​(rn%p​))⋅(j=0∑k/p−1​(jn/p​))+(k/pn/p​)i=0∑k%p​(in%p​)然后我们就发现了形式上相似的地方：记 f(x,y)=∑i=0y(xi)f(x,y)=\\sum_{i=0}^y \\binom{x}{i}f(x,y)=∑i=0y​(ix​)，则有 f(n,k)=f(n%p,p−1)⋅f(n/p,k/p−1)+(n/pk/p)⋅f(n%p,k%p) f(n,k)=f(n\\% p,p-1)\\cdot f(n/p,k/p-1)+\\binom{n/p}{k/p}\\cdot f(n\\% p, k\\%p) f(n,k)=f(n%p,p−1)⋅f(n/p,k/p−1)+(k/pn/p​)⋅f(n%p,k%p)这就是 DP 递推啊！而且进一步的，f(n%p,p−1)f(n\\% p,p-1)f(n%p,p−1) 和 f(n%p,k%p)f(n\\% p, k\\%p)f(n%p,k%p) 的状态数量只有 p2p^2p2 个，(n/pk/p)\\binom{n/p}{k/p}(k/pn/p​) 可以使用卢卡斯定理计算，f(n/p,k/p−1)f(n/p,k/p-1)f(n/p,k/p−1) 则可以递归计算。 卢卡斯定理的计算过程是 n,kn,kn,k 同步 /p/p/p，因此时间复杂度是 O(log⁡n)O(\\log n)O(logn) 的。如果令 f(x,y),x≤p,y≤pf(x,y),x\\le p,y\\le pf(x,y),x≤p,y≤p 进行 O(p2)O(p^2)O(p2) 预处理计算，令计算 f(n,k)f(n,k)f(n,k) 的复杂度为 T(x)T(x)T(x) （这里 n,kn,kn,k 的变化是同步除以 ppp，因此就使用一元了），则有 T(x)=O(1)⋅T(x/p)+O(log⁡n)⋅O(1)T(x)=O(log⁡2n) T(x)=O(1)\\cdot T(x/p) + O(\\log n)\\cdot O(1)\\\\ T(x)=O(\\log^2 n) T(x)=O(1)⋅T(x/p)+O(logn)⋅O(1)T(x)=O(log2n)因此算上预处理，这道题的时间复杂度为 O(p2+Tlog⁡2n)O(p^2+T\\log^2 n)O(p2+Tlog2n). Code #include iostreamusing i64 = long long;constexpr int P = 2333;// f(n, m) = sum_i=0^m C(n, i) (mod p)int fpow(i64 b, i64 p) int res = 1; while (p) if (p 1) res = res * b % P; b = b * b % P; p = 1; return res;int C[P + 2][P + 2];int F[P + 2][P + 2];void init() for (int i = 1; i = P; ++i) C[0][i] = 0; C[0][0] = 1; for (int i = 1; i = P; ++i) for (int j = 0; j = P; ++j) C[i][j] = 0; C[i][0] = C[i][i] = 1; for (int j = 1; j i; ++j) C[i][j] = (C[i - 1][j - 1] + C[i - 1][j]) % P; // init F[][] for (int i = 0; i = P; i++) F[i][0] = 1; for (int row = 0; row = P; row++) for (int col = 1; col = P; col++) F[row][col] = (F[row][col - 1] + C[row][col]) % P; int lucas(i64 n, i64 k) if (k == 0 || n == k) return 1; if (n k) return 0; return C[n % P][k % P] * lucas(n / P, k / P) % P;int f(i64 n, i64 k) if (n = P and k = P) return F[n][k]; return (F[n % P][P - 1] * f(n / P, k / P - 1) % P + lucas(n / P, k / P) * F[n % P][k % P] % P) % P;void run() i64 n, k; std::cin n k; std::cout f(n, k) ;int main() init(); int T; std::cin T; while (T--) run(); [SHOI2015] 聚变反应炉 656565 分想法：因为 ci=0c_i=0ci​=0 时，我们总是必须取所有点，答案为 ∑di\\sum d_i∑di​，因此有 cic_ici​ 时，考虑总是贪心选择点，使得其能够减少最多的能量消耗。 看到当 c≤5c\\le 5c≤5 时，n≤2000n\\le 2000n≤2000，思考是不是可能是 O(n2)O(n^2)O(n2) 的算法。由于前 505050 分我们使用的是贪心算法，所以这次我们考虑能否 DP. 树形 DP 我们通常可以考虑将 dp[u] 设置为某个与其子树相关的状态。这里，我们令 dp[u] 表示把 uuu 子树全部激活的最小 cost. 然后我们就能发现一个问题：对于 uuu 的儿子 viv_ivi​，我们可以一部分 viv_ivi​（以及其子树）先激活，再激活 uuu，再激活剩余的 vjv_jvj​（以及其子树）. 考虑把“先于 uuu 激活的子树”、“后于 uuu 激活的子树”都视为物品，于是我们发现，这是一个 0/10/10/1 背包！ 那么背包的容量是什么呢？考虑到若选取“先于 uuu 激活的子树 vvv”这个物品，则 d[u]d[u]d[u] 可以减免 c[v]c[v]c[v] 的能量点，这一个形式就比较像背包容量。因此，我们可以令 cap[u]=min⁡(d[u],∑v∈u.sonc[v])cap[u]=\\min(d[u], \\sum\\limits_{v\\in u.son} c[v])cap[u]=min(d[u],v∈u.son∑​c[v]) 作为背包容量。那么顺理成章地，我们就令 c[v]c[v]c[v] 作为“先于 uuu 激活的子树 vvv” 的 cost，“后于 uuu 激活的子树 vvv” 的 cost 视作 000. 顺便也改一下状态：令 dp[u, c] 表示如果 uuu 从儿子节点接受了总计 ccc 点能量的辐射，那么还需要 dp[u,c] 点能量才能激活整棵子树。 然后考察每一个物品的价值，我们希望最后背包的价值最小。先考虑“先于 uuu 激活的子树 vvv”，其激活不受到 uuu 的辐射，因此其价值就是 Uv,0=min⁡c≤cap[v]dp[v,c] U_{v,0}=\\min_{c\\le cap[v]} \\texttt{dp[v,c]} Uv,0​=c≤cap[v]min​dp[v,c]对于“后于 uuu 激活的子树 vvv”而言，从 uuu 能传到 vvv 的能量，首先不超过 c[u]c[u]c[u]，其次，还要考虑 vvv 的儿子辐射给 vvv 的能量（=d[v]−cost=d[v]-cost=d[v]−cost），因此为 min⁡cost≤cap[v](c[u],d[v]−cost)\\min_{cost\\le cap[v]}(c[u], d[v]-cost)mincost≤cap[v]​(c[u],d[v]−cost)，因此价值为激活子树所需能量去掉从 uuu 传过来的能量，即 Uv,1=min⁡cost≤cap[v](dp[v,cost]-min⁡(c[u],d[v]-cost)) U_{v,1}=\\min_{\\texttt{cost}\\le \\texttt{cap[v]}}\\Big(\\texttt{dp[v,cost]-\\(\\min(\\)c[u],d[v]-cost\\()\\)}\\Big) Uv,1​=cost≤cap[v]min​(dp[v,cost]-min(c[u],d[v]-cost))然后就是简单的 0/10/10/1 背包问题了。后 505050 分的时间复杂度为 O(cn2)O(cn^2)O(cn2). Code #include algorithm#include cassert#include iostream#include numeric#include queue#include set#include utility#include vectorusing vi = std::vectorint;using vvi = std::vectorvi;using pii = std::pairint, int;int n;void case0(vi d, vi c) assert(*std::ranges::max_element(c) == 0); std::cout std::accumulate(d.begin(), d.end(), 0) ;void case1(vi d, vi c, vvi G) vi deg(d.size(), 0); for (int u = 1; u = n; u++) deg[u] = G[u].size(); std::setpii, std::greaterpii S; std::queueint Q; for (int u = 1; u = n; u++) S.insert(deg[u] * c[u], u); int ans = 0; vi vis(n + 1, false); while (!S.empty()) auto [reduced, t] = *S.begin(); S.erase(S.begin()); Q.push(t); ans += d[t]; d[t] = 0; while (!Q.empty()) int u = Q.front(); Q.pop(); if (vis[u]) continue; vis[u] = true; for (auto v : G[u]) if (d[v] = 0) continue; d[v] -= c[u]; S.erase(deg[v] * c[v], v); deg[v]--; S.insert(deg[v] * c[v], v); if (d[v] = 0) Q.push(v); std::cout ans ;constexpr int inf = 1e9;constexpr int maxc = 1e4 + 5;void case2(vi d, vi c, vvi G) vvi f(n + 1, vi(maxc, inf)); vi cap(n + 1, 0); auto dfs = [](auto F, int u, int fa) - void f[u][0] = d[u]; int totcap = 0; for (int v : G[u]) if (v == fa) continue; totcap += c[v]; F(F, v, u); cap[u] = std::min(d[u], totcap); for (int v : G[u]) if (v == fa) continue; int up = inf, down = inf; // up: u - fa, down: fa - u for (int i = 0; i = cap[v]; i++) up = std::min(up, f[v][i]); down = std::min(down, f[v][i] - std::min(c[u], d[v] - i)); vi tmp(maxc, inf); for (int i = 0; i = cap[u]; i++) if (f[u][i] == inf) continue; tmp[i] = std::min(tmp[i], f[u][i] + down); int next_cap = std::min(cap[u], i + c[v]); tmp[next_cap] = std::min(tmp[next_cap], f[u][i] + up - std::min(c[v], d[u] - i)); for (int i = 0; i = cap[u]; i++) f[u][i] = tmp[i]; ; dfs(dfs, 1, 0); auto ans = *std::ranges::min_element(f[1]); std::cout ans ;int main() std::cin n; vi d(n + 1, 0), c(n + 1, 0); vvi G(n + 1); for (int i = 1; i = n; i++) std::cin d[i]; for (int i = 1; i = n; i++) std::cin c[i]; for (int i = 1; i n; i++) int u, v; std::cin u v; G[u].push_back(v); G[v].push_back(u); if (*std::ranges::max_element(c) == 0) case0(d, c); else if (*std::ranges::max_element(c) == 1) case1(d, c, G); else case2(d, c, G); [SHOI2015] 激光发生器 计算几何题，但是细节有点多…… 因为只要反射十次，而且只有 100100100 面镜子，所以我们可以直接模拟光线轨迹。 每次模拟中，我们计算当前射线与每一面镜子的交点（要保证在线段上），那么距离射线起点最近的就是光线的反射点。 然后取出镜子的法向量，注意要和光线在镜子的同一侧（可以使用点积判断）。计算法向量和入射光线的夹角，乘上 λ\\lambdaλ 就是反射角，那么就直接旋转法向量即可。当然这里要判断一下应该逆时针旋转还是顺时针旋转。 还有要注意的点是，计算夹角时要考虑浮点数误差，即先对 cos⁡\\coscos 的值取 clamp(-1, 1) 然后再 std::acos() 比较稳妥。 Code #include algorithm#include cassert#include cmath#include iostream#include vectorconstexpr int N = 110;constexpr double EPS = 1e-9;struct pvec double x, y; pvec operator+(const pvec other) const return x + other.x, y + other.y; pvec operator-(const pvec other) const return x - other.x, y - other.y; pvec operator*(double t) const return x * t, y * t; ;struct segment pvec s, d, e; double lambda;;int n;segment seg[N], current;int vis[N];std::vectorint order;//! methods// -1 if a b, 0 if a == b, 1 if a bint sign(double a, double b);double cross(const pvec , const pvec );double dot(const pvec , const pvec );double dist(const pvec , const pvec );double angle(const pvec , const pvec );pvec normalize(const pvec );pvec rotate_anticlock(const pvec , const double );bool do_intersect(const segment , const segment );bool on_segment(const pvec , const segment );// assuming all lines, does not check parallelpvec intersect(const segment , const segment );int main() std::cin current.s.x current.s.y current.d.x current.d.y; std::cin n; for (int i = 1; i = n; i++) double a, b; std::cin seg[i].s.x seg[i].s.y seg[i].e.x seg[i].e.y a b; seg[i].lambda = a / b; seg[i].d.x = seg[i].e.x - seg[i].s.x; seg[i].d.y = seg[i].e.y - seg[i].s.y; // iteration std::vectorstd::pairpvec, int pts; for (int _ = 1; _ = 10; _++) pts.clear(); for (int i = 1; i = n; i++) if (sign(cross(seg[i].d, current.d), 0) == 0) continue; // parallel auto res = intersect(current, seg[i]); if (do_intersect(current, seg[i])) pts.push_back(res, i); if (pts.empty()) break; std::sort(pts.begin(), pts.end(), [](const auto a, const auto b) return dist(a.first, current.s) dist(b.first, current.s); ); int idx = pts[0].second; order.push_back(idx), vis[idx] = true; pvec res = pts[0].first; pvec input_d = normalize(current.d); pvec reverse_d = -input_d.x, -input_d.y; // reverse direction pvec mirror_base = normalize(seg[idx].d); pvec normal_d = mirror_base.y, -mirror_base.x; // rotate 90 degrees if (dot(input_d, normal_d) 0) normal_d = normal_d * (-1); // 同侧 double cosA = dot(reverse_d, normal_d); assert(sign(cosA, -1) = 0 sign(cosA, 1) = 0); // cosA should be in [-1, 1] double alpha = std::acos(std::max(-1.0, std::min(1.0, cosA))); double beta = seg[idx].lambda * alpha; // angle of reflection pvec new_d; if (cross(normal_d, reverse_d) 0) new_d = rotate_anticlock(normal_d, beta); else new_d = rotate_anticlock(normal_d, -beta); res = res + new_d * 1e-5; // move a bit forward to avoid numerical issues current.s = res; current.d = new_d; current.e = current.s.x + current.d.x, current.s.y + current.d.y; if (order.empty()) std::cout NONE ; else for (auto v : order) std::cout v ; std::cout ; int sign(double a, double b) if (a b - EPS) return -1; if (a b + EPS) return 1; return 0;double cross(const pvec a, const pvec b) return a.x * b.y - a.y * b.x; double dot(const pvec a, const pvec b) return a.x * b.x + a.y * b.y; double dist(const pvec a, const pvec b) return std::sqrt((a.x - b.x) * (a.x - b.x) + (a.y - b.y) * (a.y - b.y)); double angle(const pvec a, const pvec b) double d = dot(a, b); double m = std::sqrt(dot(a, a) * dot(b, b)); return std::acos(d / m);pvec normalize(const pvec v) double len = std::sqrt(dot(v, v)); return v.x / len, v.y / len;pvec rotate_anticlock(const pvec v, const double ang) double cosa = std::cos(ang); double sina = std::sin(ang); return v.x * cosa - v.y * sina, v.x * sina + v.y * cosa;bool on_segment(const pvec p, const segment seg) pvec v1 = seg.d; pvec v2 = p.x - seg.s.x, p.y - seg.s.y; pvec v3 = p.x - seg.e.x, p.y - seg.e.y; return sign(cross(v1, v2), 0) == 0 sign(dot(v2, v3), 0) = 0;bool do_intersect(const segment lazer, const segment seg) pvec pt = intersect(lazer, seg); pvec dvec = pt.x - lazer.s.x, pt.y - lazer.s.y; return on_segment(pt, seg) sign(dot(dvec, lazer.d), 0) = 0;pvec intersect(const segment a, const segment b) double div = cross(b.d, a.d); pvec dif = a.s.x - b.s.x, a.s.y - b.s.y; double t = cross(dif, b.d) / div; return a.s.x + a.d.x * t, a.s.y + a.d.y * t; [SHOI2015] 零件组装机 排除掉不连通、自环、重边的情况后，对于一张连通图 GGG，拿出和 000 相邻的节点 {v}\\{v\\}{v}，考察一下 sizesizesize 可能的取值。根据题意我们有 size≤⌊n2⌋size\\le \\lfloor \\frac{n}{2} \\rfloorsize≤⌊2n​⌋，这是题目规定 由于和 000 邻接，所以对于 vi≥size∧vi≡0(modsize)v_i\\ge size \\land v_i\\equiv 0\\pmod {size}vi​≥size∧vi​≡0(modsize) 的节点来说，都应该和 000 邻接。 那么，只要我们找到了这样的 sizesizesize 之后，就可以把这两部分之间的边全部断开，形成两张导出子图，再递归判断即可。 于是我们的任务变成了，怎么找到这样的 sizesizesize. 首先可以想到，sizesizesize 应该至多是 ≥size\\ge size≥size 且与 000 邻接的 viv_ivi​ 的最大公约数。例如，G[0]={1,2,4,6,8}G[0]=\\{1,2,4,6,8\\}G[0]={1,2,4,6,8}，那么 size≤4size \\le 4size≤4，如果 size=4size=4size=4，则 4,6,8=0(mod4)4,6,8=0\\pmod 44,6,8=0(mod4) 而这显然不对，模数应该是 222 才能让这 333 个数和 000 连边，于是 (4,6,8)=2(4,6,8)=2(4,6,8)=2. 所以为了让 4=6=8=0(modp)4=6=8=0 \\pmod p4=6=8=0(modp)，那么必须有 p∣(4,6,8)p|(4,6,8)p∣(4,6,8). 那么 sizesizesize 可能是 ppp 的约数吗？我们可以断言不可能，因为如果 sizepsize\\lt psizep 的话，那么 4+size64+size\\lt 64+size6 也应该和 000 连边，但是并没有，所以我们只需要检验 size=p=2size=p=2size=p=2 的情况，也就是 v[i]=suffix_gcd[i]v[i]=\\texttt{suffix\\_gcd}[i]v[i]=suffix_gcd[i]，然后 O(nlog⁡n)O(n\\log n)O(nlogn) 检查是否正确连边、以及删去边，这一点可以用 std::vectorstd::setint 进行维护。总时间复杂度为 O(nlog⁡2n)O(n\\log^2 n)O(nlog2n) Code #include iostream#include numeric#include set#include vectorconstexpr int MAXN = 100005;using vi = std::vectorint;using si = std::setint;struct UnionFind vi fa, size; UnionFind(int n) : fa(n), size(n, 1) for (int i = 0; i n; ++i) fa[i] = i; int root(int u) return u == fa[u] ? u : fa[u] = root(fa[u]); bool same(int u, int v) return root(u) == root(v); void merge(int u, int v) int fu = root(u), fv = root(v); if (fu == fv) return; if (size[fu] size[fv]) std::swap(fu, fv); fa[fv] = fu; size[fu] += size[fv]; bool check_connected() bool ok = true; for (int i = 1; i fa.size(); ++i) if (!same(i, 0)) ok = false; break; return ok; ;bool dfs(std::vectorsi G, int L, int R) if (L == R) return true; int length = R - L + 1; int max_size = length / 2; int begin = L; vi nodes; for (auto v : G[begin]) if (v = L v = R) nodes.push_back(v - L); if (nodes.empty()) return false; int size = nodes.size(); vi suffixgcd(size); suffixgcd[size - 1] = nodes[size - 1]; for (int i = size - 2; i = 0; i--) suffixgcd[i] = std::gcd(suffixgcd[i + 1], nodes[i]); int c_size = -1; for (int i = size - 1; i = 0; i--) if (suffixgcd[i] == nodes[i] nodes[i] = max_size) c_size = nodes[i]; break; if (c_size 1) return false; // remove edges for (int u = L + c_size; u = R; u++) if (!G[u].contains(L + (u - L) % c_size)) return false; // no such edge G[u].erase(L + (u - L) % c_size); G[L + (u - L) % c_size].erase(u); for (int u = L; u L + c_size; u++) if (G[u].empty()) continue; if (*G[u].rbegin() = L + c_size) return false; for (int u = L + c_size; u = R; u++) if (G[u].empty()) continue; if (*G[u].begin() L + c_size) return false; return dfs(G, L, L + c_size - 1) dfs(G, L + c_size, R);void run() int n, m; std::cin n m; UnionFind uf(n); std::vectorsi G(n); bool is_valid = true; for (int i = 0, u, v; i m; i++) std::cin u v; if (u == v || G[u].contains(v)) is_valid = false; G[u].insert(v); G[v].insert(u); uf.merge(u, v); is_valid = uf.check_connected(); if (!is_valid) return void(std::cout NO ); else std::cout (dfs(G, 0, n - 1) ? YES : NO );int main() std::cin.tie(nullptr)-sync_with_stdio(false); int T; std::cin T; while (T--) run();"},{"title":"AtCoder Regular Contest 198 (Div. 2)","path":"/wiki/algo_contests/arc198.html","content":"A. I hate 1 首先 111 不可能出现在 SSS 里；其次，k,k+1k,k+1k,k+1 不可能同时出现在 SSS 中。所以 ∣S∣≤⌊n2⌋|S|\\le \\lfloor\\frac{n}{2}\\rfloor∣S∣≤⌊2n​⌋. 发现取偶数的时候刚好取到 n/2n/2n/2. 一个坑是 n=1n=1n=1 时，S={1}S=\\{1\\}S={1}，因为此时无法组成任何 pair Code n = int(input())if n == 1: print(1) print(1)else: print(n // 2) print( .join([str(i) for i in range(2, n + 1, 2)])) B. Rivalry 我们考虑怎么构造一个合法解。我们先全部写上 000，因为在题目条件下，相当于 000 可以任务放置。 然后放 111，由于每一个 111 两边有且只有一个 000，我们先满足 ≥1\\ge 1≥1 个 000：在每个 000 的两侧各放一个 111。此时，我们可以推出第一个条件：如果 c12c0c_1\\gt 2c_0c1​2c0​，那么必定有一个 111 夹在两个 111 的中间，不符合条件，所以必然有 c1≤2c0\\boxed{c_1\\le 2c_0}c1​≤2c0​​. 然后再考察插入 222。因为每一个 111 必须紧邻一个 000，因此 222 不能插入 101010 之间，只能插入 111111 之间。一共有 c0c_0c0​ 个 111111 可以插入。故得到第二个条件：c2≤c0\\boxed{c_2\\le c_0}c2​≤c0​​ 然后我们考虑 222 不足的情况，比如说 000 个 222。我们可能得到这样一个情况： 101 101 101 …10_ 101\\ 101\\ 101\\ \\dots 10\\_ 101 101 101 …10_此时注意到第一个 111 其两边都是 000，而 111 又全部插入完毕，因此必须有一个 222 插入。所以若 c1=1(mod2)c_1=1\\pmod 2c1​=1(mod2)，就必须有 c2≥1c_2\\ge 1c2​≥1；否则，若 c1=0(mod2)c_1=0\\pmod 2c1​=0(mod2)，我们可以构造出这样的结构：0110110110110…0110110110110\\dots0110110110110… 一定满足条件。所以第三个条件是：c1=0(mod2)∨c2≥1c_1=0\\pmod 2\\lor c_2\\ge 1c1​=0(mod2)∨c2​≥1 故总时间复杂度为 O(T)O(T)O(T). Code def solve(): x, y, z = map(int, input().split()) if y = 2 * x and z = x and (y % 2 == 0 or z = 1): print(Yes) else: print(No)for _ in range(int(input())): solve() C. Error Swap 看到将 AAA 数列变成 BBB 数列，我们考虑数列变换模型，考虑能否用题目的操作，操作出 ai−1,aj+1a_i-1,a_j+1ai​−1,aj​+1 的形式。 手推一下，发现可以用 444 步操作模拟。具体如下： 任意两个数之间一加一减都可以用 444 次操作完成 Prefix Dec-IncIndexijkA[Index]xyzswap(i,k)z−1yx+1swap(i,j)y−1zx+1swap(i,k)xzyswap(j,k)xy−1z+1 \\text{Prefix Dec-Inc}\\\\ \\begin{array}{c|cccc} \\text{Index}ijk\\\\ \\text{A[Index]}xyz\\\\ \\hline \\texttt{swap(i,k)}z-1yx+1\\\\ \\texttt{swap(i,j)}y-1zx+1\\\\ \\texttt{swap(i,k)}xzy\\\\ \\texttt{swap(j,k)}xy-1z+1\\\\ \\end{array} Prefix Dec-IncIndexA[Index]swap(i,k)swap(i,j)swap(i,k)swap(j,k)​ixz−1y−1xx​jyyzzy−1​kzx+1x+1yz+1​​Prefix Inc-DecIndexijkA[Index]xyzswap(j,k)xz−1y+1swap(i,k)yz−1x+1swap(i,j)z−2y+1x+1swap(i,k)xy+1z−1 \\text{Prefix Inc-Dec}\\\\ \\begin{array}{c|cccc} \\text{Index}ijk\\\\ \\text{A[Index]}xyz\\\\ \\hline \\texttt{swap(j,k)}xz-1y+1\\\\ \\texttt{swap(i,k)}yz-1x+1\\\\ \\texttt{swap(i,j)}z-2y+1x+1\\\\ \\texttt{swap(i,k)}xy+1z-1\\\\ \\end{array} Prefix Inc-DecIndexA[Index]swap(j,k)swap(i,k)swap(i,j)swap(i,k)​ixxyz−2x​jyz−1z−1y+1y+1​kzy+1x+1x+1z−1​​Suffix Inc-DecIndexijkA[Index]xyzswap(i,j)y−1x+1zswap(j,k)y−1z−1x+2swap(i,k)x+1z−1yswap(j,k)x+1y−1z \\text{Suffix Inc-Dec}\\\\ \\begin{array}{c|cccc} \\text{Index}ijk\\\\ \\text{A[Index]}xyz\\\\ \\hline \\texttt{swap(i,j)}y-1x+1z\\\\ \\texttt{swap(j,k)}y-1z-1x+2\\\\ \\texttt{swap(i,k)}x+1z-1y\\\\ \\texttt{swap(j,k)}x+1y-1z\\\\ \\end{array} Suffix Inc-DecIndexA[Index]swap(i,j)swap(j,k)swap(i,k)swap(j,k)​ixy−1y−1x+1x+1​jyx+1z−1z−1y−1​kzzx+2yz​​Suffix Dec-IncIndexijkA[Index]xyzswap(j,k)xz−1y+1swap(i,k)yz−1x+1swap(j,k)yxzswap(i,j)x−1y+1z \\text{Suffix Dec-Inc}\\\\ \\begin{array}{c|cccc} \\text{Index}ijk\\\\ \\text{A[Index]}xyz\\\\ \\hline \\texttt{swap(j,k)}xz-1y+1\\\\ \\texttt{swap(i,k)}yz-1x+1\\\\ \\texttt{swap(j,k)}yxz\\\\ \\texttt{swap(i,j)}x-1y+1z\\\\ \\end{array} Suffix Dec-IncIndexA[Index]swap(j,k)swap(i,k)swap(j,k)swap(i,j)​ixxyyx−1​jyz−1z−1xy+1​kzy+1x+1zz​​Mid Dec-IncIndexijkA[Index]xyzswap(i,k)z−1yx+1swap(j,k)z−1xy+1swap(i,j)x−1zy+1swap(j,k)x−1yz+1 \\text{Mid Dec-Inc}\\\\ \\begin{array}{c|cccc} \\text{Index}ijk\\\\ \\text{A[Index]}xyz\\\\ \\hline \\texttt{swap(i,k)}z-1yx+1\\\\ \\texttt{swap(j,k)}z-1xy+1\\\\ \\texttt{swap(i,j)}x-1zy+1\\\\ \\texttt{swap(j,k)}x-1yz+1\\\\ \\end{array} Mid Dec-IncIndexA[Index]swap(i,k)swap(j,k)swap(i,j)swap(j,k)​ixz−1z−1x−1x−1​jyyxzy​kzx+1y+1y+1z+1​​Mid Inc-DecIndexijkA[Index]xyzswap(i,j)y−1x+1zswap(j,k)y−1z−1x+2swap(i,j)z−2yx+2swap(i,k)x+1yz−1 \\text{Mid Inc-Dec}\\\\ \\begin{array}{c|cccc} \\text{Index}ijk\\\\ \\text{A[Index]}xyz\\\\ \\hline \\texttt{swap(i,j)}y-1x+1z\\\\ \\texttt{swap(j,k)}y-1z-1x+2\\\\ \\texttt{swap(i,j)}z-2yx+2\\\\ \\texttt{swap(i,k)}x+1yz-1\\\\ \\end{array} Mid Inc-DecIndexA[Index]swap(i,j)swap(j,k)swap(i,j)swap(i,k)​ixy−1y−1z−2x+1​jyx+1z−1yy​kzzx+2x+2z−1​​ 有了这一点之后，我们考虑贪心算法：每次 ai≠bia_i e b_iai​=bi​ 时，都对 ai,ai+1a_i,a_{i+1}ai​,ai+1​ 进行加减操作，直到 ai=bia_i=b_iai​=bi​ 为止。但是这样的话总操作次数可能来到 4N24N^24N2 可能会爆。因此一个优化是，对于 aibia_i\\gt b_iai​bi​，找一个 jjj 使得 ajbj∧j=arg max⁡ki∣aj−bj∣a_j\\lt b_j\\land j=\\argmax_{k\\gt i}|a_j-b_j|aj​bj​∧j=argmaxki​∣aj​−bj​∣；对于 aibia_i\\lt b_iai​bi​ 也是类似。这个剪枝就可以通过此题了。 另外值得注意的是要特殊考察 N=1,2N=1,2N=1,2 时的情况。N=2N=2N=2 时，b[]b[]b[] 只有两种情况是 OK 的，(b0,b1)=(a0,a1)/(a1−1,a0+1)(b_0,b_1)=(a_0,a_1)/(a_1-1,a_0+1)(b0​,b1​)=(a0​,a1​)/(a1​−1,a0​+1). 特判即可。 Code #include cassert#include iostream#include iterator#include numeric#include set#include utility#include vector#define all(x) (x).begin(), (x).end()using vi = std::vectorint;using pii = std::pairint, int;int main() int n; std::cin n; std::vectorint a(n), b(n); for (auto x : a) std::cin x; for (auto x : b) std::cin x; if (std::accumulate(all(a), 0) != std::accumulate(all(b), 0)) return std::cout No , 0; if (n == 2) if (a[0] == b[0] a[1] == b[1]) std::cout Yes 0 ; else if (a[1] - 1 == b[0] a[0] + 1 == b[1]) std::cout Yes 1 1 2 ; else std::cout No ; return 0; // simulation std::vectorpii answer; auto swap = [](int i, int j) assert(0 = i i j j n); std::swap(a[i], a[j]); a[i]--, a[j]++; answer.emplace_back(i, j); ; auto prefix_incdec = [](int z, int x, int y) assert(0 = z z x x y y n); swap(x, y), swap(z, y), swap(z, x), swap(z, y); ; auto prefix_decinc = [](int z, int x, int y) assert(0 = z z x x y y n); swap(z, y), swap(z, x), swap(z, y), swap(x, y); ; auto suffix_incdec = [](int x, int y, int z) assert(0 = x x y y z z n); swap(x, y), swap(y, z), swap(x, z), swap(y, z); ; auto suffix_decinc = [](int x, int y, int z) assert(0 = x x y y z z n); swap(y, z), swap(x, z), swap(y, z), swap(x, y); ; auto mid_incdec = [](int x, int y, int z) assert(0 = x x y y z z n); swap(x, y), swap(y, z), swap(x, y), swap(x, z); ; auto mid_decinc = [](int x, int y, int z) assert(0 = x x y y z z n); swap(x, z), swap(y, z), swap(x, y), swap(y, z); ; auto call = [](int T, auto F, int x, int y, int z) assert(0 = x x y y z z n); for (int i = 0; i T; i++) F(x, y, z); ; for (int i = 0; i n; i++) if (a[i] == b[i]) continue; int j = i + 1, maxv = -1, rec = -1; for (; j n; j++) if ((a[j] b[j]) != (a[i] b[i]) abs(a[j] - b[j]) maxv) rec = j, maxv = abs(a[j] - b[j]); j = rec; if (a[i] b[i]) int dec = a[i] - b[i]; if (i + 1 == j) if (i == 0) call(dec, suffix_decinc, i, j, n - 1); else call(dec, prefix_decinc, 0, i, j); else call(dec, mid_decinc, i, j - 1, j); else int inc = b[i] - a[i]; if (i + 1 == j) if (i == 0) call(inc, suffix_incdec, i, j, n - 1); else call(inc, prefix_incdec, 0, i, j); else call(inc, mid_incdec, i, j - 1, j); if (answer.size() 31000) std::cout No ; else std::cout Yes answer.size() ; for (auto [i, j] : answer) std::cout i + 1 j + 1 ;"},{"title":"2018 World Final - Beijing","path":"/wiki/algo_contests/WF2018.html","content":"A - Catch the Plane 一道很有意思的概率 DP 题。首先我们令 PiP_iPi​ 表示 iii 站点能够到达 111 的概率，然后我们就发现我们其实还需要考虑时间因素：一是因为同一个点不同时刻出发能够到达 111 的概率是单调的；二是因为，路线也导致了我们需要把同一个点上的不同时刻纳入考量。 所以我们令 P(t,i)P_{(t,i)}P(t,i)​ 表示第 ttt 时刻从 iii 出发能够到达 111 的概率。考虑线路 u→vu\\to vu→v，出发时间为 ddd，到达时间为 aaa，准点概率为 ppp： 我们采取这样的乘车策略：我们在 uuu 等到时刻 ddd，然后检查是否发车，若发车则检查 P(a,v)P(d,u)P_{(a,v)}\\gt P_{(d,u)}P(a,v)​P(d,u)​（即上车到达 vvv 之后，更有可能到达 111 我才乘车）；若不发车，则只能等下一班车，其 d′dd\\gt dd′d。 所以我们可以列出 dp 式子： P(tj,i)={P(tj+1,i)if P(ak,v)≤P(tj+1,i)p⋅P(ak,v)+(1−p)⋅P(tj+1,i)if P(ak,v)P(tj+1,i) P_{(t_j,i)}=\\begin{cases} P_{(t_{j+1}, i)} \\texttt{if \\(P_{(a_k,v)}\\le P_{(t_{j+1}, i)}\\)}\\\\ p\\cdot P_{(a_k,v)} + (1-p)\\cdot P_{(t_{j+1}, i)}\\texttt{if \\(P_{(a_k,v)}\\gt P_{(t_{j+1}, i)}\\)} \\end{cases} P(tj​,i)​={P(tj+1​,i)​p⋅P(ak​,v)​+(1−p)⋅P(tj+1​,i)​​if P(ak​,v)​≤P(tj+1​,i)​if P(ak​,v)​P(tj+1​,i)​​第一个 case 是说，如果我走 i→vi\\to vi→v 巴士线到了 vvv，结果到 111 的概率更低了，那我肯定不坐巴士而是等下一班车；第二个 case 是说，如果从 vvv 到 111 的概率更高，那么我就需要坐这辆巴士“赌”一下。 这里我们可以看到，时刻更小的 dp 状态依赖于时刻大的 dp 状态，所以，我们需要按 departure time 对巴士路线进行排序以及 DP 转移。同时，我们注意到时刻越小概率越大，且是不减的，枚举 i→vi\\to vi→v 后，我们可以通过二分快速找出 P(tj+1,i)P_{(t_{j+1},i)}P(tj+1​,i)​ 和 P(ak,v)P_{(a_k,v)}P(ak​,v)​，所以总时间复杂度是 O(mlog⁡n)O(m\\log n)O(mlogn) 的。 AC Code #include bits/stdc++.h#define all(x) x.begin(), x.end()using ll = long long;using ld = long double;constexpr int N = 1e6 + 10;struct _edge int from, to; ll depart, arrive; ld p;;std::vector_edge G;std::mapll, ld F[N];int m, n;ll k;int main() std::cin m n k; for (int i = 0, u, v; i m; i++) ll d, a; ld pr; std::cin u v d a pr; G.push_back(u, v, d, a, pr); std::sort(all(G), [](_edge a, _edge b) return a.depart b.depart; ); F[1].insert(k + 1, 1.0L); for (int i = 0; i n; i++) if (i != 1) F[i].insert(k + 1, 0.0L); for (auto [from, to, depart, arrive, p] : G) if (F[to].empty()) continue; auto arv = F[to].upper_bound(arrive); auto dpr = F[from].upper_bound(depart); ld np = p * arv-second + (1.0L - p) * dpr-second; F[from][depart] = std::max(F[from][depart], np, dpr-second); std::cout std::fixed std::setprecision(20); std::cout F[0].begin()-second ; H - Single Cut of Failure 由于题目里保证每条线段都会搭在矩形的两条边上，所以最多两条对角线一定可以切断所有线段。 所以现在就是考虑能否只用一条线段切断所有线段。我们把这个条件转换为： 考虑线段 (s,t)(s,t)(s,t)，nnn 条线段里的每条线段都恰好有一个端点位于 (s,t)(s,t)(s,t) 的下方 而且这 nnn 个端点一定是连续的 nnn 个点，因为如果 A,BA,BA,B 都在 (s,t)(s,t)(s,t) 的下方，那么线段 ABABAB 上的任意一点也会在 (s,t)(s,t)(s,t) 的下方。 所以我们可以将 2n2n2n 个端点顺时针排序，然后查看连续的 nnn 个点，用数组标记区间内点所对应的线段的 id 出现次数，若为 111 则说明相交，否则为 0/20/20/2 表示不相交。可以用双指针维护。时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn) AC Code 一个小点：双指针最后求出来的是两个端点，但是最后的答案不能离端点太近，所以一个解决方案是把点顺时针移动 0.50.50.5 的长度即可。 #include bits/stdc++.hconstexpr int N = 1e6 + 10;using ll = long long;using ld = long double;struct point ll x, y; int id; friend std::istream operator(std::istream is, point v) is v.x v.y; v.x *= 2, v.y *= 2; return is; friend std::ostream operator(std::ostream os, point v) return os ( v.x , v.y ); point operator-(point rhs) const return x - rhs.x, y - rhs.y, -1; friend ll cross(point a, point b) return a.x * b.y - a.y * b.x; ld angle() const return std::atan2(1.0L * y, 1.0L * x); ;struct line point s, e; friend std::istream operator(std::istream is, line v) return is v.s v.e; ;int n, w, h;point c;std::arrayline, N lines;std::vectorpoint p;void log(point p1) p1.x /= 2, p1.y /= 2; if (p1.x == 0) std::cout p1.x p1.y + 0.5; else if (p1.y == h) std::cout p1.x + 0.5 p1.y; else if (p1.x == w) std::cout p1.x p1.y - 0.5; else std::cout p1.x - 0.5 p1.y;int main() std::cin n w h; c.x = w, c.y = h; for (int i = 0; i n; i++) std::cin lines[i]; lines[i].s.id = lines[i].e.id = i; p.push_back(lines[i].s); p.push_back(lines[i].e); std::sort(p.begin(), p.end(), [](point a, point b) return (a - c).angle() = (b - c).angle(); ); std::cout std::setprecision(20) std::fixed; [] std::vectorint mark(n, 0); int tot = 0; for (int i = 0; i n; i++) mark[p[i].id]++; if (mark[p[i].id] == 1) tot++; for (int i = n; i n * 2; i++) mark[p[i].id]++; if (mark[p[i].id] == 1) tot++; mark[p[i - n].id]--; if (mark[p[i - n].id] == 0) tot--; if (tot == n) std::cout 1 ; log(p[i - n]); std::cout ; log(p[i]); std::cout ; return; std::cout 2 ; std::cout 0 0.5 w h - 0.5 ; std::cout 0 h - 0.5 w 0.5 ; (); I - Triangles 码量较大的数据结构题。 先想怎么统计三角形个数：一种是 upside down 的，另一种是 upright 的。我们先来考虑 upright 的情况，那么 upside down 就可以认为是原图上下反转一下。 接下来思考如何统计这样的三角形。我们考虑以扫描线的方式一次处理一排，如下图的星号/加号所示 o o---*---o o \\ / / \\ o o---* o o / \\ / \\ \\o o---o---*---o / / \\ \\ / \\ o---o---o---+---o 这样的话，比如说我们考虑第四个点（加号），它向左可以延伸 L+=2L_+=2L+​=2 条边，向左上可以延伸 UL+=2UL_+=2UL+​=2 条边，所以如果以这个点为右下角，那么最多可以产生 min⁡(L+,UL+)=2\\min(L_+,UL_+)=2min(L+​,UL+​)=2 个三角形。 那么这条直线上，哪些点 jjj 可以作为这个三角形的顶点呢？我们发现这样的 jjj 应该满足这样的关系，令 jjj 最多可以往左下延伸 DLjDL_jDLj​ 条边 i−j≤Ai=min⁡(Li,ULi)i−j≤DLj i-j\\le A_i=\\min(L_i,UL_i)\\\\ i-j\\le DL_j i−j≤Ai​=min(Li​,ULi​)i−j≤DLj​所以，我们考虑怎么维护这个。可以用 std::set 维护数对 { j+DLj,j }\\set{j+DL_j,j}{j+DLj​,j}，并且时刻剔除 j+DLjij+DL_j\\lt ij+DLj​i 的下标，在剩余的数对里找满足 j∈[i−Ai,i−1]j\\in[i-A_i,i-1]j∈[i−Ai​,i−1] 的 jjj，可以配合树状数组完成。 时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn)，nnn 是点数，可以到达 10610^6106 量级。 AC Code #include bits/stdc++.hconstexpr int M = 3005;using pii = std::pairint, int;int r, c;int cnt0;std::string T[M * 2];int ID[M * 2][M * 4];int L[M * M], UL[M * M], DL[M * M], UR[M * M], R[M * M];std::mapint, std::vectorpii line;long long ans0;std::arrayint, M * 8 fenwick;int maxsz;void reset(int maxsize) maxsz = maxsize; for (int i = 0; i = maxsize; i++) fenwick[i] = 0;void add(int pos, int v) for (; pos = maxsz; pos += pos -pos) fenwick[pos] += v;int sum(int p) int res = 0; for (; p 0; p -= p -p) res += fenwick[p]; return res;int range(int l, int r = -1) if (r == -1) r = maxsz; if (l r) return 0; else return sum(r) - sum(l - 1);void logFT() for (int i = 1; i = maxsz; i++) std::cout range(i, i) [i == maxsz];int main() std::cin r c, std::cin.ignore(); for (int i = 1; i 2 * r; i++) std::getline(std::cin, T[i]); if (i % 2 == 0) continue; for (int j = 0; j T[i].length(); j++) if (T[i][j] == x) ID[i][j] = ++cnt, line[i - j].push_back(i, j); [] // Left extend for (int i = 1; i 2 * r; i += 2) for (int j = i % 4 + 3; j T[i].length(); j += 4) if (T[i][j - 1] == -) L[ID[i][j]] = 1 + L[ID[i][j - 4]]; // Right extend for (int i = 1; i 2 * r; i += 2) for (int j = T[i].length() - 1; j = 0; j--) if (T[i][j] != x) continue; if (j + 4 T[i].length() T[i][j + 1] == -) R[ID[i][j]] = 1 + R[ID[i][j + 4]]; // Up left Up Right for (int i = 2; i 2 * r; i += 2) for (int j = 0; j T[i].length(); j++) if (T[i][j] == \\\\) UL[ID[i + 1][j + 1]] = 1 + UL[ID[i - 1][j - 1]]; else if (T[i][j] == /) UR[ID[i + 1][j - 1]] = 1 + UR[ID[i - 1][j + 1]]; // Down left for (int i = 2 * r - 2; i = 1; i -= 2) for (int j = 0; j T[i].length(); j++) if (T[i][j] == /) DL[ID[i - 1][j + 1]] = 1 + DL[ID[i + 1][j - 1]]; (); // Suppose point is Bottom Right point for (auto item : line) auto points = item.second; int m = points.size(); // for (auto p : points) // auto [x, y] = p; // T[x][y] = *; // // for (int i = 1; i 2 * r; i++) // for (int j = 0; j T[i].length(); j++) // if (T[i][j] != x) std::cout T[i][j]; // else std::cout o; // // std::cout ; // reset(m); std::setpii maintain; // j + DL[j], j for (int i = 1; i = m; i++) auto [x, y] = points[i - 1]; int id = ID[x][y]; int rg = std::min(L[id], UL[id]); while (maintain.size()) auto [val, idx] = *maintain.begin(); if (val i) add(idx, -1), maintain.erase(maintain.begin()); else break; // logFT(); // std::fprintf(stdout, l = %d, i = %d, dans = %d , maintain.begin()-second, i, range(i - rg, i - 1)); ans += range(i - rg, i - 1); add(i, 1); maintain.insert(i + DL[ID[x][y]], i); for (auto p : points) auto [x, y] = p; T[x][y] = x; std::cout ; // Suppose point is top point for (auto [_, points] : line) // for (auto p : points) // auto [x, y] = p; // T[x][y] = *; // // for (int i = 1; i 2 * r; i++) // for (int j = 0; j T[i].length(); j++) // if (T[i][j] != x) std::cout T[i][j]; // else std::cout o; // // std::cout ; // int m = points.size(); reset(m); std::setpii mt; // R[j]+j, j for (int i = 1; i = m; i++) auto [x, y] = points[i - 1]; int id = ID[x][y]; int rg = std::min(UR[id], UL[id]); while (mt.size()) auto [val, idx] = *mt.begin(); if (val i) add(idx, -1), mt.erase(mt.begin()); else break; // logFT(); // std::fprintf(stdout, l = %d, i = %d, dans = %d , mt.begin()-second, i, range(i - rg, i - 1)); ans += range(i - rg, i - 1); add(i, 1), mt.insert(i + R[ID[x][y]], i); // for (auto p : points) // auto [x, y] = p; // T[x][y] = x; // // std::cout ; std::cout ans ;"},{"title":"[2/8] Codeforces Round 1025 (Div. 2)","path":"/wiki/algo_contests/cf2109.html","content":"A. It’s Time to Duel 不合法的就两个条件，满足任何一个即不合法： 总和超过 n−1n-1n−1，因为 n−1n-1n−1 场比赛最多 n−1n-1n−1 个胜者 连续两个 000。这是因为他们之间必有一场比赛，因此必有一个胜者 Code #include headers/io/iov2.hpp#include numeric#include vectorIO::IO io;void run() int n = io.scanint(); std::vectorint s = io.scanstd::vectorint(n); if (std::accumulate(s.begin(), s.end(), 0) = n) return io.println(YES); // consecutive 0s for (int i = 0; i n - 1; i++) if (s[i] == 0 s[i + 1] == 0) return io.println(YES); io.println(NO);int main() int T = io.scanint(); while (T--) run(); B. Slice to Survival 如果换个先手，让 Fouad 先移动，那么他一定会移动到中心的位置，Mouf 每次切只能切掉一半。这时的总次数就是 ⌈log⁡2n⌉×⌈log⁡2m⌉\\lceil \\log_2 n\\rceil \\times \\lceil \\log_2 m\\rceil⌈log2​n⌉×⌈log2​m⌉. 但是现在是 Mouf 先手，所以我们考虑把第一次的先手提出来单独计算，于是剩下的就和上面一样了。总次数 1+min⁡{⌈log⁡2n⌉+⌈log⁡2min⁡(b,m−b+1)⌉,⌈log⁡2min⁡(a,n−a+1)⌉+⌈log⁡2m⌉} \\gdef\\clog#1{\\lceil\\log_2 #1\\rceil} 1+\\min\\Bigg\\{ \\clog{n}+\\clog{\\min(b,m-b+1)}, \\clog{\\min(a,n-a+1)}+\\clog{m} \\Bigg\\} 1+min{⌈log2​n⌉+⌈log2​min(b,m−b+1)⌉,⌈log2​min(a,n−a+1)⌉+⌈log2​m⌉} Code #include algorithm#include headers/io/iov2.hppIO::IO io;using i64 = long long;void run() i64 n, m, a, b; io.read(n, m, a, b); auto clog2 = [](i64 x) return x = 1 ? 0 : 64 - __builtin_clzll(x - 1); ; io.println(1 + std::min(clog2(n) + clog2(std::min(b, m - b + 1)), clog2(std::min(a, n - a + 1)) + clog2(m)));int main() int T = io.scanint(); while (T--) run();#ifndef ONLINE_JUDGE io.println(==========);#endif return 0; C1. Hacking Numbers (Easy) 想法是快速归约到一个确定的数字，然后花一步到达 target. 那么我们最多用 666 完成归约这一步骤。想到用 digit 归约：因为题目 unknown number 最大不超过 10910^9109，因此数位和最大为 818181 (对应的数为 999 个 999)，再进行一次 digit 运算则数位和最大为 161616 (对应的数为 797979)。 然后就只剩四步归约到某个确定的数上。考虑二进制（直觉说 log⁡216=4\\log_2 16=4log2​16=4），我们依次减去 20,21,22,232^0,2^1,2^2,2^320,21,22,23，于是无论如何，xxx 都会被减成 111，这个数就确定了。这样对于每个数都可以通过这 777 完成。 Code #include iostreamusing i64 = long long;int digit() std::cout digit std::endl; int d; std::cin d; return d;int add(i64 x) std::cout add x std::endl; int d; std::cin d; return d;int mul(i64 x) std::cout mul x std::endl; int d; std::cin d; return d;int div(i64 x) std::cout div x std::endl; int d; std::cin d; return d;void finish() std::cout ! std::endl; int d; std::cin d;void run() i64 target; std::cin target; digit(), digit(); for (int i = 3; i = 0; i--) add(-(1 i)); add(target - 1); finish();int main() int T = 1; std::cin T; while (T--) run(); C2. Hacking Numbers (Medium) 想法依然是快速归约到一个数字。上一题我们用了两次 digit，这一次，我们考虑先 ×9\\times 9×9，这样其数位和一定是 999 的倍数，而且最多 101010 位数，因此数位和还一定 ≤90\\le 90≤90，于是，再做一次数位和，我们就一定可以得到 999。然后再花一步得到结果即可。 D. D/D/D 想到了大致做法但是小细节想错了 orz 首先要考虑到一件事：如果我们可以用 SSS 步到达点 iii，那么我们也可以用 S+2k,k∈NS+2k,k\\in\\NS+2k,k∈N 步到达点 iii，方法是选一条边 (u,v)(u,v)(u,v) 来回走 u→v→u…u\\to v\\to u \\dotsu→v→u…. 因此我们只需要维护两个东西，一个是“通过最少的奇数步到达点 iii” dp[i][1]dp[i][1]dp[i][1]，另一个是“通过最少的偶数步到达点 iii” dp[i][0]dp[i][0]dp[i][0]. 因为只关心最少，我们直接 BFS，同时注意转移 dp[u][0]=min⁡(u,v)∈E(dp[v][1]+1)dp[u][1]=min⁡(u,v)∈E(dp[v][0]+1) dp[u][0]=\\min_{(u,v)\\in E} (dp[v][1]+1)\\\\ dp[u][1]=\\min_{(u,v)\\in E} (dp[v][0]+1) dp[u][0]=(u,v)∈Emin​(dp[v][1]+1)dp[u][1]=(u,v)∈Emin​(dp[v][0]+1) Code #include array#include headers/io/iov2.hpp#include queue#include utility#include vectorIO::IO io;using i64 = long long;void run() auto [n, m, l] = io.scanint, int, int(); std::vectori64 a = io.scandecltype(a)(l); i64 max_even = 0, max_odd = 0, sum = 0; i64 min_even = 1e18, min_odd = 1e18; for (auto i : a) sum += i; if (i % 2 == 0) min_even = std::min(min_even, i); else min_odd = std::min(min_odd, i); max_even = (sum % 2 == 0 ? sum : sum - min_odd); max_odd = (sum % 2 == 1 ? sum : sum - min_odd); std::vectorstd::vectorint G(n); for (int i = 0; i m; i++) auto [u, v] = io.scanint, int(); u--, v--; G[u].push_back(v); G[v].push_back(u); // BFS tree std::vectorstd::arrayi64, 2 dp(n, static_castint(1e18), static_castint(1e18)); std::queuestd::pairint, i64 q; q.push(0, 0); dp[0][0] = 0; // start with even while (!q.empty()) auto [u, d] = q.front(); q.pop(); i64 parity = d % 2; // 0 for even, 1 for odd for (auto v : G[u]) if (dp.at(v).at(!parity) dp.at(u).at(parity) + 1) dp.at(v).at(!parity) = dp.at(u).at(parity) + 1; q.push(v, d + 1); io.set_delim(); for (int i = 0; i n; i++) if (max_even = dp.at(i).at(0) || max_odd = dp.at(i).at(1)) io.print(1); else io.print(0); io.println();int main() int T = io.scanint(); while (T--) run();#ifndef ONLINE_JUDGE io.println(==========);#endif return 0;"},{"title":"Codeforces Round 2124 (Div. 1+2)","path":"/wiki/algo_contests/codeforces-2124.html","content":"Codeforces Round 2124 (Div. 1+2) 可惜，感觉很多时候时间在找思路上面和证明思路上面，找思路还是有点慢了。 A. Deranged Deletions 我们只需要找到一个相邻的逆序对即可，有则可以把所有其他数都删完。若没有这样的逆序对，则说明原序列里所有数已经是递增的了，此时不管怎么删，都不会改变这些数的相对顺序，因此无解。 #include iostream#include vectorvoid run() int n; std::cin n; std::vectorint a(n); for (int i = 0; i n; ++i) std::cin a[i]; for (int i = 0; i n; i++) for (int j = i + 1; j n; j++) if (a[i] a[j]) std::cout YES 2 ; std::cout a[i] a[j] ; return; std::cout NO ;int main(int argc, char *argv[]) int T; std::cin T; while (T--) run(); return 0; B. Minimise Sum 首先，我们可以又一个大致的想法：把 000 尽可能放前面，这样后面的 prefix min 就都是 000，大概率能更小一点。 所以，我们贪心地考虑前 333 个位置。如果 a1a2a_1\\gt a_2a1​a2​，则令 (i,j)=(1,2)(i,j)=(1,2)(i,j)=(1,2)，总和为 a1+a2a_1+a_2a1​+a2​；若 a1a2a_1\\lt a_2a1​a2​，则令 (i,j)=(2,3)(i,j)=(2,3)(i,j)=(2,3)，总和为 a1+a1a_1+a_1a1​+a1​，于是此时的总和最小可以达到 a1+min⁡(a1,a2) a_1+\\min(a_1,a_2) a1​+min(a1​,a2​)而 S≥a1+min⁡(a1,a2)S\\ge a_1+\\min(a_1,a_2)S≥a1​+min(a1​,a2​)，所以这就是最小。 C. Subset Multiplication D. Make a Palindrome E. Make it Zero F1. Appending Permutations (Easy Version) F2. Appending Permutations (Hard Version) G. Maximise Sum H. Longest Good Subsequence I. Lexicographic Partition"},{"title":"IOI 2024","path":"/wiki/algo_contests/ioi-2024.html","content":"象形文字序列 首先需要观察到一个性质： UCS 的长度 令字符集合 SSS，对于某个字符 s∈Ss\\in Ss∈S，其在 AAA 中出现 CA(s)C_A(s)CA​(s) 次，在 BBB 中出现 CB(s)C_B(s)CB​(s) 次，则 UCS 的长度必须为 ∑s∈Smin⁡(CA(s),CB(s)) \\sum_{s\\in S} \\min(C_A(s), C_B(s)) s∈S∑​min(CA​(s),CB​(s))【简要证明】不妨 CA(s)≤CB(s)C_A(s)\\le C_B(s)CA​(s)≤CB​(s)，如果 UCS 里 sss 只出现了 CA(s)−1C_A(s)-1CA​(s)−1 次，那么由于 ss…s⏟CA(s)\\underbrace{ss\\dots s}_{C_A(s)}CA​(s)ss…s​​ 是 A,BA,BA,B 共同的子串，但是并不是 UCS 的子串，这与 UCS 的定义矛盾。因此 UCS 的长度其实是可以确定的。 那么，既然我们知道了 UCS 的字符构成，我们开始思考怎么去从给定的字符中构造 UCS. 由于字符串长度较长，所以应该是 O(n)O(n)O(n) 或者 O(nlog⁡n)O(n\\log n)O(nlogn) 的构造。 如果对于某个字符 s∈Ss\\in Ss∈S 且 CA(s)≤CB(s)C_A(s)\\le C_B(s)CA​(s)≤CB​(s)，就说明 UCS 里出现的所有 sss 刚好和 AAA 全部匹配上，把 sss 在 AAA 里所有出现的位置记为 sss-critical-in-AAA. 类似地有 sss-critical-in-BBB. 把 sss 记为 critical char of A/BA/BA/B，sss 在 A,BA,BA,B 里面的下标记为 critical position. 尝试一下“贪心构造再检查”的策略，那就是考虑怎么把 critical char of A,BA,BA,B 放入 UCS. 同时我们要考虑到 critical char 之间的顺序：例如样例一，AAA 的 critical char 为 0,1,0,20,1,0,20,1,0,2，那么 UCS 里就不能是 2,0,1,02,0,1,02,0,1,0（不然的话 222-critical-in-AAA 就无法匹配了）。 由此我们得到了一个贪心的重要依据：字符串 A,BA,BA,B 里的 critical char 必须按照这个 char 在 A/BA/BA/B 内的顺序进行排列，也就是类似于双指针排序的过程（如果 s1s_1s1​ 在 AAA 字符串里排在 s2s_2s2​ 前面，那么 s1s_1s1​ 在 UCS 里也必须排在 s2s_2s2​ 前面）。 所以，我们可以用双指针指向 A,BA,BA,B’s critical positions of sss，每次判断该放 critical char of BBB 还是 of AAA. 然后，我们考虑如何判断。假设已经有了一段 UCS，分别匹配了前缀 A[1,i],B[1,j]A[1,i],B[1,j]A[1,i],B[1,j]，当前的 critical position of A,BA,BA,B 分别为 kA,kBk_A,k_BkA​,kB​. 如果 A[kA]A[k_A]A[kA​] 上的字符可以放入 UCS，那么： B[j,kB−1]B[j,k_B-1]B[j,kB​−1] 这一段子串里，应该有一个字符等于 A[kA]A[k_A]A[kA​]，这样才能匹配上并且 kBk_BkB​ 这个字符不会被跳过（被跳过就无法放入 UCS 了） 放入 A[kA]A[k_A]A[kA​] 后，UCS 匹配的前缀是 A[1,kA]A[1,k_A]A[1,kA​]。为了让 B[kB]B[k_B]B[kB​] 也能够完成匹配，在 A[kA+1,end]A[k_A+1, \\texttt{end}]A[kA​+1,end] 这段后缀里，应该有足够多的字符让 BBB’s remaining critical chars 匹配。 对 B[kB]B[k_B]B[kB​] 也是同理。 如果都不能放，那么 UCS 无解。如果都可以放，那么也就是说，A,BA,BA,B 之间有两个子序列，一个子序列在这个位置是 A[kA]A[k_A]A[kA​] 另一个却是 B[kB]B[k_B]B[kB​]（这两个字符不一样），而且长度还都是一样的，这样的情况下，这两个子序列不可能同时是 UCS 的子序列，所以也无解。 构造出 UCS 后，我们再来检查构造出来的 UCS 是否合法。对 i∈[1,n]i\\in [1,n]i∈[1,n]，我们处理出对应的 min⁡j\\min jminj 使得 A[1,i]A[1,i]A[1,i] 可以匹配 B[1,j]B[1,j]B[1,j] 而无法匹配 B[1,j−1]B[1,j-1]B[1,j−1]. (prepare()) Code #include algorithm#include iostream#include utility#include vectorusing vi = std::vectorint;constexpr int V = 2e5 + 5;struct Sequence int length; int pos; vi first_of; vi next_same; vi count_same; vi data; Sequence(vi T) : dataT length = T.size(), pos = 0; first_of.assign(V, length); next_same.assign(length + 1, length); count_same.assign(length + 1, 0); for (int i = length - 1; i = 0; i--) next_same[i] = first_of[T[i]]; first_of[T[i]] = i; count_same[i] = count_same[next_same[i]] + 1; int next(int ch) const return first_of[ch]; int remain(int ch) const int p = first_of[ch]; return p length ? count_same[p] : 0; void step() first_of[data[pos]] = next_same[pos]; pos++; void move_to(int new_pos) while (pos new_pos) step(); void match(int ch) move_to(first_of[ch] + 1); ;vi calculate_ucs(vi A, vi B, vi pickA, vi pickB) int n = A.size(), m = B.size(); vi cntA(V, 0), cntB(V, 0); for (int x : A) cntA[x]++; for (int x : B) cntB[x]++; vi partA, partB; for (int i = 0; i n; i++) if (cntA[A[i]] = cntB[A[i]]) partA.push_back(i); for (int i = 0; i m; i++) if (cntB[B[i]] cntA[B[i]]) partB.push_back(i); partA.push_back(n), partB.push_back(m); Sequence seqA(A), seqB(B); Sequence skipA(A), skipB(B); auto pA = partA.begin(), pB = partB.begin(); skipA.move_to(*pA), skipB.move_to(*pB); vi candidate; while (*pA != n or *pB != m) bool can_putA = true, can_putB = true; if (*pA != n) // can we fill A[i]? can_putA = seqB.remain(A[*pA]) skipB.remain(A[*pA]); if (*pB != m) can_putA = skipA.remain(B[*pB]) = skipB.remain(B[*pB]); else can_putA = false; if (*pB != m) can_putB = seqA.remain(B[*pB]) skipA.remain(B[*pB]); if (*pA != n) can_putB = skipB.remain(A[*pA]) = skipA.remain(A[*pA]); else can_putB = false; std::cerr pA: *pA , pB: *pB , can_putA: can_putA , can_putB: can_putB ; if (can_putA == can_putB) return -1; else if (can_putA) int cha = A[*pA]; candidate.push_back(cha); seqB.match(cha); std::swap(seqA, skipA); seqA.step(); pA++; skipA.move_to(*pA); else int chb = B[*pB]; candidate.push_back(chb); seqA.match(chb); std::swap(seqB, skipB); seqB.step(); pB++; skipB.move_to(*pB); pickA.push_back(seqA.pos - 1); pickB.push_back(seqB.pos - 1); return candidate;vi prepare(vi A, vi B) int n = A.size(), m = B.size(); std::vectorvi appear(V); for (int i = 0; i m; i++) appear[B[i]].push_back(i); for (auto v : appear) v.push_back(m); vi last(V, -1); std::vectorstd::pairint, int stack; stack.push_back(-1, -1); vi match(n, -1); for (int i = 0; i n; i++) int L = last[A[i]]; int where = std::lower_bound(stack.begin(), stack.end(), std::make_pair(L, -1))-second; if (where != m) where = *std::upper_bound(appear[A[i]].begin(), appear[A[i]].end(), where); match[i] = where; while (!stack.empty() stack.back().second = where) stack.pop_back(); stack.push_back(i, where); last[A[i]] = i; return match;bool check(vi A, vi B, vi c, vi posA, vi posB) int n = A.size(), m = B.size(), t = c.size(); vi matchA = prepare(A, B); vi atA(n, -1), prev_atA(n + 1, -1); for (int i = 0; i t; i++) atA[posA[i]] = i; prev_atA[posA[i] + 1] = i; for (int i = 1; i = n; i++) if (prev_atA[i] == -1) prev_atA[i] = prev_atA[i - 1]; for (int i = 0; i n; i++) if (atA[i] != -1) continue; int index = prev_atA[i]; if (index != -1 matchA[i] = posB[index]) return false; return true;vi ucs(vi A, vi B) vi posa, posb; auto ans = calculate_ucs(A, B, posa, posb); if (ans.size() == 1 ans[0] == -1) return -1; if (!check(A, B, ans, posa, posb)) return -1; if (!check(B, A, ans, posb, posa)) return -1; return ans;int main() int n, m; std::cin n m; vi A(n), B(m); for (int x : A) std::cin x; for (int x : B) std::cin x; vi result = ucs(A, B); std::cout result.size() ; for (auto x : result) std::cout x ; std::cout ;"},{"title":"JOISC 2021 记录","path":"/wiki/algo_contests/joisc2021.html","content":"Day1. Food Court 又是做了很久，有几个关键点没有想出来。 第一个关键点是，如何处理从队伍头部移走人数？ 第二个关键点，现在知道每一个查询对应的是第几个元素，也知道每次操作加入的颜色和数量，怎么处理查询？ Code 实现的时候，用了 AtCoder Library 的线段树 #include iostream#include vectorusing i64 = long long;constexpr i64 inf = 1e18;constexpr int N = 3e5 + 5;using namespace atcoder;int n, m, q;// EXPLAIN: seg1 for maintaining the total people come into queue// EXPLAIN: seg3 for maintain out/in-source customer at time axis along Restaurant axis.struct S i64 max;;struct F1 i64 add;;S op1(S a, S b) return std::max(a.max, b.max); S e1() return 0; S mp1(F1 f, S s) return s.max + f.add; F1 cp1(F1 a, F1 b) return a.add + b.add; F1 id1() return 0; lazy_segtreeS, op1, e1, F1, mp1, cp1, id1 seg1(N), seg3(N);// EXPLAIN: seg2 for maintain current # of people in queuestruct F2 i64 add, max;;S mp2(F2 f, S s) return std::max(f.max, s.max + f.add); F2 cp2(F2 a, F2 b) return a.add + b.add, std::max(b.max + a.add, a.max); F2 id2() return 0, 0; lazy_segtreeS, op1, e1, F2, mp2, cp2, id2 seg2(N);// EXPLAIN: Scan linestruct Segment i64 timing, delta; // INFO: this is for scan line.;i64 color[N];int mark[N];i64 ans[N];std::vectorSegment scan[N], query[N];int main() std::cin n m q; for (int i = 1, op; i = q; i++) std::cin op; if (op == 1) int l, r; i64 k; std::cin l r color[i] k; seg1.apply(l, r + 1, k); seg2.apply(l, r + 1, k, -inf); scan[l].push_back(i, k), scan[r + 1].push_back(i, -k); else if (op == 2) int l, r; i64 k; std::cin l r k; seg2.apply(l, r + 1, -k, 0); else i64 a, b; std::cin a b; mark[i] = true; query[a].push_back(i, b + seg1.get(a).max - seg2.get(a).max); for (int i = 1; i = n; i++) for (auto [T, D] : scan[i]) seg3.apply(T, q + 1, D); for (auto [T, req] : query[i]) // std::cerr query[ i ][ T ] = req ; ans[T] = seg3.max_right(0, [](S s) return s.max req; ); for (int i = 1; i = q; i++) if (!mark[i]) continue; // std::cerr ans[ i ] = ans[i] ; std::cout (ans[i] i ? 0 : color[ans[i]]) ;"},{"title":"NOI 2025","path":"/wiki/algo_contests/noi-2025.html","content":"[NOI2025] 机器人 如何快速写出正确的 Dijkstra .jpg 每次取出 queue top 后，只走一步 很容易想到把 ppp 和 distancedistancedistance 都看作 dijkstra 的状态（因为 ppp 的取值也会影响边权）。于是，我们在某个路口可以用两种操作 通过操作把 ppp 降低到某个值 在当前路口走第 ppp 条路 我就错在这里：我把 dis[u][p]dis[u][p]dis[u][p] 定义成了 min⁡v→udis[v][p]+w\\min_{v\\to u} dis[v][p] + wminv→u​dis[v][p]+w，但实际上还需要算上 dis[u][p′]+∑w[]dis[u][p] + \\sum w[]dis[u][p′]+∑w[] 或者 ∑v[]\\sum v[]∑v[]. Code dis[][] 由于第二维是不确定的，所以这里我使用了哈希表的写法。 #include algorithm#include ext/pb_ds/assoc_container.hpp#include iostream#include queue#include tuple#include vectorusing i64 = long long;template typename Tusing vec = std::vectorT;template typename K, typename Vusing umap = __gnu_pbds::gp_hash_tableK, V;template typename Tusing lqueue = std::priority_queueT, std::vectorT, std::greaterT;constexpr int N = 3e5 + 5;constexpr i64 INF = 2e18;int c, n, m, k, deg[N];i64 v[N], w[N];vecstd::tupleint, i64 G[N];umapint, i64 dis[N];i64 ans[N];int main() #ifdef ONLINE_JUDGE std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0);#endif std::cin c n m k; for (int i = 1; i = k - 1; i++) std::cin v[i], v[i] += v[i - 1]; for (int i = 2; i = k; i++) std::cin w[i], w[i] += w[i - 1]; for (int u = 1; u = n; u++) std::cin deg[u]; for (int i = 0, y, z; i deg[u]; i++) std::cin y z; G[u].emplace_back(y, z); ans[u] = INF; lqueuestd::tuplei64, int, int pq; dis[1][1] = 0; ans[1] = 0; pq.push(0, 1, 1); while (!pq.empty()) auto [D, p, u] = pq.top(); pq.pop(); if (dis[u][p] != D) continue; if (p = deg[u]) auto [to, z] = G[u][p - 1]; if (dis[to].find(p) == dis[to].end() || D + z dis[to][p]) dis[to][p] = D + z; ans[to] = std::min(ans[to], dis[to][p]); pq.push(D + z, p, to); for (int i = 1; i = std::min(deg[u], k); i++) i64 cost = 0; if (i p) cost = w[p] - w[i]; else cost = v[i - 1] - v[p - 1]; if (dis[u].find(i) == dis[u].end() || D + cost dis[u][i]) dis[u][i] = D + cost; ans[u] = std::min(ans[u], dis[u][i]); pq.push(D + cost, i, u); for (int i = 1; i = n; i++) std::cout (ans[i] == INF ? -1 : ans[i]) ; std::cout ;"},{"title":"牛客练习赛 139","path":"/wiki/algo_contests/nowcoder-practice-139.html","content":"A. B/C. 注意到题目描述里是从第一列的给定位置走到第 mmm 列的给定位置，并且我们往右走是无法回头的。 所以这个提醒我们可以按列进行考虑。 D. 树到云端 因为通过 val[]{val}[]val[] 计算出来的 T[]T[]T[] 数组是已知的，而 val[]{val}[]val[] 是未知的，这就相当于我们在解方程，于是就可以想到消元（其实就是作差）。考虑以 111 为根，考虑树边 u→vu\\to vu→v 其中 dep[u]dep[v]\\texttt{dep}[u]\\texttt{dep}[v]dep[u]dep[v]，则有 T[u]−T[v]=∑i∈subtree(v)val[i]−∑i∉subtree(v)val[i]=(2×∑i∈subtree(v)val[i])−S \\begin{aligned} T[u]-T[v]=\\sum_{i\\in \\text{subtree(v)}}val[i]-\\sum_{i otin\\text{subtree(v)}}val[i]\\\\ =\\Big(2\\times\\sum_{i\\in\\text{subtree(v)}}val[i]\\Big)-S \\end{aligned} T[u]−T[v]​=i∈subtree(v)∑​val[i]−i∈/subtree(v)∑​val[i]=(2×i∈subtree(v)∑​val[i])−S​这里 S=∑ival[i]S=\\sum_i val[i]S=∑i​val[i]. 我们再把所有边加起来： 这一步我做的时候没有想到…… -.- ∑(u→v)∈Edep[u]dep[v]T[u]−T[v]=−(n−1)S+2(∑v≠1∑i∈subtree(v)val[i]) \\begin{aligned} \\sum_{(u\\to v)\\in E}^{\\texttt{dep}[u]\\texttt{dep}[v]}T[u]-T[v] =-(n-1)S+2\\Bigg( \\sum_{v e 1}\\sum_{i\\in\\text{subtree(v)}}val[i] \\Bigg) \\end{aligned} (u→v)∈E∑dep[u]dep[v]​T[u]−T[v]​=−(n−1)S+2(v=1∑​i∈subtree(v)∑​val[i])​然后我们注意看括号里的和式，我们换一个角度考察。现在的和式是“对于每一个 vvv，统计其子树内的 val[]val[]val[] 的和”。我们把视角放回 vvv 何时可以被某个 subtree(u)\\text{subtree(u)}subtree(u) 统计到，我们发现当且仅当 uuu 是 vvv 的祖先时可以，而 vvv 有多少的祖先呢？一共 dep[v]\\texttt{dep[\\(v\\)]}dep[v] 个（这里令 dep[root]=0\\texttt{dep[root]}=0dep[root]=0，而且要注意到 subtree(u)\\text{subtree}(u)subtree(u) 要求 u≠rootu e\\texttt{root}u=root)，于是这个和式可以进一步改写为 =−(n−1)S+2(∑v≠1(val[v]×dep[v])) =-(n-1)S+2\\Bigg( \\sum_{v e 1}(\\texttt{val[\\(v\\)]}\\times \\texttt{dep[\\(v\\)]}) \\Bigg) =−(n−1)S+2(v=1∑​(val[v]×dep[v]))刚好这个和式就等于 T[1]T[1]T[1]，于是： ∑(u→v)∈ET[u]−T[v]=−(n−1)S+2T[1]S=2T[1]−∑(u→v)∈ET[u]−T[v]n−1 \\sum_{(u\\to v)\\in E}T[u]-T[v]=-(n-1)S+2T[1]\\\\ S=\\frac{2T[1]-\\sum_{(u\\to v)\\in E}T[u]-T[v]}{n-1} (u→v)∈E∑​T[u]−T[v]=−(n−1)S+2T[1]S=n−12T[1]−∑(u→v)∈E​T[u]−T[v]​所以我们也可以求出子树 vvv 的 val[]val[]val[] 和了： ∑i∈subtree(v)val[i]=S+T[u]−T[v]2 \\sum_{i\\in\\text{subtree(v)}}val[i]=\\frac{S+T[u]-T[v]}{2} i∈subtree(v)∑​val[i]=2S+T[u]−T[v]​考虑节点 uuu，那么其 val[u]val[u]val[u] 值就是其子树和减去所有儿子的子树和 val[u]=∑i∈subtree(u)val[i]−∑v∈sonu∑i∈subtree(v)val[i] val[u]=\\sum_{i\\in\\text{subtree(u)}}val[i]-\\sum_{v\\in son_u}\\sum_{i\\in\\text{subtree(v)}}val[i] val[u]=i∈subtree(u)∑​val[i]−v∈sonu​∑​i∈subtree(v)∑​val[i]对 root=1root=1root=1 特殊考虑一下。 Code #include iostream#include vectorusing i64 = long long;using vi = std::vectori64;using vvi = std::vectorvi;using tree = std::vectorstd::vectorint;int main() int n; std::cin n; tree G(n + 1); for (int i = 1, u, v; i n; i++) std::cin u v; G.at(u).push_back(v); G.at(v).push_back(u); vi T(n + 1, 0); for (int i = 1; i = n; i++) std::cin T.at(i); vi val(n + 1, 0), subtree(n + 1, 0); i64 delta = 0; auto dfs = [](auto self, int u, int fa) - void for (auto v : G.at(u)) if (v == fa) continue; delta += T.at(u) - T.at(v); self(self, v, u); ; dfs(dfs, 1, 1); i64 sumval = (2 * T.at(1) - delta) / (n - 1); auto dfs2 = [](auto self, int u, int fa) - void for (auto v : G.at(u)) if (v == fa) continue; self(self, v, u); subtree.at(u) += subtree.at(v); if (u != 1) val.at(u) = (T.at(fa) - T.at(u) + sumval) / 2 - subtree.at(u); else val.at(u) = sumval - subtree.at(u); subtree.at(u) += val.at(u); ; dfs2(dfs2, 1, 1); for (int i = 1; i = n; i++) std::cout val.at(i) [i == n]; F. 信条 首先，廻文串是回文串，我们先用马拉车 O(n)O(n)O(n) 计算出回文串，然后再在回文串的基础上判断廻文串。 除去长度为 111 的廻文串，因为廻文串满足 ww′ww′wwwwww′ww′ 的 pattern，所以长度必须为偶数，因此，在 augmented string A[]A[]A[] 里，廻文串的中心必须是 padding char (#) 考虑第 iii 个位置 A[i]=‘#’A[i]=\\texttt{`\\#}A[i]=‘#’，以其为中心的最大回文串长度为 p[i]p[i]p[i]，如下图所示，我们希望知道它是多少个廻文串的中心。 廻文串 我们关注右侧的黄色部分，注意到其实 jjj 也是一个回文中心，我们可以得出 jjj 需要满足的条件： jjj 为中心的最大回文串至少需要包住 iii 考虑以 A[i…j]A[i\\dots j]A[i…j] 为左半段的回文串，其右端点不能超过 iii 的回文半径（下图浅蓝色红色部分），即 i+1≤j≤i+pi2i+1\\le j\\le i+\\frac{p_i}{2}i+1≤j≤i+2pi​​ 我们考虑对于 iii 怎么维护满足条件的 jjj。一个想法就是，如果让某个数据结构满足 T[j]=1T[j]=1T[j]=1 表示对于 iii，jjj 满足条件，那么我只需要求和 T[i+1]+…T[i+pi2]T[i+1]+\\dots T[i+\\frac{p_i}{2}]T[i+1]+…T[i+2pi​​] 即可。那么该怎么表示 ∑i+1≤j≤i+pi2j−p[j]≤iT[j] \\sum_{i+1\\le j\\le i+\\frac{p_i}{2}}^{j-p[j]\\le i} T[j] i+1≤j≤i+2pi​​∑j−p[j]≤i​T[j]我们考虑把 jjj 挂载到 pos=j−p[j]pos=j-p[j]pos=j−p[j] 上，这样我们顺序处理的时候，如果经过 pospospos，就表明 jjj 的回文中心的字符串可以到达 pospospos 这个位置，那么对于 i≥posi\\ge posi≥pos 的 iii 来说，j−p[j]≤ij-p[j]\\le ij−p[j]≤i 就自动满足了。然后我们挂载的时候，把 T[j]T[j]T[j] 设置为 111，这样，对于位置 iii 他就可以通过查询 ∑i+1≤j≤i+pi2T[j]\\sum_{i+1\\le j\\le i+\\frac{p_i}{2}} T[j]∑i+1≤j≤i+2pi​​​T[j] 找到对 iii 来说所有满足要求的 jjj。 j 的选择 再来看第二问，这是一个典型的贪心问题。对于廻文中心在 A[i]A[i]A[i] 的字符串，我们只需要考虑最长的廻文子串即可。这样一共有 O(n)O(n)O(n) 条线段。我们按左端点排序，维护一个“已经被覆盖的区间 [l,r][l,r][l,r]”，然后顺序遍历所有线条，我们下一条线段取“左端点在 [l,r][l,r][l,r] 里、右端点最大”的线段。时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn). 取最长的廻文子串需要找到对于 iii 来说 [i+1,i+pi2][i+1,i+\\frac{p_i}{2}][i+1,i+2pi​​] 中最远的那个 111. 这一点可以用线段树完成（如果为 111，maxpos 设置为 pospospos，否则为 −1-1−1，查询区间的时候查找区间内的 maxpos）时间复杂度也为 O(nlog⁡n)O(n\\log n)O(nlogn) Code #include algorithm#include cassert#include iostream#include string#include utility#include vectorusing pii = std::pairint, int;class SegTree std::vectorint tree; std::vectorint maxpos; int n; void add(int p, int l, int r, int pos, int val) if (l == r) tree[p] += val; maxpos[p] = pos; return; int mid = l + ((r - l) 1); if (pos = mid) add(p * 2, l, mid, pos, val); else add(p * 2 + 1, mid + 1, r, pos, val); tree[p] = tree[p * 2] + tree[p * 2 + 1]; maxpos[p] = std::max(maxpos[p * 2], maxpos[p * 2 + 1]); int sum(int p, int l, int r, int ql, int qr) const if (ql r || qr l) return 0; if (ql = l r = qr) return tree[p]; int mid = l + ((r - l) 1); return sum(p * 2, l, mid, ql, qr) + sum(p * 2 + 1, mid + 1, r, ql, qr); int mr(int p, int l, int r, int ql, int qr) if (ql r || qr l) return -1; if (ql = l r = qr) return maxpos[p]; int mid = l + ((r - l) 1); return std::max(mr(p * 2, l, mid, ql, qr), mr(p * 2 + 1, mid + 1, r, ql, qr)); public: SegTree(int n_) : tree(5 * n_, 0), nn_, maxpos(5 * n_, -1) void add(int pos, int val) add(1, 0, n - 1, pos, val); int sum(int ql, int qr) const return ql = qr ? sum(1, 0, n - 1, ql, qr) : 0; int maxr(int pos, int l, int r) if (l = r) return pos; else return mr(1, 0, n - 1, l, r); int operator[](int pos) const assert(pos = 0 pos n); return sum(pos, pos); ;int main() std::string s; std::cin s; std::string aug = #; for (char c : s) aug += c; aug += #; std::vectorint maxlen; std::vectorstd::vectorint lbound(aug.size() + 1); int l = 0, r = -1; for (int i = 0, lr = aug.size(); i lr; i++) int tmp = (i r ? 0 : std::min(maxlen[l + r - i], r - i)); while (i - tmp = 0 i + tmp lr aug[i - tmp] == aug[i + tmp]) ++tmp; tmp--; maxlen.push_back(tmp); if (i + tmp r) l = i - tmp; r = i + tmp; if (i % 2 == 0) lbound.at(i - tmp).push_back(i); long long ans = 0; SegTree st(aug.size() + 1); std::vectorpii str; // L, R for (int i = 0; i aug.size(); i++) for (auto e : lbound.at(i)) assert(e % 2 == 0); st.add(e, 1); if (i % 2 == 0) int v = st.sum(i + 1, i + maxlen[i] / 2); ans += v; int len = st.maxr(i, i + 1, i + maxlen[i] / 2); if (len i) str.push_back((i + 1 - (len - i) * 2) / 2, +(i - 1 + (len - i) * 2) / 2); else str.push_back(i / 2, i / 2); std::sort(str.begin(), str.end(), [](const pii a, const pii b) return a.first b.first || (a.first == b.first a.second b.second); ); int cnt = 0, R = -1; for (int i = 0; i str.size();) int tr = -1, j = i; while (j str.size() str.at(j).first = R) tr = std::max(tr, str[j].second); j++; tr = std::max(tr, str[j].second); if (j str.size()) cnt++, R = tr; i = j + 1; std::cout ans + s.size() cnt ;"},{"title":"北京大学 2024 年《数据结构与算法A（实验班）》期末考试","path":"/wiki/algo_contests/pku-2024-ds-and-algo-A-final.html","content":"D. MST 算法流程 先对 “(边权, ID)” 进行排序（从小到大） 初始化…… 并查集，两颗线段树（一棵维护从左向右的字符串哈希值，另一棵维护从右向左的字符串哈希值） 给并查集上的每一个连通块分配一个随机数（nnn 个，初始时并查集都不连通） 初始化 vector，记录一下每一个连通块内的所有点 遍历排序过的边…… (w,id)(w,id)(w,id) 首先，计算一下 ididid 对应的边集，(1,id−1),(2,id−2),…,(id−12,id−id−12)(1,id-1), (2,id-2), \\dots, (\\frac{id-1}{2}, id-\\frac{id-1}{2})(1,id−1),(2,id−2),…,(2id−1​,id−2id−1​) 从 l=id−12,r=id−id−12l=\\frac{id-1}{2},r=id-\\frac{id-1}{2}l=2id−1​,r=id−2id−1​ 开始向外二分查找下一条要连接的边 (l−M,r+M)(l-M, r+M)(l−M,r+M) 在并查集上连接这一条边对应的两个点 l−M,r+Ml-M,r+Ml−M,r+M， 同时维护 vector（更新同一个连通块内的点，这里需要用启发式合并）。 维护 vector 的同时，在线段树上同步修改对应点的值（修改成新连通块的值），即同步维护字符串的哈希值。 每合并两个点，就把边权加入答案 最后输出答案 正确性证明 考虑根据 Kruskal 算法的思路，我们总是尝试从边权最小的边 e=(u,v)e=(u,v)e=(u,v) 开始尝试加入 MST，如果 (u,v)(u,v)(u,v) 在并查集里不连通，那么就说明这条边可以加入 MST. 现在的话，边都是以 ai+ja_{i+j}ai+j​ 的形式给出，例如对于 aka_kak​，它所代表的边为 (1,k−1),(2,k−2),…(1,k-1),(2,k-2),\\dots(1,k−1),(2,k−2),…。 如果我们给并查集里的每一个连通块分配一个字母，那么考虑并查集里的两个点 i,ji,ji,j 且我们正在考虑 aka_kak​ 满足 k=i+jk=i+jk=i+j：那么我们可以发现的一点是，如果 i,ji,ji,j 已经连通，那么他们的字母应该是相同的，否则就不相同。如果字母相同，我们就不需要在 (i,j)(i,j)(i,j) 之间连边，因为他们已经在同一个连通块里（根据 Kruskal 算法，加入 (i,j)(i,j)(i,j) 这条边会产生一个环）；否则我们就可以加入这条边，在并查集里把他们连起来。 然后我们就可以发现一件事：如果对每一条 aka_kak​ 对应的边 (i,j)(i,j)(i,j) 来说，都不需要向 MST 里添加这条边，这意味着 i,ji,ji,j 对应的值相等；而 i+j=ki+j=ki+j=k，因此总是有 val[i]=val[k−i]val[i]=val[k-i]val[i]=val[k−i]，也就是说 aka_kak​ 所对应的点 1…k−11\\dots k-11…k−1 是回文串！ 这意味着我们可以通过不断判断某一段前后缀是否回文，来看这一段前后缀是不是已经在并查集（也即 MST）上连接完毕。我们可以用二分快速进行查找和判断。 时间复杂度分析 根据 MST 的性质，MST 是一棵树，最多 n−1n-1n−1 条边，而因为每一次二分必将连接一条边，因此“二分枚举待连接的边”这个操作最多进行 n−1n-1n−1 次，即 O(n)O(n)O(n) 次。每一次二分最多在包含 nnn 条边的集合内搜索，因此二分次数是 O(nlog⁡n)O(n\\log n)O(nlogn) 的。每一次二分都需要在线段树上进行区间查询，而单次区间查询是 O(log⁡n)O(\\log n)O(logn) 的，所有二分部分的时间复杂度是 O(nlog⁡2n)O(n\\log^2 n)O(nlog2n) 接着考虑启发式合并部分的复杂度。由于每一次都将小的集合添加到大的集合里，因此每一次合并，被添加的元素所在的集合大小都会翻倍，由于最多有 nnn 个点，因此最坏情况下一个元素会被添加 log⁡n\\log nlogn 次，因此启发式合并一共会产生 O(nlog⁡n)O(n\\log n)O(nlogn) 次添加元素操作。 但是在每次添加元素的时候，我们还要维护线段树，对单点修改、区间查询线段树而言，修改一次的复杂度是 O(log⁡n)O(\\log n)O(logn)，因此启发式合并以及维护线段树的总体复杂度就是 O(nlog⁡2n)O(n\\log^2 n)O(nlog2n) 因此总体时间复杂度为 O(nlog⁡2n)O(n\\log^2n)O(nlog2n) Code #include algorithm#include cassert#include functional#include iostream#include numeric#include random#include utility#include vectorusing u64 = unsigned long long;constexpr u64 P = 4816069;std::mt19937_64 rng(std::random_device());std::vectoru64 base, rd;struct ModTree struct Node int l, r; u64 fhash, bhash; ; std::vectorNode tree; int n; Node update(Node l, Node r) Node res; res.l = l.l, res.r = r.r; res.fhash = l.fhash * base[r.r - r.l + 1] + r.fhash; res.bhash = r.bhash * base[l.r - l.l + 1] + l.bhash; return res; void init(int n) tree.assign(n * 4, Node()); this-n = n; void build(std::vectoru64 vec, int p, int l, int r) tree[p].l = l, tree[p].r = r; if (l == r) tree[p].fhash = tree[p].bhash = vec[l]; return; int mid = (l + r) 1; build(vec, p 1, l, mid); build(vec, p 1 | 1, mid + 1, r); tree[p] = update(tree[p 1], tree[p 1 | 1]); void modify(int p, int pos, u64 val) if (tree[p].l == tree[p].r) tree[p].fhash = tree[p].bhash = val; return; int mid = (tree[p].l + tree[p].r) 1; if (pos = mid) modify(p 1, pos, val); else modify(p 1 | 1, pos, val); tree[p] = update(tree[p 1], tree[p 1 | 1]); Node query(int p, int l, int r) if (l = tree[p].l tree[p].r = r) return tree[p]; int mid = (tree[p].l + tree[p].r) 1; if (r = mid) return query(p 1, l, r); else if (l mid) return query(p 1 | 1, l, r); Node res = update(query(p 1, l, mid), query(p 1 | 1, mid + 1, r)); return res; ;int main() std::ios::sync_with_stdio(false); std::cin.tie(nullptr); std::cout.tie(nullptr); int n; std::cin n; [] base.resize(n + 1), rd.resize(n + 1); base[0] = 1; for (int i = 1; i = n; i++) base[i] = base[i - 1] * P, rd[i] = rng(); (); std::vectorstd::pairu64, int vec; std::vectorstd::vectorint nodes(n + 1); for (int i = 3; i n * 2; i++) u64 x; std::cin x; vec.emplace_back(x, i); std::vectorint fa(n + 1, 0); std::iota(fa.begin(), fa.end(), 0); std::functionint(int) find = [](int x) return fa[x] == x ? x : fa[x] = find(fa[x]); ; ModTree tree; tree.init(n); tree.build(rd, 1, 1, n); for (int i = 1; i = n; i++) nodes[i].push_back(i); auto unite = [](int u, int v) u = find(u), v = find(v); if (u == v) return; if (nodes[u].size() nodes[v].size()) std::swap(u, v); fa[u] = v; for (int i : nodes[u]) nodes[v].emplace_back(i); rd[i] = rd[v]; tree.modify(1, i, rd[i]); nodes[u].clear(); ; std::sort(vec.begin(), vec.end()); long long ans = 0; for (auto [E, id] : vec) int l = (id - 1) / 2, r = id - l; int size = std::min(l, n - r + 1); int L = l - size + 1, R = r + size - 1; while (r = R) // std::cerr l r L R ; // std::cerr tree.query(1, L, l).fhash tree.query(1, r, R).bhash ; // for (int i = 1; i = n; i++) std::cerr i : rd[i] ; if (tree.query(1, L, l).fhash == tree.query(1, r, R).bhash) break; int lb = 0, ub = R - r; while (lb = ub) int mid = (lb + ub) 1; if (tree.query(1, l - mid, l).fhash != tree.query(1, r, r + mid).bhash) ub = mid - 1; else lb = mid + 1; l -= lb, r += lb; unite(l, r); ans += E; std::cout ans ; return 0;"},{"title":"3rd UCup Stage 30, Petrozavodsk Winter 2025. Day 7. SPb SU Contest","path":"/wiki/algo_contests/qoj1917.html","content":"A. Archaeology 本场的计算几何题。考虑 ∠ARB90∘\\angle ARB\\lt 90^\\circ∠ARB90∘ 的等价条件，先连线 RBRBRB，然后过 RRR 做 RBRBRB 垂线，则 AAA 点和 BBB 点应该在这条直线的同侧（即直线左半平面所记录的区域）。 也就是说，每次询问都可以用一条直线，把 AAA 可能所在的区域变小一点。诶？多个半平面的交集，这不就是半平面交吗！诶，那么 909090 次的限制呢？我们每次让这个面积缩小一半不就是二分了吗！log⁡2101890\\log_2 10^{18}\\lt 90log2​101890 诶正好！ 那么怎么让这个面积缩小一半呢？我们可以维护半平面交形成的凸包的最小覆盖矩形（其实只需要 min⁡x,min⁡y,max⁡x,max⁡y\\min x,\\min y,\\max x, \\max yminx,miny,maxx,maxy 这四个值，然后构造矩形即可），然后询问中心，询问到的新点继续分割半平面交出来的凸包。 Code 需要注意的实现细节： 可以在一开始加入四条直线把 [1,N]×[1,N][1,N]\\times [1,N][1,N]×[1,N] 的矩形全部包住，避免半平面交没有封闭从而需要讨论 可能需要特殊讨论 n=1n=1n=1 的情况 可能会出现新加入的直线并没有“切掉”凸包或者只“切掉”一点点凸包。为了避免这种情况下反复询问某个点，这里额外计算了两次凸包面积减少了多少。如果小于 10%10\\%10%，那么挑选相邻的点进行询问。 x,yx,yx,y 上下取整都可以 当剩余的格点数量少于剩余询问次数时，直接 break 暴力询问过去。 #include geometry/halfplane.hpp#include geometry/line.hpp#include geometry/real.hpp#include geometry/vec.hpp#include iostream#include tuple#include vectorusing namespace geometry;using i64 = long long;int n, steps = 90;std::vectorline lines;polygon poly;bool verdict;int main() std::cin n; if (n == 1) std::cout 1 1 std::endl; int rx, ry; std::cin rx ry; return 0; double tn = n; lines.push_back(from_points(vec1, 1, vectn, 1)); lines.push_back(from_points(vectn, 1, vectn, tn)); lines.push_back(from_points(vectn, tn, vec1, tn)); lines.push_back(from_points(vec1, tn, vec1, 1)); i64 l = 1e18, r = -1e18, u = -1e18, d = 1e18; double pa = -1; while (steps) std::tie(lines, poly, verdict) = solve_halfplane(lines); double a = area(poly); double ratio = (pa - a) / pa * 100; l = 1e18, r = -1e18, u = -1e18, d = 1e18; for (auto [x, y] : poly) l = min(l, std::round(x)); r = max(r, std::round(x)); u = max(u, std::round(y)); d = min(d, std::round(y)); i64 rem = (r - l + 1) * (u - d + 1); if (rem = steps) break; i64 x = (l + r + 1) / 2, y = (u + d + 1) / 2, rx, ry; if (sign(pa, -1) != 0 sign(ratio, 10) = 0) i64 tx = x, ty = y; while (tx == x ty == y) x += rand() % 3 - 1; y += rand() % 3 - 1; std::cout x y std::endl; steps--; std::cin rx ry; if (x == rx y == ry) return 0; i64 dx = rx - x, dy = ry - y; i64 ddx = dy, ddy = -dx; lines.push_back(from_points(vecx * 1.0, y * 1.0, vec1.0 * (x + ddx), 1.0 * (y + ddy))); pa = a; for (int i = l; i = r; i++) for (int j = d; j = u; j++) std::cout i j std::endl; steps--; if (steps 0) return 0; int rx, ry; std::cin rx ry; if (rx == i ry == j) return 0;"},{"title":"The 3rd Universal Cup. Stage 39: Tokyo","path":"/wiki/algo_contests/qoj2071.html","content":"A. Array Similarity 我们先来考察 aia_iai​ 什么时候可以成为区间 [l,r][l,r][l,r] 的某个前缀最大值。我们用单调栈预处理出 LiL_iLi​ 满足 aLi−1ai∧aLi≤aia_{L_i-1}\\gt a_i\\land a_{L_i}\\le a_iaLi​−1​ai​∧aLi​​≤ai​（即 aia_iai​ 向前可以是多少个数的最大值），那么就只要 Li≤l∧i≤rL_i\\le l\\land i\\le rLi​≤l∧i≤r，aia_iai​ 就可以成为 [l,r][l,r][l,r] 的前缀最大值。 我们要判断两个区间是否 similar，也就是说对于 [l,r][l,r][l,r]，我们需要把所有符合条件的 aia_iai​ 都拿出来，并且需要计算 i1−l,i2−l…i_1-l,i_2-l\\dotsi1​−l,i2​−l…（即相对于 lll 的相对位置）是否相同。 比较是否相同这一点可以用哈希，道理和字符串哈希是一样的，只不过我们这里只关注“哈希可以表示相对位置”这个特点。 那么怎么提取所有符合条件的 aia_iai​ 呢？我们注意到 aia_iai​ 满足的条件是一个二维偏序， Li≤li≤r L_i\\le l\\\\ i\\le r Li​≤li≤r解决二维偏序的经典方法是对一个维度进行排序，然后用数据结构维护另一个维度。 这里，我们对 Li,lL_i,lLi​,l 维度进行排序，用双指针将满足 Li≤lL_i\\le lLi​≤l 的 aia_iai​ 插入数据结构，然后在数据结构里查询 [l,r][l,r][l,r] 的区间信息。结合上文所说的利用哈希确定 { i }\\set{i}{i} 的集合，我们可以往数据结构里插入哈希值。取出 [l,r][l,r][l,r] 的哈希值之和后，再除以 base[l-1] 即可。 时间复杂度：排序是 O(nlog⁡n+qlog⁡q)O(n\\log n +q\\log q)O(nlogn+qlogq) 的。我写的求哈希有点小问题，导致这一块的时间复杂度变成了 O(nlog⁡V)O(n\\log V)O(nlogV)，但可以做到 O(n+log⁡V)O(n+\\log V)O(n+logV)。总时间复杂度的瓶颈在排序上，因此为 O(nlog⁡n+qlog⁡q)O(n\\log n+q\\log q)O(nlogn+qlogq). Code #include algorithm#include chrono#include cstdio#include iostream#include random#include utility#include vectorusing pii = std::pairint, int;using u64 = long long;constexpr int N = 2e5 + 10;constexpr u64 MOD = 998244353;constexpr u64 B = 100003;constexpr u64 P = 1373;struct Range int l, r, qid; bool left;;struct Query Range x, y; u64 Lval, Rval; query[N];std::mt19937 rng(std::chrono::steady_clock::now().time_since_epoch().count());int n, q;int a[N], L[N];u64 hash[N], base[N];std::vectorRange range;std::vectorpii rk;u64 fw[N], count[N];void update(u64 *x, int i, u64 val) for (; i = n; i += i -i) x[i] += val, x[i] %= MOD;u64 get(u64 *x, int p) u64 res = 0; for (; p 0; p -= p -p) res += x[p], res %= MOD; return res;u64 get(u64 *x, int l, int r) return ((get(x, r) - get(x, l - 1)) % MOD + MOD) % MOD; int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0); std::cin n q; base[0] = 1; hash[0] = 1; for (int i = 1; i = n; i++) std::cin a[i], hash[i] = hash[i - 1] * B % MOD, base[i] = base[i - 1] * B % MOD; [] auto fpow = [](u64 b, u64 p) u64 res = 1; while (p) if (p 1) res = res * b % MOD; b = b * b % MOD; p = 1; return res; ; for (int i = 1; i = n; i++) base[i] = fpow(base[i], MOD - 2) % MOD; (); [] // monotonous stack using pii = std::pairint, int; std::vectorpii stk; stk.push_back(1e9 + 10, 0); for (int i = 1; i = n; i++) while (!stk.empty() stk.back().first = a[i]) stk.pop_back(); L[i] = stk.back().second + 1; stk.push_back(a[i], i); (); for (int i = 1; i = q; i++) std::cin query[i].x.l query[i].x.r query[i].y.l query[i].y.r; query[i].x.qid = query[i].y.qid = i; query[i].x.left = true, query[i].y.left = false; range.push_back(query[i].x); range.push_back(query[i].y); for (int i = 1; i = n; i++) rk.push_back(L[i], i); std::sort(rk.begin(), rk.end()); std::sort(range.begin(), range.end(), [](const Range a, const Range b) return a.l b.l || (a.l == b.l a.r b.r); ); auto pl = rk.begin(); auto pr = range.begin(); while (pr != range.end()) while (pl != rk.end() pl-first = pr-l) update(fw, pl-second, hash[pl-second]); update(count, pl-second, 1); pl++; u64 res = get(fw, pr-l, pr-r); u64 cnt = base[pr-l - 1] % MOD; if (pr-left) query[pr-qid].Lval = as; else query[pr-qid].Rval = as; pr++; for (int i = 1; i = q; i++) std::cout (query[i].Lval == query[i].Rval ? Yes : No) ; B. Bracket Character Frequency 合法括号序的条件是：考虑从 111 到某一个位置 iii 有多少个左右括号，对于每一个 iii 应该都有左括号数量 ≥\\ge≥ 右括号数量. 以及结束时左括号数量 === 右括号数量 对于位置 iii，如果有 kkk 个左括号，那么就有 i−ki-ki−k 个右括号，我们需要有 k≥i−kk\\ge i-kk≥i−k，所以 k≥i/2 ⟹ k≥⌈i2⌉k\\ge i/2\\implies k\\ge \\lceil\\frac{i}{2}\\rceilk≥i/2⟹k≥⌈2i​⌉. 所以，我们对 aia_iai​ 进行累加，必须满足 ∑ai≥n⋅⌈i2⌉\\sum a_i\\ge n\\cdot \\lceil\\frac{i}{2}\\rceil∑ai​≥n⋅⌈2i​⌉. 时间复杂度 O(n)O(n)O(n) Code #include iostreamusing i64 = long long;constexpr i64 K = 2e5 + 5;i64 d[K * 2];void run() i64 n, k, tot; std::cin n k; for (int i = 1; i = 2 * k; i++) std::cin d[i]; i64 left = 0, right = 0; for (int i = 1; i = k * 2; i++) left += d[i]; if (left n * ((i + 1) / 2)) std::cout No ; return; if (n * k == left) std::cout Yes ; else std::cout No ;int main() int T; std::cin T; while (T--) run(); C. Card Deck 对于一个集合 SSS，我们先来考虑怎么取，使得取出 SSS 的方式是唯一的，这样我们就不会重复计数了。一个取法是：每次拿出前 KKK 张牌，从中拿走 出现在 SSS 的卡牌，然后放回。 我们可以发现的一件事是：如果上一次取牌回合中有 aaa 张卡牌没有被拿走，那么这 aaa 张牌在后续轮次里也不会被拿走，也就是说，下一次拿牌相当于是在 K−aK-aK−a 张新牌里拿牌。 所以，我们考虑这 MMM 次操作每次拿走 viv_ivi​ 张牌，那么有 v1≥v2≥v3≥v4≥…vMv_1\\ge v_2\\ge v_3\\ge v_4\\ge\\dots v_Mv1​≥v2​≥v3​≥v4​≥…vM​，取牌的方案数为 ∑{ v }(Kv1)(v1v2)(v2v3)…(vM−1vM) \\sum_{\\set{v}} \\binom{K}{v_1}\\binom{v_1}{v_2}\\binom{v_2}{v_3}\\dots \\binom{v_{M-1}}{v_M} {v}∑​(v1​K​)(v2​v1​​)(v3​v2​​)…(vM​vM−1​​)因为要计算 size，所以改写一下要求的式子 ∑{ v }(v1+v2+v3+⋯+vM)(Kv1)(v1v2)(v2v3)…(vM−1vM) \\sum_{\\set{v}} (v_1+v_2+v_3+\\dots +v_M) \\binom{K}{v_1}\\binom{v_1}{v_2}\\binom{v_2}{v_3}\\dots \\binom{v_{M-1}}{v_M} {v}∑​(v1​+v2​+v3​+⋯+vM​)(v1​K​)(v2​v1​​)(v3​v2​​)…(vM​vM−1​​)乍一看这个式子非常难求，可实际上他就等于 KM(M+1)K2 \\frac{KM(M+1)^K}{2} 2KM(M+1)K​ 简单证明 我们用数学归纳法对 MMM 进行归纳证明。当 M=1M=1M=1 时，式子为 ∑1≤v≤Kv(Kv)\\sum_{1\\le v\\le K}v\\binom{K}{v}∑1≤v≤K​v(vK​). Code #include iostreamusing i64 = long long;constexpr i64 M = 998244353;constexpr i64 fpow(i64 b, i64 p) i64 res = 1; while (p) if (p 1) res = 1ll * res * b % M; b = 1ll * b * b % M; p = 1; return res;constexpr i64 inv2 = fpow(2, M - 2) % M;void run() i64 m, k; std::cin k m; i64 res = 1ll * k * m % M; res = 1ll * fpow(m + 1, k) * res % M; res = 1ll * res * inv2 % M; std::cout res ;int main() int t; std::cin t; while (t--) run(); D. Digits of Prefix Product 核心是考虑 ai=10d−1a_i=10^d-1ai​=10d−1. 以 x=36483,d=5x=36483, d=5x=36483,d=5 为例，相当于 x×10d−xx\\times 10^d-xx×10d−x，则有 x36483×10d3648300000−x−368433648263157 \\begin{array}{r|r} x 36483\\\\ \\times 10^d 3648300000\\\\ -x -36843\\\\ \\hline 3648263157 \\end{array} x×10d−x​364833648300000−368433648263157​​可以发现，只要 xxx 选的好，再令 d=len(x)d=len(x)d=len(x)，那么 (10d−1)x(10^d-1)x(10d−1)x 就基本上不会有相邻两个数位相同。所以我们只需要选好初值即可。 Code import syssys.set_int_max_str_digits(0)a = int(4267185198539595929746816985463149678978597531947912876793684139682713457571713713824196914894814137)b = aprint(a)for i in range(1, 10):\tval = int(9 * len(str(b)))\tb *= val\tprint(val) G. Guarding Plan 首先可以想到的是，有一些点 pip_ipi​ 可以被其他点 pjp_jpj​ 覆盖（只需要 pjp_jpj​ 在 pip_ipi​ 的右上角）。我们先排序结合单调栈预处理，排除那些一点会被其他现有的点覆盖掉的那些点。 接下来，我们考虑这样的点：不在凸包上，且没有被其他点覆盖的点 SSS。这就意味着我们必须在凸包上的点之间取点，来把 SSS 覆盖进去。 因为对于两个点 (x1,y1),(x2,y2)(x_1,y_1),(x_2,y_2)(x1​,y1​),(x2​,y2​) 来说，最小的能同时覆盖两个点的点是 (max⁡{ x1,x2 },max⁡{ y1,y2 })(\\max\\set{x_1,x_2},\\max\\set{y_1,y_2})(max{x1​,x2​},max{y1​,y2​})，我们只需要确保这个点如果在凸包内部那么就一定可以选点覆盖住这两个点。反之，如果这个点不在凸包内，那么这两个点一定不可能被一个点覆盖。 由此，一个重要的观察是：考虑一个凸包上的点 pi=(xi,yi)p_i=(x_i,y_i)pi​=(xi​,yi​)，和其两侧但不在凸包上的点 sa=(x1,y1),sb=(x2,y2)s_a=(x_1,y_1),s_b=(x_2,y_2)sa​=(x1​,y1​),sb​=(x2​,y2​)，他们满足 x1xix2,y1yiy2x_1\\lt x_i\\lt x_2, y_1\\gt y_i\\gt y_2x1​xi​x2​,y1​yi​y2​，则永远不可能在某条直线上选出点把这两个点覆盖起来。 所以，被凸包边覆盖的点，只可能被凸包边上的点覆盖。这话可能有点抽象，还是看图 Example 浅蓝色的点被蓝色的边覆盖（都在灰色线左侧），淡绿色的点被绿色的边覆盖（都在灰色线右侧），灰色线就是经过凸包点的竖直线 所以问题就变成了，在每个颜色的点的内部，计算出最少需要新建几个点、最少需要几个点才能全覆盖。这是一个 DP 问题，记录 dpi=(a,b)dp_i=(a,b)dpi​=(a,b) 表示最少需要 aaa 个点，需要新建 bbb 个点。 考虑第 iii 个点，我们可以写出如下的转移： 如果不新建点，直接把这个点作为 final guard，那么就有 dpi=dpi−1+(1,0) dp_i=dp_{i-1} + (1, 0) dpi​=dpi−1​+(1,0) 如果考虑新建点，那么我们需要选取 lll 使得 [l,i][l,i][l,i] 之间的所有点都可以被凸包边上的某个点覆盖，根据之前讨论的，也就是 (max⁡{ xl,xi },max⁡{ yl,yi })(\\max\\set{x_l,x_i}, \\max\\set{y_l,y_i})(max{xl​,xi​},max{yl​,yi​}) 在凸包内部，此时可以得出转移 dpi=dpl−1+(1,1) dp_i=dp_{l-1} + (1, 1) dpi​=dpl−1​+(1,1) 而且注意到，dpidp_idpi​ 是单调不减的，也就是我们只关心 min⁡l\\min lminl 即可。 接着，我们发现，由于点之前已经排过序，所以 xxx 必然严增且 yyy 必然严减，故 max⁡{ xl,xi }=xi\\max\\set{x_l,x_i}=x_imax{xl​,xi​}=xi​，所以，min⁡l⟺max⁡yl\\min l\\Longleftrightarrow \\max y_lminl⟺maxyl​，我们只需要维护一个单调队列即可。 所以总的时间复杂度为 O(nlog⁡n)O(n\\log n)O(nlogn) Code #include algorithm#include iostream#include queue#include utility#include vectorusing i64 = long long;using point = std::pairi64, i64;using vi = std::vectorint;constexpr int N = 2e5 + 10;std::ostream operator(std::ostream os, const point p) return os ( p.first , p.second ); point operator+(point a, point b) return a.first + b.first, a.second + b.second; int n;point p[N];int convex[N];int main() std::cin n; for (int i = 1; i = n; i++) std::cin p[i].first p[i].second; std::sort(p + 1, p + 1 + n); [] // extract useful points std::vectorpoint stk; for (int i = 1; i = n; i++) while (!stk.empty() p[i].second = stk.back().second) stk.pop_back(); stk.push_back(p[i]); for (int i = 1; i = stk.size(); i++) p[i] = stk[i - 1]; n = stk.size(); (); auto cross = [](int px, int a, int b) i64 v1x = p[a].first - p[px].first, v1y = p[a].second - p[px].second; i64 v2x = p[b].first - p[px].first, v2y = p[b].second - p[px].second; return v1x * v2y - v1y * v2x; ; std::vectorint stk; int tp = -1; for (int i = 1; i = n; i++) while (tp = 1 cross(stk[tp - 1], stk[tp], i) = 0) convex[stk.back()] = false; stk.pop_back(); tp--; convex[i] = true; stk.push_back(i); tp++; std::vectorpoint dp(n + 1, 0, 0); int cnt = 0, c1 = 0; for (int i = 1; i = n; i++) if (convex[i]) continue; int r = i; while (r = n !convex[r]) r++; // [i, r) std::queueint q; for (int l = i; l r; l++) dp[l] = dp[l - 1] + point1, 0; while (!q.empty()) int u = q.front(); point t = p[l].first, p[u].second; p[n + 1] = t; if (cross(i - 1, r, n + 1) = 0) break; q.pop(); if (!q.empty()) dp[l] = std::min(dp[l], dp[q.front() - 1] + point1, 1); q.push(l); cnt += dp[r - 1].second; c1 += dp[r - 1].first; i = r; std::cout stk.size() + c1 cnt ; K. K-rep Array 首先我们可以想到的一个思路是，如果 kkk 是合法的，那么对于 ∀i,j,i=j(modk)\\forall i,j,i=j\\pmod k∀i,j,i=j(modk)，要么 ai=−1a_i=-1ai​=−1 或者 aj=−1a_j=-1aj​=−1，要么 ai=aja_i=a_jai​=aj​. 那么一个 O(n2)O(n^2)O(n2) 的做法是显而易见的：枚举 kkk，然后枚举每一个 iii，用哈希表 hash[imod k]hash[i\\mod k]hash[imodk] 记录 aia_iai​ 是否都相等 考虑怎么加速这一过程。现在的判断方式基本不涉及四则运算，考虑改写一下是否有什么性质。让 ai=−1a_i=-1ai​=−1 的 aia_iai​ 变成 000，且令 f(d)=∑i−j=daiaj(ai−aj)2=∑i−j=daiaj3−2ai2aj2+ai3aj f(d)=\\sum_{i-j=d}a_ia_j(a_i-a_j)^2=\\sum_{i-j=d} a_ia_j^3 - 2a_i^2a_j^2 + a_i^3a_j f(d)=i−j=d∑​ai​aj​(ai​−aj​)2=i−j=d∑​ai​aj3​−2ai2​aj2​+ai3​aj​如果 kkk 是合法的，那么应该有 fk=f2k=⋯=fik=0f_k=f_{2k}=\\dots=f_{ik}=0fk​=f2k​=⋯=fik​=0. 只要我们可以求出 f(d),d∈[1,n]f(d),d\\in [1,n]f(d),d∈[1,n] 的值，那么我们枚举 kkk，然后检查 f(k),f(2k),…,f(⌊nk⌋⋅k)f(k),f(2k),\\dots,f(\\lfloor\\frac{n}{k}\\rfloor\\cdot k)f(k),f(2k),…,f(⌊kn​⌋⋅k) 就可以在 O(nlog⁡n)O(n\\log n)O(nlogn) 的时间内解出答案。 考虑怎么快速求出 f(d),d∈[1,n]f(d),d\\in [1,n]f(d),d∈[1,n]. 我们可以使用多项式乘法进行加速. 比如说，把 f(d)f(d)f(d) 拆成三个部分：∑i−j=daiaj3\\sum_{i-j=d}a_ia_j^3∑i−j=d​ai​aj3​，∑i−j=dai2aj2\\sum_{i-j=d}a_i^2a_j^2∑i−j=d​ai2​aj2​，∑i−j=dai3aj\\sum_{i-j=d}a_i^3a_j∑i−j=d​ai3​aj​ 对于第一部分 ∑i−j=daiaj3\\sum_{i-j=d}a_ia_j^3∑i−j=d​ai​aj3​，考虑构造这么两个多项式 g1(x)=a13+a23x+a33x2+a43x3+…an3xn−1g2(x)=an+an−1x+an−2x2+an−3x3+…a1xn−1 g_1(x)=a_1^3+a_2^3x+a_3^3x^2+a_4^3x^3+\\dots a_n^3x^{n-1}\\\\ g_2(x)=a_n+a_{n-1}x+a_{n-2}x^2+a_{n-3}x^3+\\dots a_1x^{n-1}\\\\ g1​(x)=a13​+a23​x+a33​x2+a43​x3+…an3​xn−1g2​(x)=an​+an−1​x+an−2​x2+an−3​x3+…a1​xn−1所以 h1(x)=g1(x)∗g2(x)h_1(x)=g_1(x)\\ast g_2(x)h1​(x)=g1​(x)∗g2​(x) 的卷积结果为 h1(x)=a13an+(a13an−1+a23an)x+(a13an−2+a23an−1+a33an)x2+… h_1(x)=a_1^3a_n+\\Big( a_1^3a_{n-1}+a_2^3a_{n} \\Big)x + \\Big( a_1^3a_{n-2} + a_2^3a_{n-1} + a_3^3a_{n} \\Big)x^2 + \\dots h1​(x)=a13​an​+(a13​an−1​+a23​an​)x+(a13​an−2​+a23​an−1​+a33​an​)x2+…我们可以发现的规律是：(∑i−j=daiaj3)xn−1−d\\Big( \\sum_{i-j=d} a_ia_j^3 \\Big)x^{n-1-d}(∑i−j=d​ai​aj3​)xn−1−d（我们这里只需要考虑前 n−1n-1n−1 项，即 d={ 1,2,…,n−1 }d=\\set{1,2,\\dots, n-1}d={1,2,…,n−1}） 同理，我们再做两次这样的卷积 h2(x),h3(x)h_2(x),h_3(x)h2​(x),h3​(x)，就可以求出 H(x)=h1(x)−2h2(x)+h3(x) H(x)=h_1(x)-2h_2(x)+h_3(x) H(x)=h1​(x)−2h2​(x)+h3​(x)再将 f(d)f(d)f(d) 对应到 H(x)H(x)H(x) 的系数就可以了。时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn) Code 我这里用 NTT 实现多项式卷积，FFT 用 long double 也不行，会有精度问题。模数用 998244343998244343998244343 也会被卡 #include algorithm#include cassert#include iostreamconstexpr int N = 2e5 + 10;constexpr int M = 524289; // 524288 + 1constexpr int MOD = 740294657;constexpr int P = 3;using i64 = long long;int n, lim = 1;i64 a[M], r[M];i64 op1[M], op2[M];i64 p1[M], p2[M], p3[M], p[M];int ans[N];int fpow(int a, int b, int mod) int res = 1; while (b) if (b 1) res = (i64)res * a % mod; a = (i64)a * a % mod; b = 1; return res;void NTT(i64 *d, int lim, int inv) for (int i = 0; i lim; i++) if (i r[i]) std::swap(d[i], d[r[i]]); for (int m = 2; m = lim; m = 1) int k = m 1; int gn = fpow(P, (MOD - 1) / m, MOD); for (int i = 0; i lim; i += m) int g = 1; for (int j = 0; j k; j++, g = 1ll * g * gn % MOD) int tmp = 1ll * d[i + j + k] * g % MOD; d[i + j + k] = (d[i + j] - tmp + MOD) % MOD; d[i + j] = (d[i + j] + tmp) % MOD; if (inv == -1) std::reverse(d + 1, d + lim); int inv_lim = fpow(lim, MOD - 2, MOD); for (int i = 0; i lim; i++) d[i] = 1ll * d[i] * inv_lim % MOD; int main() std::cin n; while (lim (n 1)) lim = 1; assert(lim M); for (int i = 0; i n; i++) std::cin a[i], a[i] += a[i] == -1; for (int i = 0; i lim; i++) r[i] = (i 1) * (lim 1) + (r[i 1] 1); // PART 1 for (int i = 0; i n; i++) op1[i] = a[i] * a[i] % MOD * a[i] % MOD; op2[i] = a[n - 1 - i]; NTT(op1, lim, 1), NTT(op2, lim, 1); for (int i = 0; i lim; i++) p1[i] = op1[i] * op2[i] % MOD, op1[i] = op2[i] = 0; NTT(p1, lim, -1); // PART 2 for (int i = 0; i n; i++) op1[i] = a[i] * a[i] % MOD; op2[i] = a[n - 1 - i] * a[n - 1 - i] % MOD; NTT(op1, lim, 1), NTT(op2, lim, 1); for (int i = 0; i lim; i++) p2[i] = op1[i] * op2[i] % MOD, op1[i] = op2[i] = 0; NTT(p2, lim, -1); // PART 3 for (int i = 0; i n; i++) op1[i] = a[i]; op2[i] = a[n - 1 - i] * a[n - 1 - i] % MOD * a[n - 1 - i] % MOD; NTT(op1, lim, 1), NTT(op2, lim, 1); for (int i = 0; i lim; i++) p3[i] = op1[i] * op2[i] % MOD, op1[i] = op2[i] = 0; NTT(p3, lim, -1); // H(x) for (int i = 0; i lim; i++) p[i] = ((p1[i] - 2ll * p2[i] % MOD + p3[i]) % MOD + MOD) % MOD; for (int d = n - 1, pw = 0; d 0; d--, pw++) ans[d] = p[pw] != 0; for (int g = 1; g n; g++) int w = ans[g]; for (int j = 2 * g; j n; j += g) w |= ans[j]; std::cout 1 - w; std::cout 1 ; M."},{"title":"The 3rd Universal Cup. Stage 40: Potyczki","path":"/wiki/algo_contests/qoj2135.html","content":"A. AGI 至少答对 ⌊n−12⌋\\lfloor \\frac{n-1}{2}\\rfloor⌊2n−1​⌋ 个 evaluation 这个条件非常独特：/2/2/2 暗示，如果我们可以将线段两两分组，只要每组至少对一个，那么就 OK 了. 然后我们考虑怎么分组。考虑每一组的两条线段 a,ba,ba,b：如果 a,ba,ba,b 是不相交的，那么只要都 reject 就可以保证条件了；如果 aaa 包含了 bbb，那么我们可以 accept aaa 且 reject bbb 即可（第三种情况，即如果 a,ba,ba,b 相交，我们先等会讨论） 所以我们可以先得出一个初步的贪心策略：将线段按左端点排完序后，用 std::deque 维护还没有被匹配掉的线段，每有一条新线段，优先查看是否能其他匹配出第一、二种情况，第一种情况和 deque.front() 判断是否相离，第二种情况和 deque.back() 判断。 所以就剩下线段相交的情况了，并且 deque 里任意两条线段都有交集（不然的话，如果 front 和 back 没有交集，他们应该会在上面的过程里被匹配掉），这个就直接交替 reject 和 accept 即可。 时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn) Code #include bits/extc++.h#include bits/stdc++.hconstexpr int N = 5e5 + 10;struct Segment int l, r, id; seg[N];int mark[N];int n;void run() std::cin n; for (int i = 1; i = n; i++) seg[i].id = i; mark[i] = false; std::cin seg[i].l seg[i].r; std::sort(seg + 1, seg + 1 + n, [](const Segment a, const Segment b) if (a.l != b.l) return a.l b.l; else if (a.r != b.r) return a.r b.r; else return a.id b.id; ); std::dequeSegment q; for (int i = 1; i = n; i++) if (!q.empty() q.front().r = seg[i].l) mark[q.front().id] = false; mark[seg[i].id] = false; q.pop_front(); continue; if (!q.empty() q.back().r = seg[i].r) mark[q.back().id] = true; mark[seg[i].id] = false; q.pop_back(); continue; q.push_back(seg[i]); int t = false; for (auto index : q) mark[index.id] = t; t = 1 - t; for (int i = 1; i = n; i++) std::cout (mark[i] ? T : N); std::cout ;int main() #ifdef ONLINE_JUDGE std::cin.tie(nullptr)-sync_with_stdio(false); std::cout.tie(nullptr);#endif int t; std::cin t; while (t--) run(); B. Collatz Sum 感觉就差了一点，想到了按奇偶分类递归，归纳成等差数列的形式，但是在处理如何递归的时候想错了：想的是按序列长度的奇偶性划分，但是实际上应该按照公差和首项分类。 考虑到 N≤1012N\\le 10^{12}N≤1012 但是 k≤32k\\le 32k≤32，所以不可能暴力。但是 kkk 比较小，考虑能不能在 kkk 上做点文章。 我们发现，如果一个序列都是奇数或者都是偶数，那么我们可以直接 consume 掉一层 f(x)f(x)f(x)： ∑x are evenfk(x) ⟹ ∑x/2fk−1(f(x)) ⟹ ∑x/2fk−1(x/2)∑x are oddfk(x) ⟹ ∑3x+1fk−1(f(x)) ⟹ ∑3x+1fk−1(3x+1) \\sum_{x\\text{ are even}} f^k(x) \\implies \\sum_{x/2} f^{k-1}(f(x))\\implies \\sum_{x/2} f^{k-1}(x/2)\\\\ \\sum_{x\\text{ are odd}} f^k(x) \\implies \\sum_{3x+1} f^{k-1}(f(x))\\implies \\sum_{3x+1} f^{k-1}(3x+1) x are even∑​fk(x)⟹x/2∑​fk−1(f(x))⟹x/2∑​fk−1(x/2)x are odd∑​fk(x)⟹3x+1∑​fk−1(f(x))⟹3x+1∑​fk−1(3x+1)所以这就是我们的 baseline，而且同时注意到当我把等差数列划分成奇偶数 { l },{ r }\\set{l},\\set{r}{l},{r} 的时候，{ l },{ r }\\set{l},\\set{r}{l},{r} 也会是等差数列，且公差恰好翻倍。 若当前序列长度为 nnn 且公差 ddd 为偶数，说明此时这个等差数列奇偶性相同，可以直接 consume 一层 f(x)f(x)f(x) 然后递归；否则如果 ddd 是奇数，说明当前序列可以分解成 222 个公差为 2d2d2d 的等差序列，且长度为 n/2n/2n/2，可以进一步递归。 考虑这样做的时间复杂度。考虑长度为 nnn 的等差序列，公差 ddd 为奇数，我们分成奇偶两部分，对于奇数部分，可以连拆两层 x←3x+12x\\gets \\frac{3x+1}{2}x←23x+1​，而偶数部分可以拆一层 f(x)f(x)f(x)。 我们直接考虑每走 333 步会分出多少序列，我们用 (s,d,n)(s,d,n)(s,d,n) 表示首项为 sss、公差为 ddd、有 nnn 项的等差数列： graph TB; A(\"(1,1,n)\") -->|1 step| A1[[\"(1,2,n/2)\"]] A1 -->|\"1 step, consume f(x)\"| A1t(\"(4,6,n/2)\") -->|\"1 step, consume f(x)\"| A1tt[[\"(2, 3, n/2)\"]] A -->|1 step| A0[[\"(2,2,n/2)\"]] -->|\"1 step, consume f(x)\"| A0t(\"(1,1,n/2)\") A0t -->|1 step| A01[[\"(1,2,n/4)\"]] A01 -->|\"1 step, consume f(x)\"| A01t(\"(4,6,n/4)\") A0t -->|1 step| A00[[\"(2,2,n/4)\"]] A00 -->|\"1 step, consume f(x)\"| A00t(\"(1,1,n/4)\") A1tt -->|\"1 step\"| A11(\"(2,6,n/4)\") A1tt -->|\"1 step\"| A10(\"(5,6,n/4)\") A11 -->|\"1 step, consume f(x)\"| A11t[[\"(1,3,n/4)\"]] A10 -->|\"1 step, consume f(x)\"| A10t[[\"(16,3,n/4)\"]] A00t -->|\"1 step\"| A001[[\"(1,2,n/8)\"]] A00t -->|\"1 step\"| A000[[\"(2,2,n/8)\"]] A01t -->|\"1 step, consume f(x)\"| A01tt[[\"(2,3,n/4)\"]] A01tt --> X(....) 继续推下去，可以发现每层的方块节点数量是比较有规律的，是 Fibonacci 数列，所以时间复杂度差不多是 O(ϕk),ϕ=1+52O(\\phi^k),\\phi=\\frac{1+\\sqrt 5}{2}O(ϕk),ϕ=21+5​​. 可以通过此题 Code #include maths/integers/modint.hpp#include bits/stdc++.husing i64 = long long;using Vector = std::arrayZn, 3;i64 n, k;Zn inv2 = Zn(1) / 2;Zn sum(i64 start, i64 d, i64 num) Zn sum = start, dt = d; sum *= num; dt *= num, dt *= (num - 1), dt *= inv2; return sum + dt;// start, d, num: 记录等差数列Zn solve(i64 start, i64 d, i64 num, int level, int depth = 0) if (num = 0) return Zn(0); if (level == 0) return sum(start, d, num); if (d % 2 == 0) // consume one level of f() if (start % 2 == 1) return solve(start * 3 + 1, d * 3, num, level - 1, depth); else return solve(start / 2, d / 2, num, level - 1, depth); // split into 2 parts: odd, even else return solve(start, d * 2, (num + 1) / 2, level, depth + 1) + solve(start + d, d * 2, num / 2, level, depth + 1);int main() std::cin n k; std::cout solve(1, 1, n, k) ; E. One Bit 题目要求最少改几个数，我们正难则反，来求最多可以不改多少个数。由于相邻两个数最多只能有一个 bit 不同，所以对于 ai,aj,ija_i,a_j,i\\gt jai​,aj​,ij 来说，最多有 ∣i−j∣|i-j|∣i−j∣ 个 bit 不同。记 dpidp_idpi​ 表示 a1∼aia_1\\sim a_ia1​∼ai​ 最多可以保留 dpidp_idpi​ 个数，则 dpi=max⁡ji{ dpj+1 }if bitdiff(ai,aj)≤∣i−j∣ dp_i=\\max_{j\\lt i}\\set{dp_j+1} \\quad\\text{if } \\mathop{\\mathrm{bitdiff}}(a_i,a_j)\\le |i-j| dpi​=jimax​{dpj​+1}if bitdiff(ai​,aj​)≤∣i−j∣乍一看是 O(n2)O(n^2)O(n2) 的，但考虑到 ai≤260a_i\\le 2^{60}ai​≤260 所以我们只需要考虑 i−64≤jii-64\\le j\\lt ii−64≤ji 就行了，对于 ji−64j\\lt i-64ji−64 我们只需要保留 dp1∼dpjdp_1\\sim dp_jdp1​∼dpj​ 的最大值即可。 时间复杂度 O(n)O(n)O(n) Code #include bits/extc++.h#include bits/stdc++.htemplate typename K, typename Vusing umap = __gnu_pbds::gp_hash_tableK, V;using i64 = long long;constexpr int MAXN = 3e5 + 10;i64 a[MAXN];int n;int dp[MAXN], g[MAXN];int main() std::cin n; for (int i = 1; i = n; i++) std::cin a[i]; for (int i = 1; i = n; i++) dp[i] = 1; for (int j = std::max(1, i - 64); j i; j++) int v = std::__popcount(a[i] ^ a[j]); if (v = i - j) dp[i] = std::max(dp[i], dp[j] + 1); if (i = 65) dp[i] = std::max(dp[i], g[i - 64] + 1); g[i] = std::max(g[i - 1], dp[i]); int ans = 0; for (int i = 1; i = n; i++) ans = std::max(ans, dp[i]); std::cout n - ans ; H. Merge Sort 考虑把 [l,m)[l,m)[l,m) 和 [m,r)[m, r)[m,r) 两个区间归并起来的过程。我们希望 a,ba,ba,b 这两个数在这个双指针归并的过程中会产生比较，那么一定是左右两个指针同时指到 a,ba,ba,b，也就是说排在 a,ba,ba,b 之前的数应该都小于 aaa。 换句话说，我们关心的只有 (a,b)(a,b)(a,b) 这个区间的数，因为这些数绝对不能出现在 bbb 所在的一侧，因为这样会导致 aaa 提前被双指针归并掉；而 aaa 这一侧取什么数都无所谓。 所以，我们这样计数：先选出 bbb 这一侧的所有数字，然后进行全排列；然后在剩下的数字里，选出 aaa 这一侧的数字，然后进行全排列；最后，对 [0,l),[r,n)[0, l), [r, n)[0,l),[r,n) 的所有剩下的数字进行全排列，于是 [l,m)[l,m)[l,m) 和 [m,r)[m, r)[m,r) 归并的答案为 Cn−(b−a+1)∣B∣−1⋅A∣B∣∣B∣⋅Cn−∣B∣−1∣A∣−1⋅A∣A∣∣A∣⋅An−∣A∣−∣B∣n−∣A∣−∣B∣ C_{n-(b-a+1)}^{|B|-1}\\cdot A_{|B|}^{|B|}\\cdot C_{n-|B|-1}^{|A|-1}\\cdot A_{|A|}^{|A|}\\cdot A_{n-|A|-|B|}^{n-|A|-|B|} Cn−(b−a+1)∣B∣−1​⋅A∣B∣∣B∣​⋅Cn−∣B∣−1∣A∣−1​⋅A∣A∣∣A∣​⋅An−∣A∣−∣B∣n−∣A∣−∣B∣​这里 ∣B∣|B|∣B∣ 表示 bbb 这一侧的数组长度，∣A∣|A|∣A∣ 表示 aaa 这一侧的数组长度 Cn−(b−a+1)∣B∣−1C_{n-(b-a+1)}^{|B|-1}Cn−(b−a+1)∣B∣−1​ 中 n−(b−a+1)n-(b-a+1)n−(b−a+1) 表示 bbb 这一侧可以选择的数字个数（不能选 a,ba,ba,b 和 (a,b)(a,b)(a,b) 之间的数） ∣B∣−1|B|-1∣B∣−1 表示要选择多少个数，之所以 −1-1−1 是因为必须放一个 bbb A∣B∣∣B∣A_{|B|}^{|B|}A∣B∣∣B∣​ 是全排列。我们只关心排列个数，是因为在自底向上归并的过程中这一块一定会被排序成有序的。 Cn−∣B∣−1∣A∣−1C_{n-|B|-1}^{|A|-1}Cn−∣B∣−1∣A∣−1​ 中 n−∣B∣−1n-|B|-1n−∣B∣−1 是 aaa 这一侧可以选的数字个数，因为 aaa 侧什么数都可以取，但是不能取已经在 bbb 侧被取走的数 ∣A∣−1|A|-1∣A∣−1 是因为 aaa 侧必须放一个 aaa An−∣A∣−∣B∣n−∣A∣−∣B∣A_{n-|A|-|B|}^{n-|A|-|B|}An−∣A∣−∣B∣n−∣A∣−∣B∣​ 是剩下的数进行全排列。 这样的复杂度是多少呢？因为是从 [0,n)[0, n)[0,n) 开始的，所以至多 O(log⁡n)O(\\log n)O(logn) 个不同的长度，对于一个长度，我们只需要计算一次，然后再计算这个长度出现的多少次即可。总体时间复杂度 O(Tlog⁡n)O(T\\log n)O(Tlogn). Code 注意，这里我没有计算长度出现几次，而是用 dp[len] += dp[left] + dp[right] 的方式。 不加这行代码的话，比如说 len 之前已经计算了一次，那么 if 会直接 return，进而少计算一次 (len+1)/2 的情况而导致答案错误。 #include maths/integers/modint.hpp#include bits/extc++.h#include bits/stdc++.hconstexpr int N = 1e6 + 10;template class K, class Vusing umap = __gnu_pbds::gp_hash_tableK, V;Zn fac[N], ifac[N];void precompute() fac[0] = 1; for (int i = 1; i N; i++) fac[i] = fac[i - 1] * i; ifac[N - 1] = Zn(1) / fac[N - 1]; for (int i = N - 2; i = 0; i--) ifac[i] = ifac[i + 1] * (i + 1);Zn A(int n, int k = -1) if (k == -1) return fac[n]; else return fac[n] * ifac[n - k];Zn C(int n, int k) if (k 0 || k n) return 0; return fac[n] * ifac[k] * ifac[n - k];void run() int n, a, b; std::cin n a b; if (a b) std::swap(a, b); int m = b - a - 1; Zn ans = 0; umapint, Zn dp; dp[1] = 0; auto dfs = [](auto F, int l, int r, int len) if (len == 1 || dp.find(len) != dp.end()) ans += dp[len]; return; assert(r - l == len); Zn tmp = 0; int left = (len + 1) / 2, right = len - left; tmp += A(left) * A(right) * C(n - m - 2, right - 1) * C(n - right - 1, left - 1) * A(n - len); tmp += A(right) * A(left) * C(n - m - 2, left - 1) * C(n - left - 1, right - 1) * A(n - len); dp[len] = tmp; ans += tmp; F(F, l, l + left, left); dp[len] += dp[left]; F(F, l + left, r, right); dp[len] += dp[right]; ; dfs(dfs, 0, n, n); std::cout ans ;int main() precompute(); int t; std::cin t; while (t--) run(); I. Santa Claus 细节题。首先解题关键就是排序不等式。我们一步一步进行处理： 如果 a[],b[]a[], b[]a[],b[] 都有 0\\gt 00 的部分，我们就先匹配他们：最大匹配最大，最小匹配最小；都有 0\\lt 00 的部分也是同样的处理。如果此时已经超过 kkk 个了，直接结束。 否则接着处理 a[],b[]a[], b[]a[],b[] 中 =0=0=0 的部分， N. Frequency Function 我们模拟一次 a ⟹ f(a)a \\implies f(a)a⟹f(a)，就会发现如果 { a }\\set{a}{a} 长度为 nnn，则 f(a)f(a)f(a) 里至多有 O(n)O(\\sqrt n)O(n​) 个数是非零的。 这个怎么证明呢？考虑让 f(a)f(a)f(a) 里非零的数尽可能得多。由于 { f(a) }v\\set{f(a)}_v{f(a)}v​ 表示 { a }\\set{a}{a} 里 ai=va_i=vai​=v 的 iii 的个数，所以为了让 f(a)f(a)f(a) 里非零的个数尽可能多，我们优先挑选 vvv 较小的。比如说我们挑选了 1…k1\\dots k1…k 非零，那么也就是说，{ a }\\set{a}{a} 里有 111 个 111、222 个 222…… 即 12+22+33+⋯+k2≤n 1^2+2^2+3^3+\\dots +k^2\\le n 12+22+33+⋯+k2≤n所以我们可以推断 k=O(n)k=O(\\sqrt n)k=O(n​). 也就是说，经过有限次 f()f()f() 操作后，就可以变成终态 1,0,0,0,…1,0,0,0,\\dots1,0,0,0,…. 也就是说，如果询问的 kkk 比较小，我们可以直接维护出来；否则，我们大概率只需要取前 600600600 个 f10(a)f^{10}(a)f10(a) 然后迭代不超过 323232 次（这个 323232 是随便取的数）就可以计算出 fk(a)f^k(a)fk(a) 了。这里的时间复杂度差不多是 O(n)O(\\sqrt n)O(n​) 查询有了，那么修改呢？假设我们维护了 LLL 层，即 f0(a),f1(a),…,fL−1(a),fL(a)f^0(a), f^1(a),\\dots,f^{L-1}(a), f^L(a)f0(a),f1(a),…,fL−1(a),fL(a)，把 f0(a)if^0(a)_if0(a)i​ 从 vvv 修改为 v′vv′，需要在 f1(a)f^1(a)f1(a) 里扣除 vvv 的一次出现次数，然后再加上 v′vv′ 的一次出现次数；对于 f1(a)f^1(a)f1(a)，想要 f1(a)[v]←f1(a)[v]−1f^1(a)[v]\\gets f^1(a)[v]-1f1(a)[v]←f1(a)[v]−1，令 x=f1(a)[v]x=f^1(a)[v]x=f1(a)[v] 需要先在 f2(a)[x]f^2(a)[x]f2(a)[x] 扣除 111，更新完 f1(a)[v]f^1(a)[v]f1(a)[v] 后再在 f2(a)f1(a)[v]′f^2(a)_{f^1(a)[v]}f2(a)f1(a)[v]′​ 加上 111…… 于是我们发现这就是一个递归的关系，终止条件是到达第 LLL 层或者当前位置的值为 000（我们只关心值域 1∼n1\\sim n1∼n 的出现次数）。令这一部分时间复杂度为 T(L)T(L)T(L)，则有 T(L)=2T(L−1)+O(1) T(L)=2T(L-1)+O(1) T(L)=2T(L−1)+O(1)解得这一部分是 T(L)=O(2L)T(L)=O(2^L)T(L)=O(2L)，实现里，我们可以把 LLL 取得小一点。 于是总的时间复杂度是 O(nL+q(2L+32B))O(nL+q(2^L+32B))O(nL+q(2L+32B)). 实现里，我取 L=10L=10L=10，查询时取前 B=600B=600B=600，迭代最多 323232 次。（建议取小一点，这个取的差点 TLE 了） Code #include bits/stdc++.hconstexpr int N = 3e5 + 10;constexpr int LV = 10;int a[N], n, m;int cnt[LV][N], t[N], r[N];void update(int level, int x, int val) if (x = 0 || level = LV) return; if (cnt[level][x] 0) update(level + 1, cnt[level][x], -1); // remove cnt[level][x] += val; if (cnt[level][x] 0) update(level + 1, cnt[level][x], 1); // addvoid log() for (int lv = 0; lv LV; lv++) std::cout Level lv : ; for (int i = 1; i = n; i++) std::cout cnt[lv][i] ; std::cout ; int main() std::cin n m; for (int i = 1; i = n; i++) std::cin a[i], cnt[0][i] = a[i]; for (int i = 1; i LV; i++) for (int j = 1; j = n; j++) cnt[i][cnt[i - 1][j]] += 1; while (m--) int op, x, y; std::cin op x y; if (op == 2) if (x LV) std::cout cnt[x][y] ; continue; for (int i = 1; i = 600; i++) r[i] = cnt[LV - 1][i]; for (int lv = LV; lv = std::min(32, x); lv++) for (int i = 1; i = 600; i++) t[i] = r[i], r[i] = 0; for (int i = 1; i = 600; i++) r[t[i]] += 1; std::cout r[y] ; else update(1, a[x], -1); // remove old value a[x] = cnt[0][x] = y; update(1, a[x], 1); // add new value"},{"title":"2025 清华大学算法竞赛训练营 Day 2","path":"/wiki/algo_contests/thucamp2025-day2.html","content":"B. Forcefield 题意：平面上 n≤100n\\le 100n≤100 个圆，半径可能不同，但保证互不重叠。你需要用一根皮筋把所有的圆全都包住，要求皮筋的周长最小，同时求出此时皮筋包住的区域的面积。 本场一道比较好写的计算几何。首先考虑两个圆 A,BA,BA,B 的情况，一定是 A,BA,BA,B 的一段弧再加上两条外公切线，就能把这两个圆包住，同时长度最小。 我们把这段皮筋分成两个部分：直线部分和弧线部分。直线部分就是外公切线，弧线就是圆弧。 所以我们把它扩展到多个圆的情况：我们先两两求出其外公切点，然后对这些点我们求出凸包。这样，我们得到的凸包全都是外公切点，相邻两点之间的连线要么是圆的割线，要么是两个圆之间的外公切线。 现在假设我们的凸包是按顺时针，我们先算周长 CCC。每次拿出相邻两点 a,ba,ba,b，如果 a,ba,ba,b 属于不同的圆，则他们构成外公切线，即 C←C+∣ab∣C\\gets C+|ab|C←C+∣ab∣；否则找出 ababab 这条割线对应的弧，如果其对应的圆心在凸包里面，则对应的是劣弧，否则对应的是优弧。 这里有一个 trick。令凸包上的点是顺时针排列的，那么考虑凸包上相邻两点 vi,vi+1v_i,v_{i+1}vi​,vi+1​ 和其对应的圆心 ccc，考虑叉积 V=vic⃗×vi+1c⃗V=\\vec{v_ic}\\times\\vec{v_{i+1}c}V=vi​c​×vi+1​c​：如果 V≥0V\\ge 0V≥0，那么圆心在凸包外；否则 V0V\\gt 0V0，则圆心在凸包内。 面积也是类似的。我们先计算凸包的面积，然后如果相邻两点 a,ba,ba,b 属于同一个圆，如果圆心 ccc 在凸包内部，则先减去 abcabcabc 的面积，再加上一段劣弧对应的扇形面积；如果在凸包外部，则先加上 abcabcabc 三角形的面积，再加上优弧对应的面积。 Code 注意几点： 精度问题：long double 和 set_precision(20) #include bits/stdc++.h#include iomanip#define M_PI 3.141592653589793#define double long doubleusing namespace std;constexpr int N = 1000;constexpr double eps = 1e-9;struct point double x, y; int id;;point operator-(point a, point b) return a.x - b.x, a.y - b.y, -1; double scale(point a) return sqrt(a.x * a.x + a.y * a.y); double dot(point a, point b) return a.x * b.x + a.y * b.y; double cross(point a, point b) return a.x * b.y - a.y * b.x; int sign(double l, double r) if (l - r eps) return 1; else if (r - l eps) return -1; else return 0;struct circle point c; double r; int id; point coord(double ang) double cs = cos(ang), ss = sin(ang); return c.x + r * cs, c.y + r * ss, id; ;vectorpoint p;circle c[N];int n;tuplevectorpoint, vectorpoint tangent(circle A, circle B) vectorpoint a, b; if (A.r B.r) swap(A, B), swap(a, b); double dist = sqrt((A.c.x - B.c.x) * (A.c.x - B.c.x) + (A.c.y - B.c.y) * (A.c.y - B.c.y)); double rsum = A.r + B.r, rdif = A.r - B.r; double base = atan2(B.c.y - A.c.y, B.c.x - A.c.x); double ang = acos(rdif / dist); a.push_back(A.coord(base + ang)), b.push_back(B.coord(base + ang)); a.push_back(A.coord(base - ang)), b.push_back(B.coord(base - ang)); double ang1 = acos(rsum / dist); a.push_back(A.coord(base + ang1)), b.push_back(B.coord(base + ang1 + M_PI)); a.push_back(A.coord(base - ang1)), b.push_back(B.coord(base - ang1 + M_PI)); return a, b;int main() #ifdef ONLINE_JUDGE std::cin.tie(0)-sync_with_stdio(0);#endif std::cin n; for (int i = 1; i = n; i++) std::cin c[i].c.x c[i].c.y c[i].r, c[i].id = i; for (int i = 1; i = n; i++) for (int j = i + 1; j = n; j++) auto [x, y] = tangent(c[i], c[j]); for (auto v : x) p.push_back(v); for (auto v : y) p.push_back(v); // convex auto v = [] sort(p.begin(), p.end(), [](point a, point b) if (sign(a.x, b.x) != 0) return a.x b.x; else return a.y b.y; ); auto crs = [](point p, point a, point b) return cross(a - p, b - p); ; vectorpoint stack; int top = -1, m = p.size(); for (int i = 0; i m; i++) while (top = 1 sign(crs(stack[top - 1], stack[top], p[i]), 0) = 0) stack.pop_back(); top--; stack.push_back(p[i]), top++; int t = top; for (int i = m - 2; i = 0; i--) while (top t sign(crs(stack[top - 1], stack[top], p[i]), 0) = 0) stack.pop_back(); top--; stack.push_back(p[i]); top++; return stack; (); std::reverse(v.begin(), v.end()); double C = 0; double A = 0; // compute A for (int i = 0, m = v.size(); i + 1 m; i++) A -= cross(v[i], v[i + 1]) * 0.5; for (int i = 0, m = v.size(); i + 1 m; i++) if (v[i].id != v[i + 1].id) C += scale(v[i] - v[i + 1]); continue; int cid = v[i].id; double len = scale(v[i] - v[i + 1]); double val = (c[cid].r * c[cid].r * 2 - len * len) / (c[cid].r * c[cid].r * 2); double cosv = acos(min(1.0L, max(-1.0L, val))); point v1 = v[i] - c[cid].c; point v2 = v[i + 1] - c[cid].c; A += cross(v[i] - c[cid].c, v[i + 1] - c[cid].c) * 0.5; if (sign(cross(v[i] - c[cid].c, v[i + 1] - c[cid].c), 0) = 0) cosv = 2 * M_PI - cosv; C += c[cid].r * cosv; A += cosv * c[cid].r * c[cid].r * 0.5; else C += c[cid].r * cosv, A += cosv * c[cid].r * c[cid].r * 0.5; std::cout setprecision(20) fixed C A ;"},{"title":"2025 清华大学算法竞赛训练营 Day 4","path":"/wiki/algo_contests/thucamp2025-day4.html","content":"9/13，感觉这一场的出题人比较仁慈（ E - Enduring the Pokémon 【题意】 有 nnn 只宝可梦，每只拥有力量 aia_iai​ 和购买费用 bib_ibi​。 你可以任选任意多只买下，作为“自己的宝可梦”。 安排其余只按自定顺序与它单挑。 若当前力量严格大于对手则获胜，力量 +1+1+1，继续；否则立刻淘汰。 若最终击败全部对手，则赢得比赛。 求能完成上述目标的最小购买费用，若无解输出 −1-1−1。 首先可以判断的是如何安排剩下的 pokemon，肯定是先把力量低的放在前面刷经验，initial strength 比他大的则从小到大放在后面。 然后一个重要的观察是，如果某只 strength 为 xxx 的 pokemon 可以作为答案，那么所有 strength ≥x\\ge x≥x 的都可以作为答案。 接着我们发现，我们只需要一只 pokemon 即可。考虑两只 pokemon 其力量分别为 x,y,xyx,y,x\\lt yx,y,xy，如果 x,yx,yx,y 有一个可以，那么我们只需要一个；如果 x,yx,yx,y 都不行，说明 yyy 就算刷经验了也打不过 boss，那么两个臭皮匠也还是打不过诸葛亮。 所以我们发现了可以二分的性质：把 pokemon 按力量排序，找到力量最小的 xxx，使得其可以可以通过刷经验的方式干掉所有其他的 pokemon. check() 的话可以直接 O(n)O(n)O(n) 扫描判断。找到后还要找出力量 ≥x\\ge x≥x 最小的花费。 总时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn) AC Code #include bits/stdc++.h#define FOR(i, l, r) for(int i = (l); i = (r); i++)#define ROF(i, r, l) for(int i = (r); i = (l); i--)#define sz(a) int((a).size())#define ll long long#define vi vectorint#define pii pairint, intusing namespace std;const int N = 1e5 + 10;int n, a[N], b[N];pii c[N];int check(int x) int v = a[x]; FOR(i, 1, n) if(i != x) if(a[i] v) v++; else return 0; return 1;int main() ios :: sync_with_stdio(0), cin.tie(0), cout.tie(0); cin n; FOR(i, 1, n) cin a[i]; FOR(i, 1, n) cin b[i]; FOR(i, 1, n) c[i] = a[i], b[i]; sort(c + 1, c + n + 1); FOR(i, 1, n) a[i] = c[i].first; b[i] = c[i].second; int l = 1, r = n, best = -1; while(l = r) int mid = (l + r) / 2; if(check(mid)) best = mid, r = mid - 1; else l = mid + 1; if(best == -1) cout -1 ; return 0; int ans = b[n]; FOR(i, best, n) ans = min(ans, b[i]); cout ans ; H - Hashing 【题意】给定长度为 nnn 的数组 aaa，计算所有连续子数组 a[l…r]a[l\\dots r]a[l…r] 的“kkk 进制模 ppp 哈希值”等于 xxx 的子数组个数。保证 ppp 是素数，x,kpx,k\\lt px,kp. 子数组 a[l…r]a[l\\dots r]a[l…r] 哈希值定义为：H(l,r)=∑i∈[l,r]a[i]×kr−imod pH(l,r)=\\sum_{i\\in[l,r]}a[i]\\times k^{r-i} \\mod pH(l,r)=∑i∈[l,r]​a[i]×kr−imodp 经典的序列统计数对思想：用数据结构维护 1∼i−11\\sim i-11∼i−1 并用数据结构来求 (x,i),xi(x,i),x\\lt i(x,i),xi 的个数。这样可以做到不重不漏。 考虑如何条件的 (l,r),lr(l,r),l\\lt r(l,r),lr 满足什么条件 H(l,r)≡x(modp) H(l,r)\\equiv x\\pmod p H(l,r)≡x(modp)我们计算哈希的前缀和 H(i)=H(1,i)H(i)=H(1,i)H(i)=H(1,i)，用两段前缀相减计算 H(l,r)H(l,r)H(l,r) H(r)−H(l−1)×kr−l+1≡x(modp) H(r)-H(l-1)\\times k^{r-l+1}\\equiv x\\pmod p H(r)−H(l−1)×kr−l+1≡x(modp)移项，使得一侧只有 lll 另一侧只有 rrr H(r)−xkr≡H(l−1)kl−1(modp) \\frac{H(r)-x}{k^r}\\equiv \\frac{H(l-1)}{k^{l-1}} \\pmod p krH(r)−x​≡kl−1H(l−1)​(modp)所以，计算出 rrr 对应的值（等式左侧），我们只需要在数据结构里查询多少 lll 满足上式即可。考虑到 ppp 比较大，这里选用的是 std::map. 时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn) AC Code 注意插入 H(0)=1H(0) = 1H(0)=1，把 l=1l=1l=1 的情况算进去。 #include bits/stdc++.husing ll = long long;constexpr int N = 2e5 + 10; ll n, k, p, x;ll a[N], h[N];std::mapll, int cnt;ll base[N], ibase[N]; ll fpow(ll b, ll p, ll m) ll res = 1; while (p) if (p 1) res = res * b % m; b = b * b % m; p = 1; return res; int main() #ifdef ONLINE_JUDGE std::cin.tie(0)-sync_with_stdio(0);#endif std::cin n k p x; for (int i = 1; i = n; i++) std::cin a[i]; base[0] = 1; for (int i = 1; i = n; i++) base[i] = base[i - 1] * k % p; ibase[n] = fpow(base[n], p - 2, p); for (int i = n - 1; i = 0; i--) ibase[i] = ibase[i + 1] * k % p; h[0] = 0; ll ans = 0; cnt[0] = 1; for (int i = 1; i = n; i++) h[i] = (h[i - 1] * k + a[i]) % p; ll q = ((h[i] - x) % p + p) % p * ibase[i] % p; ans += cnt[q]; cnt[h[i] * ibase[i] % p]++; std::cout ans ; J - Just a Turtle Problem 给定长度 nnn 的数组 aaa，构造 n×nn\\times nn×n 表格： 第 111 行：a1,a2,…,ana_1,a_2,\\dots,a_na1​,a2​,…,an​； 第 iii 行 (i≥2)(i\\ge 2)(i≥2)：把第 i−1i-1i−1 行做左循环移位（首元素移到末尾）。 海龟从左上 (1,1)(1,1)(1,1) 出发，每次只能向右或向下走一步，目标到达右下 (n,n)(n,n)(n,n)。求经过格子数字之和的最小值。1≤n≤100 0001\\le n\\le 100\\,0001≤n≤100000，∣ai∣≤109\\lvert a_i\\rvert\\le 10^9∣ai​∣≤109。 结论：无论怎么走，路径和是固定的。 这是因为考虑 n×nn\\times nn×n 方格的从左下往右上的斜线上的数，由于每行都是 left shift，所以都是相等的。所以 (i,j)(i,j)(i,j) 出发可以到达的两个点 (i+1,j),(i,j+1)(i+1,j),(i,j+1)(i+1,j),(i,j+1) 上的数是一样的。 所以答案就是 2∑i=1nai−an2\\sum_{i=1}^n a_i-a_n2∑i=1n​ai​−an​（因为 ana_nan​ 的大斜线只有一条） AC Code 代码是从 000 下标开始的，所以有些许出入。 #include bits/stdc++.husing ll = long long;void solve() int n; std::cin n; std::vectorint a(n); for (int x : a) std::cin x; ll ans = a[0]; for (int i = 1; i = 2 * (n - 1); i++) ans += a[i % n]; std::cout ans ;int main() std::ios::sync_with_stdio(false); std::cin.tie(nullptr); int T = 1; while (T--) solve(); return 0; K - Keeping the Medians Close 1 首先我们可以猜到答案的下界是 cn+1−cnc_{n+1}-c_ncn+1​−cn​. 于是任务就是把 n−12\\frac{n-1}{2}2n−1​ 个小于和大于中位数的数通过 swap 换到两个序列里面，而这个一定是可以做到的。不妨设 aaa 序列里面 cn\\lt c_ncn​ 的数更多（有 mn−12m\\gt \\frac{n-1}{2}m2n−1​ 个），那么 bbb 里面最多有 n−mn−12n-m\\lt \\frac{n-1}{2}n−m2n−1​ 个 cn\\lt c_ncn​ 的数，于是最多有 n−mn-mn−m 个 iii 使得 aicn,bicna_i\\lt c_n,b_i\\lt c_nai​cn​,bi​cn​（即交换后没有效果），所以至少 2m−n2m-n2m−n 个 iii 满足 aicn,bicn+1a_i\\lt c_n,b_i\\gt c_{n+1}ai​cn​,bi​cn+1​，所以交换他们一定可以组成答案。 时间瓶颈在排序，时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn) AC Code #include bits/stdc++.hvoid run() int n; std::cin n; std::vectorint a(n), b(n), c; for (int i = 0; i n; i++) std::cin a[i], c.push_back(a[i]); for (int i = 0; i n; i++) std::cin b[i], c.push_back(b[i]); std::sort(c.begin(), c.end()); int x = c[n - 1], y = c[n]; std::vectorint pos; for (int i = 0; i n; i++) if (a[i] == x || a[i] == y || b[i] == x || b[i] == y) pos.push_back(i); if (pos.size() == 2) int t = 0; for (auto i : pos) if (a[i] == x || a[i] == y) t++; if (t == 2 || t == 0) std::swap(a[pos[0]], b[pos[0]]); else pos.push_back(pos[0]); int t = (a[pos[0]] x) + (a[pos[1]] x); for (int i = 0; i n; i++) if (i == pos[0] || i == pos[1]) continue; if (a[i] x) if (t * 2 + 1 n) t++; else if (b[i] y) std::swap(a[i], b[i]); else t++; if (t * 2 + 1 n) for (int i = 0; i n t * 2 + 1 n; i++) if (i == pos[0] || i == pos[1]) continue; if (a[i] y b[i] x) std::swap(a[i], b[i]), t++; if (t * 2 + 1 n) for (int i = 0; i n t * 2 + 1 n; i++) if (i == pos[0] || i == pos[1]) continue; if (a[i] x b[i] y) std::swap(a[i], b[i]), t--; for (int i = 0; i n; i++) std::cout a[i] [i == n - 1]; for (int i = 0; i n; i++) std::cout b[i] [i == n - 1];int main() #ifdef ONLINE_JUDGE std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0);#endif int t; std::cin t; while (t--) run(); //, std::fprintf(stderr, ---------------- );"},{"title":"2025 清华大学算法竞赛训练营 Day 5","path":"/wiki/algo_contests/thucamp2025-day5.html","content":"今天这场的出题人也比较仁慈，塞了几道 Div2A 难度的题（ A - An Experiment in Optics Lab 【题意】 从原点开始，xxx 正半轴上依次摆放 n≤105n\\le 10^5n≤105 面棱镜，棱镜 iii 的宽度为 li∈[1,1000]l_i\\in [1,1000]li​∈[1,1000]、折射系数为 ri∈[104,1.6×104]r_i\\in [10^4, 1.6\\times 10^4]ri​∈[104,1.6×104]. 考察分界处，光的折射与左右两面棱镜 i−1,ii-1, ii−1,i 满足： ri−1sin⁡θi−1=risin⁡θir_{i-1}\\sin \\theta_{i-1}=r_i\\sin\\theta_iri−1​sinθi−1​=ri​sinθi​其中 θi−1\\theta_{i-1}θi−1​ 是入射角，θi\\theta_iθi​ 是折射角。 然后，你需要维护 q≤105q\\le 10^5q≤105 次操作，每次操作会是两种中的一个： 1 id l r: 用一块新棱镜替换原来的第 ididid 块棱镜，其宽度为 lll，折射系数为 rrr 2 p angle: 查询操作。假设有一束光从 (p,0)(p,0)(p,0) 的位置、与 xxx 正半轴成 angle3600\\frac{angle}{3600}3600angle​ 度夹角（在 xxx 轴上方）发射出去，当这束光从最后一面棱镜出来的时候，折射点的 yyy 坐标是多少。 查询的精度在 10−610^{-6}10−6 以内，时间 4s4s4s. 我们先梳理一下我们可以推出来的信息。考虑在一块棱镜 iii 内部，一束光从其左侧以 θi\\theta_iθi​ 的角度到达右侧，则这束光的折射点升高了 Δy=litan⁡θi\\Delta y=l_i\\tan\\theta_iΔy=li​tanθi​，也就是说，我们最后的答案是 ∑ilitan⁡θi \\sum_i l_i\\tan\\theta_i i∑​li​tanθi​注意到第 iii 次折射的折射角等于第 i+1i+1i+1 次折射的入射角，所以，我们可以在 sin⁡θi\\sin\\theta_isinθi​ 之间转移。现在的问题就是如何从 sin⁡θi\\sin\\theta_isinθi​ 的关系推理出 tan⁡θi\\tan\\theta_itanθi​ 之间的关系 sin⁡θi\\sin\\theta_isinθi​ 之间可以以 O(1)O(1)O(1) 的方式转移，因为根据折射公式，sin⁡θi+1=riri+1sin⁡θi\\sin\\theta_{i+1}=\\frac{r_i}{r_{i+1}}\\sin\\theta_isinθi+1​=ri+1​ri​​sinθi​，但是我们从中却推不出 tan⁡θi+1=f(tan⁡θi)\\tan\\theta_{i+1}=f(\\tan\\theta_i)tanθi+1​=f(tanθi​)，这个 f()f()f() 怎么样都得带个开根号，我们就很难用数据结构快速维护了，线段树只能快速维护一堆东西求和。 所以，一个解决办法是，把 tan⁡θi\\tan\\theta_itanθi​ 对 sin⁡θi\\sin\\theta_isinθi​ 进行泰勒展开！这样 tan⁡θi=∑jajsin⁡jθi\\tan\\theta_i=\\sum_j a_j\\sin^j\\theta_itanθi​=∑j​aj​sinjθi​，而 aja_jaj​ 是固定的，这意味着我们就可以用多棵线段树分别维护 sin⁡jθi\\sin^j\\theta_isinjθi​ 这就是算法的 backbone: 我们考虑这样一个向量乘法，我们假设先用泰勒展开的前三项对 tan⁡(x)=f(sin⁡(x))\\tan(x)=f(\\sin(x))tan(x)=f(sin(x)) 近似 [yiaisin⁡θia2sin⁡2θia3sin⁡3θi]T[1000liriri+100li0(riri+1)20li00(riri+1)3]=[yi+li(aisin⁡θi+a2sin⁡2θi+a3sin⁡3θi)a1riri+1sin⁡θia2(riri+1)2sin⁡2θia3(riri+1)3sin⁡3θi]T=[yi+1aisin⁡θi+1a2sin⁡2θi+1a3sin⁡3θi+1]T \\begin{bmatrix} y_i\\\\a_i\\sin\\theta_i\\\\a_2\\sin^2\\theta_i\\\\a_3\\sin^3\\theta_i \\end{bmatrix}^T \\begin{bmatrix} 1000\\\\ l_i\\frac{r_i}{r_{i+1}}00\\\\ l_i0(\\frac{r_i}{r_{i+1}})^20\\\\ l_i00(\\frac{r_i}{r_{i+1}})^3\\\\ \\end{bmatrix}\\\\ =\\begin{bmatrix} y_i+l_i(a_i\\sin\\theta_i+a_2\\sin^2\\theta_i+a_3\\sin^3\\theta_i)\\\\a_1\\frac{r_i}{r_{i+1}}\\sin\\theta_i\\\\a_2(\\frac{r_i}{r_{i+1}})^2\\sin^2\\theta_i\\\\a_3(\\frac{r_i}{r_{i+1}})^3\\sin^3\\theta_i \\end{bmatrix}^T\\\\ =\\begin{bmatrix} y_{i+1}\\\\a_i\\sin\\theta_{i+1}\\\\a_2\\sin^2\\theta_{i+1}\\\\a_3\\sin^3\\theta_{i+1} \\end{bmatrix}^T ​yi​ai​sinθi​a2​sin2θi​a3​sin3θi​​​T​1li​li​li​​0ri+1​ri​​00​00(ri+1​ri​​)20​000(ri+1​ri​​)3​​=​yi​+li​(ai​sinθi​+a2​sin2θi​+a3​sin3θi​)a1​ri+1​ri​​sinθi​a2​(ri+1​ri​​)2sin2θi​a3​(ri+1​ri​​)3sin3θi​​​T=​yi+1​ai​sinθi+1​a2​sin2θi+1​a3​sin3θi+1​​​T观察结果向量，我们从中发现了可以递推的关系，并且乘的矩阵 MiM_iMi​ 本身只和第 i,i+1i,i+1i,i+1 个棱镜有关，而与入射角无关，经过多面棱镜，就相当于接着乘 Mi+1,Mi+2,…M_{i+1},M_{i+2},\\dotsMi+1​,Mi+2​,…，所以我们可以线段树维护 MiM_iMi​，修改棱镜就相当于在线段树上单点修改两次。 时间复杂度 O(kn+kqlog⁡n)O(kn+kq\\log n)O(kn+kqlogn). 其中 kkk 是泰勒展开的项数，越高越精确，也越慢。 AC Code 需要注意几点： 不要真的写矩阵乘法，这样的话矩阵乘就是 O(K3)O(K^3)O(K3) 的。实际上除了第一列和主对角线，MiM_iMi​ 剩下的位置都是 000，而且这样两个矩阵乘起来 MiMi+1M_iM_{i+1}Mi​Mi+1​ 也依然满足除了第一列和主对角线剩下元素都是零. 所以我们可以 O(K)O(K)O(K) 记录、计算矩阵乘法。详见 combine(Matrix, Matrix) 函数。 处理查询的时候，首先要判断出初始光线在那个棱镜内部，然后先让这条光线在棱镜内部运动，直到碰到右边界。后面的交给矩阵乘法处理。这里我用二分+树状数组找光源在哪个棱镜内部，所以我的实现其实是 O(kn+qlog⁡2n+kqlog⁡n)O(kn+q\\log^2 n+kq\\log n)O(kn+qlog2n+kqlogn) 的，O(kn)O(kn)O(kn) 是线段树的初始化，O(qlog⁡2n)O(q\\log^2 n)O(qlog2n) 是二分+树状数组找棱镜，O(kqlog⁡n)O(kq\\log n)O(kqlogn) 则是从线段树里提取出矩阵并 combine()。 其次注意精度问题，一是用 long double，二是我这里用了泰勒展开前 K=40K=40K=40 项近似 f(x)=x1−x2f(x)=\\frac{x}{\\sqrt{1-x^2}}f(x)=1−x2​x​. 这个式子的泰勒展开比较有规律，只有 sin⁡jx\\sin^j xsinjx 指数为奇数的项，而且通项为 f(x)=x+∑n=1∞1⋅3⋅5⋅⋯⋅(2n−1)2nn!x2n−1 f(x)=x+\\sum_{n=1}^{\\infin}\\frac{1\\cdot 3\\cdot 5\\cdot \\dots\\cdot (2n-1)}{2^n n!}x^{2n-1} f(x)=x+n=1∑∞​2nn!1⋅3⋅5⋅⋯⋅(2n−1)​x2n−1最后，为了提速，Row, Matrix 最好都用 std::arrayld, K 而不是 std::vectorld. #pragma GCC optimize(3)#pragma GCC optimize(Ofast,unroll-loops)#include bits/stdc++.husing ld = long double;constexpr int N = 1e5 + 10, K = 40;constexpr ld pi = 3.14159265358979323846;std::vectorld C;int n, q;struct Row ld y; std::arrayld, K sines;;struct Mirror ld l, r1; m[N];struct Matrix std::arrayld, K col, diag; void set(int i) auto x = m[i]; for (int i = 0; i K; i++) col[i] = x.l; ld v = m[i].r / m[i + 1].r, t = v; for (int i = 0; i K; i++, t = t * v * v) diag[i] = t; friend Matrix combine(const Matrix lhs, const Matrix rhs) Matrix res; int k = lhs.col.size(); for (int i = 0; i lhs.col.size(); i++) res.col[i] = lhs.col[i] + lhs.diag[i] * rhs.col[i]; res.diag[i] = lhs.diag[i] * rhs.diag[i]; return res; friend Row combine(const Row lhs, const Matrix rhs) Row res; res.y = lhs.y; for (int i = 0; i K; i++) res.y += lhs.sines[i] * rhs.col[i]; for (int i = 0; i K; i++) res.sines[i] = lhs.sines[i] * rhs.diag[i]; return res; ;Matrix T[N * 4];void build(int p = 1, int l = 1, int r = n) if (l == r) T[p].set(l); return; int mid = l + ((r - l) 1); build(p * 2, l, mid), build(p * 2 + 1, mid + 1, r); T[p] = combine(T[p * 2], T[p * 2 + 1]);void modify(int q, int p = 1, int l = 1, int r = n) if (l == r) T[p].set(l); return; int mid = l + ((r - l) 1); if (q = mid) modify(q, p * 2, l, mid); else modify(q, p * 2 + 1, mid + 1, r); T[p] = combine(T[p * 2], T[p * 2 + 1]);Matrix range(int ql, int qr, int p = 1, int l = 1, int r = n) if (ql = l r = qr) return T[p]; int mid = l + ((r - l) 1); if (qr = mid) return range(ql, qr, p * 2, l, mid); else if (ql mid) return range(ql, qr, p * 2 + 1, mid + 1, r); else auto L = range(ql, qr, p * 2, l, mid); auto R = range(ql, qr, p * 2 + 1, mid + 1, r); return combine(L, R); std::arrayint, N b;int sum(int p) int res = 0; for (; p 0; p -= p -p) res += b[p]; return res;int ask(int l, int r) return l r ? 0 : sum(r) - sum(l - 1); int at(int p) return ask(p, p); void add(int p, int v) for (; p = n; p += p -p) b[p] += v;void set(int p, int v) add(p, -at(p)), add(p, v); int start_from(int p) int l = 0, r = n + 1; while (l + 1 r) int mid = l + ((r - l) 1); if (sum(mid) p) r = mid; else l = mid; return r;int main() #ifdef ONLINE_JUDGE std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0);#endif //! initialize C C.push_back(1), C.push_back(0.5); for (int i = 2; i K; i++) C.push_back(C.back() * (2 * i - 1) / 2 / i); std::cin n; for (int i = 1, L; i = n; i++) std::cin L m[i].r; add(i, L); m[i].l = L; build(1, 1, n); std::cin q; while (q--) int op, id, p, deg; std::cin op; if (op == 2) std::cin p deg; int r = start_from(p); ld theta = pi * deg / 3600 / 180; int dist = sum(r) - p; ld ans = (ld)dist * std::tan(theta); if (r + 1 = n) ld ntheta = m[r].r * std::sin(theta) / m[r + 1].r; Row initial; initial.y = ans; ld v = ntheta, t = v; for (int i = 0; i K; i++, t = t * v * v) initial.sines[i] = t * C[i]; auto mat = range(r + 1, n); initial = combine(initial, mat); ans = initial.y; std::cout std::setprecision(20) std::fixed ans ; else std::cin id; std::cin m[id].l m[id].r; modify(id); if (id 1) modify(id - 1); set(id, int(m[id].l));"},{"title":"CCPC 2024 广州","path":"/wiki/algo_contests/ucup038-guangzhou.html","content":"B. Add One 3 有两个 sub task: 如何确定 [l,r][l, r][l,r] 能否成为唯一的 MSS 最少需要几步 判断问题 先来回答第一个问题。因为我们只能对某个数 +1+1+1，所以对于一段区间来说，其和只能增大不能减少。考虑和 [l,r][l,r][l,r] 邻接的前缀和后缀（即 [i,l−1][i,l-1][i,l−1] 和 [r+1,j][r+1,j][r+1,j] 这两类区间），如果这两类区间的和 ≥0\\ge 0≥0，那么我们完全可以把 [l,r][l,r][l,r] 接上这个 subrange，这样 ∑[i,r]≥∑[l,r]\\sum [i,r]\\ge\\sum [l,r]∑[i,r]≥∑[l,r] 或者 ∑[l,j]≥∑[l,r]\\sum [l,j]\\ge \\sum [l,r]∑[l,j]≥∑[l,r]，这就意味着 [l,r][l,r][l,r] 永远不可能单独成为 MSS. 方案数 接下来处理最少需要几步。因为整个段需要是唯一的 MSS： 如果 [l,r][l,r][l,r] 的某个前缀和小于等于零，即 ∃k,∑i=lkai≤0\\exists k,\\sum_{i=l}^k a_i\\le 0∃k,∑i=lk​ai​≤0，那么我们完全可以扔掉这个前缀，剩下的部分 [k+1,r][k+1,r][k+1,r] 可以组成 sum 更大的 MSS。这就意味着所有前缀都必须 0\\gt 00 类似的，[l,r][l,r][l,r] 的所有后缀也都必须是 0\\gt 00. 同时，考虑不在 [l,r][l,r][l,r] 区间的 MSS，设最大的 MSS 为 MMM。如果存在这样的 MSS（不在 [l,r][l,r][l,r] 里而其和仍为 MMM），那么 [l,r][l,r][l,r] 至少需要达到 M+1M+1M+1. 所以，我们找两个下标 p1p2p_1\\lt p_2p1​p2​ 使得 [l,p1][l,p_1][l,p1​] 这个前缀与 [p2,r][p_2,r][p2​,r] 这个后缀都是负数（也可以是只有负数前缀而没有后缀，或者只有后缀没有前缀） 一个疑惑点是：难道没有可能前缀 [l,p1][l,p_1][l,p1​] 和后缀 [p2,r][p_2,r][p2​,r] 有重叠吗？这种情况下，显然只可能是 [p1,p2][p_1,p_2][p1​,p2​] 为负数，且绝对值比较大，而 [l,p1][l,p_1][l,p1​] 可正可负。这时，我们可以把这类情况归类到无负前缀或者前后缀均负的分类里。 所以，我们找到负数绝对值最大的前缀与后缀，先将其 +1+1+1 填成 000 后，还需要再 +1+1+1 填成 111，也就是说要 max⁡p1,p2∈[l,r]−∑i=lp1ai−∑i=p2rai+C \\max_{p_1,p_2\\in [l,r]} -\\sum_{i=l}^{p_1} a_i -\\sum_{i=p_2}^{r} a_i+C p1​,p2​∈[l,r]max​−i=l∑p1​​ai​−i=p2​∑r​ai​+C 为什么只需要找绝对值最大的前缀 p1p_1p1​ 呢？因为就算还有 jp1j\\lt p_1jp1​ 它的前缀是负数 x=∑[l,j]∑[l,p1]x=\\sum [l,j]\\lt \\sum[l,p_1]x=∑[l,j]∑[l,p1​]，我们可以先在 ∑[l,j]\\sum [l,j]∑[l,j] 里加上 xxx，然后再把剩下的 +1+1+1 加到 [l,p1][l,p_1][l,p1​] 上。 这里的 +C+C+C 是这样算的：如果有非负的前缀与非负后缀，则 +2+2+2；若只有其一，则只 +1+1+1。 然后一个变形是，把这里代表前后缀的 ∑\\sum∑ 换成代表 [l,r][l,r][l,r] 内最大子段和的形式： max⁡p1,p2(−∑i=lrai+∑i=p1+1p2−1ai+C) \\max_{p_1,p_2}\\Big( -\\sum_{i=l}^r a_i + \\sum_{i=p_1+1}^{p_2-1} a_i + C\\Big) p1​,p2​max​(−i=l∑r​ai​+i=p1​+1∑p2​−1​ai​+C)对于 MSS 不在 [l,r][l,r][l,r] 的情况来说，我们要把 [l,r][l,r][l,r] 补到 M+1M+1M+1，所以次数为 M+1−∑i=lrai\\boxed{M+1-\\sum_{i=l}^r a_i}M+1−i=l∑r​ai​​ 所以，总结一下就是： 当没有非负前后缀时，就只用考虑 MSS 在 [l,r][l,r][l,r] 外的情况 M+1−∑i=lraiM+1-\\sum_{i=l}^r a_iM+1−∑i=lr​ai​ 当只有非负前缀的时候，就需要考虑 MSS 在 [l,r][l,r][l,r] 外 M+1−∑i=lraiM+1-\\sum_{i=l}^r a_iM+1−∑i=lr​ai​ 把自己的非负后缀都填到 0\\gt 00，次数为 −∑[p2,r]+1-\\sum [p_2,r]+1−∑[p2​,r]+1 当只有非负前缀时是类似的 当有非负前后缀时，四种情况都要考虑。 可以使用线段树维护区间的 MSS、最负后缀、最负前缀，时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn). Code 我的代码里实现方式是不分类讨论，直接对四种情况取 max⁡\\maxmax. 可以这样做的原因是因为，四种式子分别对应满足各自情况时需要的最少次数，那么要满足所有情况，就得取其中最大的次数。 例如，如果 [l,r][l,r][l,r] 之间没有非负前后缀，这就说明对于“无非负前缀”、“无非负后缀”这两个判定条件我只需要花 000 步，但是又有 MSS 不在 [l,r][l,r][l,r] 里，所以需要把 x0x\\gt 0x0 步把 [l,r][l,r][l,r] 补到 M+1M+1M+1，所以最终需要 xxx 步。 #include algorithm#include cassert#include iostream#include vectorusing ll = long long;using vi = std::vectorint;constexpr ll inf = 1e18;struct Node ll sum0; ll max_psum-inf; ll max_ssum-inf; ll min_psuminf; ll min_ssuminf; ll max_inseg_sum-inf; int l, r, segl, segr, suf, prf; void set(ll x, int pos) sum = x; max_psum = x; max_ssum = x; max_inseg_sum = x; min_ssum = x; min_psum = x; ;void run() int n, q; std::cin n q; vi a(n + 1); for (int i = 1; i = n; i++) std::cin a[i]; std::vectorNode T(6 * n); auto up = [](Node l, Node r) Node t; t.sum = l.sum + r.sum; t.max_psum = std::max(l.max_psum, l.sum + r.max_psum); t.max_ssum = std::max(r.max_ssum, r.sum + l.max_ssum); t.max_inseg_sum = std::max(l.max_inseg_sum, r.max_inseg_sum, l.max_ssum + r.max_psum); t.min_psum = std::min(l.min_psum, l.sum + r.min_psum); t.min_ssum = std::min(r.min_ssum, r.sum + l.min_ssum); return t; ; auto build = [](auto F, int p, int l, int r) - void if (l == r) T[p].set(a[l], l); return; int mid = l + ((r - l) 1); F(F, 2 * p, l, mid), F(F, 2 * p + 1, mid + 1, r); T[p] = up(T[p * 2], T[p * 2 + 1]); ; build(build, 1, 1, n); auto query = [](auto F, int p, int l, int r, int ql, int qr, bool debug = false) - Node if (qr ql) return Node(); // empty range if (ql = l r = qr) return T[p]; int mid = l + ((r - l) 1); Node res; if (qr = mid) res = F(F, 2 * p, l, mid, ql, qr, debug); else if (ql mid) res = F(F, 2 * p + 1, mid + 1, r, ql, qr, debug); else Node left = F(F, 2 * p, l, mid, ql, mid, debug); Node right = F(F, 2 * p + 1, mid + 1, r, mid + 1, qr, debug); res = up(left, right); return res; ; auto q1 = [](int L) if (L == 1) return true; else Node t = query(query, 1, 1, n, 1, L - 1); return t.max_ssum 0 /* max suffix sum of [1, l-1] 0 */; ; auto q2 = [](int R) if (R == n) return true; else Node t = query(query, 1, 1, n, R + 1, n); return t.max_psum 0 /* max prefix sum of [r+1, n] 0 */; ; Node maxx = query(query, 1, 1, n, 1, n); // std::cerr global max subarray sum= maxx.max_inseg_sum ; while (q--) int ql, qr; std::cin ql qr; bool b1 = q1(ql), b2 = q2(qr); if (!b1 || !b2) std::cout -1 ; continue; Node t = query(query, 1, 1, n, ql, qr); auto mid = query(query, 1, 1, n, ql + 1, qr - 1); Node lf = query(query, 1, 1, n, 1, ql - 1); Node rf = query(query, 1, 1, n, qr + 1, n); ll outside = std::max(lf.max_inseg_sum, rf.max_inseg_sum, 0ll); if (ql == qr) std::cout std::max(0ll, outside - a[ql] + 1) ; continue; ll ans = std::max(0ll, mid.max_inseg_sum) - t.sum + 2; // mss in (l, r) ans = std::max(ans, 1 - t.min_psum, 1 - t.min_ssum, outside - t.sum + 1); // no prefix = 0, no suffix = 0, mss outside [l,r] std::cout std::max(0ll, ans) ; int main() int T; std::cin T; while (T--) run(); C. Iridescent Universe 考虑图，如果一个点出发的所有边都有相同的 target color，那么我们可以先随便染色其他边，最后再染色这个点，那么我们就不需要理睬这个点的 indicent edge 之前是什么颜色的。 用第一个样例来解释，我们不管之前到底是怎么染色的，只要最后一次染色是 222 号点染色 222 号颜色，那么最后的图里，222 出发的所有边一定都是 222 号颜色。 所以我们可以拓扑排序：每次染色点，其 incident edges 都有相同的 target color，然后删边。最后如果还有边留下来，那么我们检查他们的 target color 和 initial color 匹配即可。 Code 我这里使用 mapint, int 记录每个点的边的颜色信息。时间复杂度是 O(nlog⁡n)O(n\\log n)O(nlogn). #include algorithm#include iostream#include map#include queue#include utility#include vectorusing vi = std::vectorint;constexpr int N = 2e5 + 5;struct edge int uv, c, tc; int colored; E[N];vi G[N];std::mapint, int C[N];int vis[N];int main() int n, m, k; std::cin n m k; for (int i = 0, u, v, c, t; i m; i++) std::cin u v c t; E[i] = u ^ v, c, t, false; G[u].push_back(i), G[v].push_back(i); C[u][t]++, C[v][t]++; std::queueint q; std::vectorstd::pairint, int ans; for (int i = 1; i = n; i++) if (C[i].size() == 1) q.push(i), vis[i] = true; int cnt = 0; while (!q.empty()) int u = q.front(); ans.push_back(u, C[u].begin()-first); for (auto eid : G[u]) E[eid].colored = true; cnt++; q.pop(); for (int eid : G[u]) int v = E[eid].uv ^ u; if (vis[v]) continue; C[v][E[eid].tc]--; if (C[v][E[eid].tc] == 0) C[v].erase(E[eid].tc); if (C[v].size() == 1) q.push(v); vis[v] = true; if (cnt != n) for (int eid = 0; eid m; eid++) if (E[eid].colored) continue; if (E[eid].c != E[eid].tc) std::cout -1 ; return 0; std::reverse(ans.begin(), ans.end()); std::cout ans.size() ; for (auto [u, c] : ans) std::cout u c ; E. Omniscient Artist 矩形覆盖数点会联想到扫描线：按 xxx 递增扫描矩形，同时用数据结构维护 yyy 轴找到符合条件的点。所以，我们把矩形的左右两边加入扫描线，左侧边 +1+1+1 表示从这个 xxx 坐标开始 [y1,y2][y_1,y_2][y1​,y2​] 之间的点会多出现 111；右侧边 −1-1−1. 那么我们“维护 yyy 轴”，想要维护的是什么呢？其实是 map[x] = y，意义为有 yyy 个点出现 xxx 次。这样，我们就可以遍历 cmcmcm，快速加和点数。 我们怎么维护这个东西呢？线段树并不好做，这个东西维护起来很怪，所以我们使用分块. 块上维护区间加的 tag（注意查询 cmcmcm 的时候也要考虑这个 tag），和一个 unordered_map（其实建议用数组而不是 unordered_map） 但是如果直接查询 cmcmcm 的话，时间复杂度是错的，因为枚举 cmcmcm 需要 O(n/m)O(n/m)O(n/m)，还需要枚举第几块 O(n)O(\\sqrt n)O(n​)，总共 nnn 轮，时间复杂度是 O(n2nm)O(\\frac{n^2\\sqrt n}{m})O(mn2n​​) 可能变成 O(n2)O(n^2)O(n2). min⁡,max⁡\\min,\\maxmin,max 优化上下界 我们给每一个块额外维护 min⁡,max⁡\\min,\\maxmin,max 分别表示最少出现几次、最多出现几次，然后查询时枚举块，ccc 的范围由 min⁡,max⁡\\min,\\maxmin,max 决定。这样，单轮查询的时间复杂度就从 O(nnm)O(\\frac{n\\sqrt n}{m})O(mnn​​) 降为 O(nm)O(\\frac{n}{m})O(mn​) 为什么这样子做会降低复杂度呢？我们考察每一次区间操作对于区间 max⁡i−min⁡i\\max_i-\\min_imaxi​−mini​ 的影响。每一次区间操作会横跨 xxx 个完整的区间，以及最多 222 个散块。 如果对于整个块都 +1/−1+1/-1+1/−1 的话，其 max⁡−min⁡\\max-\\minmax−min 不变，因为两者会同步变化。对于散块而言，这个散块所属的整块，其 max⁡−min⁡\\max-\\minmax−min 最多 +1/−1+1/-1+1/−1。 因此，每一次区间操作使得 ∑imax⁡i−min⁡i\\sum_{i} \\max_i-\\min_i∑i​maxi​−mini​ 最多增加 222，故 nnn 次区间操作使得这个和式 ≤2n\\le 2n≤2n，即 O(n)O(n)O(n). Code 一个实现上的细节：我第一版代码是对 xxx 坐标 sort 之后，进行扫描线，先更新点集后，选择用 x - prevX 计算宽（和计算矩形的面积那样） 但是这个问题就在于，如果是先加入线段再统计点数，那么刚加进来的线段覆盖的点 (xi,yi)(x_i,y_i)(xi​,yi​) 会有 p+1p+1p+1 次 occurrance，而在其左侧 yyy 坐标相同的点却只出现了 ppp 次。 错误代码 prvX = scan[0].x-1;for (int i = 0, j; i scan.size();) // 统计答案 int x = scan[i].x; int len = x - prvX; j = i; while (j scan.size() scan[j].x == scan[i].x) add(scan[j].lb, scan[j].ub, scan[j].exit); j++; for (int b = 1; b = blocks; b++) for (int c = std::max(1, (min[b] + m - 1) / m); c * m = max[b]; c++) if (c * m block_add[b]) continue; i64 cnt = cntpoint[b][c * m - block_add[b]]; ans[c] += cnt * len; prvX = x; i = j; 所以，一个代码难度更小的做法是，直接让 xxx 从 111 到 nnn 递增，且只处理横座标为 xxx 的线段，在分块上维护。维护完后统计点数。这样，查询的时间复杂度是 O(n2m)O(\\frac{n^2}{m})O(mn2​)，也不会 TLE. #include algorithm#include cassert#include cmath#include iostream#include vectorconstexpr int N = 3e5 + 5;constexpr int B = 600;using i64 = long long;struct Rec int l, r, t, b; r[N];struct Seg int x, ub, lb, exit, id;;int n, m;std::vectorSeg scan[N];int blocksize, blocks;int bid[N], bl[B], br[B];int cnty[N], prvX-1, block_add[B], max[B], min[B];int cntpoint[B][N];i64 ans[B];void rebuild(int l, int r, int w) assert(bid[l] == bid[r]); int id = bid[l]; for (int i = bl[id]; i = br[id]; i++) cntpoint[id][cnty[i]]--; max[id] = 0, min[id] = N; for (int i = bl[id]; i = br[id]; i++) cnty[i] += block_add[id]; if (l = i i = r) cnty[i] += w; cntpoint[id][cnty[i]]++; max[id] = std::max(max[id], cnty[i]); min[id] = std::min(min[id], cnty[i]); block_add[id] = 0;void add(int l, int r, int w) if (bid[l] == bid[r]) rebuild(l, r, w); else rebuild(l, br[bid[l]], w); for (int i = bid[l] + 1; i = bid[r] - 1; i++) block_add[i] += w, max[i] += w, min[i] += w; rebuild(bl[bid[r]], r, w); void debug() for (int b = 1; b = blocks; b++) std::cerr Block b : ; for (int i = bl[b]; i = br[b]; i++) std::cerr \\tPos i = cnty[i] + block_add[b] ; for (int cnt = 0; cnt = max[b]; cnt++) if (cntpoint[b][cnt] == 0) continue; std::cerr \\tCount cnt + block_add[b] : cntpoint[b][cnt] points ; int main() std::cin.tie(0)-sync_with_stdio(0); std::cin n m; for (int i = 1; i = n; i++) std::cin r[i].l r[i].r r[i].b r[i].t; r[i].t--; scan[r[i].l].push_back(r[i].l, r[i].t, r[i].b, 1, i); // scan.push_back(r[i].r - 1, r[i].t, r[i].b, 0, i); scan[r[i].r].push_back(r[i].r, r[i].t, r[i].b, -1, i); // blocks blocksize = (int)std::sqrt(n); for (int i = 1; i = n; i++) bid[i] = (i - 1) / blocksize + 1; for (int i = 1; i = bid[n]; i++) bl[i] = (i - 1) * blocksize + 1; br[i] = i * blocksize; br[bid[n]] = n, blocks = bid[n]; for (int b = 1; b = blocks; b++) cntpoint[b][0] = br[b] - bl[b] + 1; max[b] = 0, min[b] = 0; for (int i = 1; i = n; i++) for (auto [x, ub, lb, v, id] : scan[i]) add(lb, ub, v); for (int b = 1; b = blocks; b++) for (int c = std::max(1, (min[b] + m - 1) / m); c * m = std::min(n, max[b]); c++) if (c * m block_add[b]) continue; ans[c] += (i64)cntpoint[b][c * m - block_add[b]]; for (int c = 1; c * m = n; c++) std::cout ans[c] ; F. Witnessing the Miracle 这个题本质上是两个序列之间进行匹配，即考虑 sis_isi​ 最终状态下能否和 tjt_jtj​ 进行匹配。 不过，由于题目有“移除 kkk 个磁铁”的设定，实际上，对于某一个 sis_isi​，假设其左边激活 ddd 个磁铁，那么其在 ttt 序列里对应的位置是可以唯一确定的：j=i+d−(k−d)j=i+d-(k-d)j=i+d−(k−d). 我们令 dp 状态为 dp[i,d]dp[i,d]dp[i,d] 表示在 s[1,i−1]s[1,i-1]s[1,i−1] 中间激活 ddd 个磁铁的方案数量. 那么我们就考虑怎么从 dp[i−1]dp[i-1]dp[i−1] 转移到 dp[i]dp[i]dp[i] 上. 考虑 sis_isi​ 和 tjt_jtj​ 各自可以填什么数字： 如果 si=0,tj=0s_i=0,t_j=0si​=0,tj​=0，可行。 都没有磁铁的位置显然可以互相匹配，所以 dp[i, d] += dp[i-1, d] 如果 si=0,tj=1s_i=0, t_j=1si​=0,tj​=1，不可行 原来没有磁铁不可能凭空出现磁铁（就算有，那也是从 sss 的其他地方跑过来的），所以不更新 dp 如果 si=1,tj=1s_i=1,t_j=1si​=1,tj​=1，可行 有磁铁的位置匹配有磁铁的位置，说明这块磁铁没有被激活，dp[i, d] += dp[i-1, d] 如果 si=1,tj=0s_i=1,t_j=0si​=1,tj​=0，可行 有磁铁的位置最终匹配到没有磁铁的位置，说明这块磁铁被激活了，dp[i, d+1] += dp[i-1, d] 这个 dp 乍一看比较难懂。实际上真的比较难懂。我建议还是按 dp[i,j]dp[i,j]dp[i,j] 的思路来看 最后的答案即为 dp[n, k]. G. Addition, Multiplication with a Sugar 首先，肯定先把开头的一连串 111 和结尾的一连串 111 拿走，他们一定是加起来的。 然后考虑中间的一段，先看成 x1:≠11×ax2:≠11×b… \\boxed{x_1: e 1}\\boxed{1\\times a}\\boxed{x_2: e 1}\\boxed{1\\times b}\\dots x1​:=1​1×a​x2​:=1​1×b​…如果切开更优，则有 ∑xi+n∏xin+1∏(xi−1) \\begin{aligned} \\sum x_i+n\\gt \\prod x_i\\\\ n+1\\gt \\prod (x_i-1) \\end{aligned} ∑xi​+nn+1​∏xi​∏(xi​−1)​ M. Under the Epilogue 我们先来考察题目的性质：因为对于某个点集 SSS，我们希望能找到一个点 uuu，使得其可以经过 SSS 内的每个点，最后回到 uuu。我们可以证明，这样的 SSS 一定是一个强连通分量。 这是因为，考虑 SSS 内的任意两个点 a,ba,ba,b，因为 u→uu\\to uu→u 的路径上必定会有下面两种情况中的一个： u→a→b→uu\\to a\\to b\\to uu→a→b→u，这是就有 a→ba\\to ba→b 的路径了 u→b→a→uu\\to b\\to a\\to uu→b→a→u，此时，我们可以让 a→u→ba\\to u\\to ba→u→b 所以，SSS 一定是强连通分量，而且是导出子图。因此，我们的任务就是：给定边集，求出强连通导出子图的数量。考虑到 n≤50n\\le 50n≤50，开始往 dp 的方向上想。如果用 dp[...] 表示最终答案 SSS 的数量，该怎么设计状态呢？ 到目前为止，我们只利用了 SSS 的性质（从题目定义出发推出的），还没有利用 [li,ri][l_i,r_i][li​,ri​] 是连续区间这个性质. 先从简单例子入手思考一下： 首先，只有一个点显然是 SCC. 接着考虑两个点的情况，一定是 a→b,b→aa\\to b, b\\to aa→b,b→a 都有边才行. 接着考虑三个点的情况，我们假定 abca\\lt b\\lt cabc，首先，绝对不可能出现导出子图里三个点出度均为 111 的情况，即 a→b→c→aa\\to b\\to c\\to aa→b→c→a. 这是因为如果 ccc 可以到 aaa，由于 [lc,rc][l_c,r_c][lc​,rc​] 是连续的，所以 c→bc\\to bc→b 这条边一定存在，因此，任意的三个点里，一定存在两个点，他们组成的点集 S2S_2S2​ 也是满足条件的 SSS. 所以，所有的 S:∣S∣=3S:|S|=3S:∣S∣=3 可以看成是 S:∣S∣=2S:|S|=2S:∣S∣=2 的 S2[i]={ a,b },abS_2[i] = \\set{a,b},a\\lt bS2​[i]={a,b},ab 再加上一个点 ccc，而且这个点 c∉[a,b]c otin [a,b]c∈/[a,b]（证明和上一段类似，也是利用 [li,ri][l_i,r_i][li​,ri​] 的连续性）. 所以，ccc 和 { a,b }\\set{a,b}{a,b} 之间必有连边，即 max⁡(ra,rb)≥c\\max(r_a,r_b)\\ge cmax(ra​,rb​)≥c（a/ba/ba/b 向 ccc 连一条边）并且 lc≤bl_c\\le blc​≤b（ccc 向 a/ba/ba/b 连一条边，lcl_clc​ 至少要比 bbb 靠左）. 这个结论可以推广一下，考虑任意两个 vertex set S1,S2S_1, S_2S1​,S2​ 合并的判定条件： min⁡i∈S2ai≤max⁡i∈S1bi\\min_{i\\in S_2}a_i\\le \\max_{i\\in S_1}b_ii∈S2​min​ai​≤i∈S1​max​bi​ 也就是两个连通块之间可以互相一步到达。 但是如果单纯是这个条件的话，还是比较难统计. 我们这里作出的一个等价限制是：S=∪i[xi,yi]S=\\cup_i [x_i,y_i]S=∪i​[xi​,yi​] 其中 lyi≤yi−1l_{y_i}\\le y_{i-1}lyi​​≤yi−1​. 这是因为，如果 uuu 要能走回 uuu 且 SSS 里有比 uuu 小的元素，那么无论如何 uuu 都应该可以走到某个 vuv\\lt uvu 的位置上，就可以让 yi−1=v,yi=uy_{i-1}=v, y_i=uyi−1​=v,yi​=u."},{"title":"使用 Dify 的“节点”完成数据预处理","path":"/wiki/dify/custom-data-preprocess.html","content":"372f7a9688aae3bb4ec6e6afab19ca439bb95e6d91164a50dda5c3f9a4f813ff Password is needed."},{"title":"安装与部署 Dify","path":"/wiki/dify/install-n-use.html","content":"下载 Dify 的 docker compose 文件 git clone 启动 docker 确保 docker 可以共享文件夹 在 docker desktop - settings - resource share 里查看 folder 是否存在。 进入本地部署的 Dify 使用 ip a 查看 docker0 对应的 IP 地址（inet），然后用这个 IP 进入，而不是 localhost"},{"title":"SVD","path":"/wiki/linear-algebra/SVD.html","content":"SVD 分解"},{"title":"CAPM 资本资产定价模型","path":"/wiki/fina/CAPM.html","content":"公式 E[Rp]=Rf+βp(RM−Rf) \\mathbb E[R_{p}]=R_f+\\beta_{p}(R_M-R_f) E[Rp​]=Rf​+βp​(RM​−Rf​)市场风险溢价 例题"},{"title":"IRR","path":"/wiki/fina/IRR.html","content":"Definition The IRR of the project is the single annual discount rate at which the NPV of the project is equal to zero. 公式 NPV=C0+∑i=1m(11+IRR)iCi=0 NPV=C_0+\\sum_{i=1}^{m} \\Big(\\frac{1}{1+IRR}\\Big)^i C_i=0 NPV=C0​+i=1∑m​(1+IRR1​)iCi​=0 IRR Rule 定义 Minimum required return 计算 IRRIRRIRR，与 required return 进行比较 问题 还需要将 Interest Rate 纳入考量 解决办法 先算出两个 project 的 NPV 相等时的利率 RRR，然后根据现在的利率，判断选择那个 project. Amortization Schedule monthly: IRR12\\frac{IRR}{12}12IRR​"},{"title":"Bonds","path":"/wiki/fina/bonds.html","content":"Bonds 债券 不拥有 Ownership 不能投票 无论是否盈利，盈利多少，企业都必须支付本息；否则视为破产 有法律义务支付本息 企业先支付本息，再支付分红 由于支付利息视作企业运营的开支，因此可以抵扣税款 Terms Maturity Date Face Value (Par Value) FFF Coupon Payment CCC Variations Zero-Coupon Bonds: 无利息 Bond Valuation NPV Approach Yield to Maturity (YTM) YTM 使得债券 NPV 为 000 时的 discount rate 假设一年付 mmm 次，那么每次付的比率是 YTMm\\frac{YTM}{m}mYTM​；假设 nnn 年 −P0+C×1−(1+YTM/m)−nmYTM/m+F×(1+YTM/m)−nm=0 -P_0+C\\times\\frac{1-(1+YTM/m)^{-nm}}{YTM/m}+F\\times\\Big( 1+YTM/m \\Big)^{-nm}=0 −P0​+C×YTM/m1−(1+YTM/m)−nm​+F×(1+YTM/m)−nm=0 The investors are getting an annual rate of return equal to the return they require on the investment The YTM can be obtained ONLY IF we hold the bond until maturity Risk of Investment 债券投资的风险 利率风险：市场利率上升导致债券价格下跌。 信用风险：发行人违约可能性（如公司破产）。 流动性风险：债券难以快速变现。 通胀风险：通胀削弱固定收益的实际购买力。"},{"title":"Capital Investment Decision","path":"/wiki/fina/capital-investment-decision-01.html","content":"Incremental Cash Flow=Operating Cash Flow−Incremental Cash Flow on Capital Spending−Incremental Cash Flow on Change in Net Working Capital \\begin{aligned} \\text{Incremental Cash Flow}\\\\ \\quad =\\text{Operating Cash Flow}\\\\ \\quad -\\text{Incremental Cash Flow on Capital Spending}\\\\ \\quad -\\text{Incremental Cash Flow on Change in Net Working Capital} \\end{aligned} ​Incremental Cash Flow=Operating Cash Flow−Incremental Cash Flow on Capital Spending−Incremental Cash Flow on Change in Net Working Capital​ EBIT approach 先计算 EBIT Tax Shield approach OCF=(PQ−FC−VC)(1−Tc)+Dep⋅Tc OCF=(PQ-FC-VC)(1-T_c)+Dep\\cdot T_c OCF=(PQ−FC−VC)(1−Tc​)+Dep⋅Tc​ 例题 Opportunity Cost 相关 Parker Stone, Inc. is considering building a new manufacturing plant in South Park to produce garden tools. The company purchased land six years ago for $2.8\\$2.8$2.8 million with the intention of using it as a warehouse and distribution site. However, the company later decided to lease these facilities from a competitor instead. If sold today, the land could generate $3.2\\$3.2$3.2 million in net proceeds. The company now plans to build the new manufacturing plant on this land, which will cost $14.3\\$14.3$14.3 million to construct. Additionally, the site requires $825,000\\$825,000$825,000 in grading expenses to prepare it for construction. Question: What is the appropriate cash flow amount to use as the initial investment in fixed assets when evaluating this project? Explain your reasoning."},{"title":"Capital Structure 资产结构","path":"/wiki/fina/capital-structure.html","content":"Handout10: Key Take-Away 企业现值：无负债 V=EV=EV=E；有负债 V=E+DV=E+DV=E+D Cost of Equity Cost of Debt MM Proportion 1 在无税收、无破产成本的情况下，有 VU=VLV_U=V_LVU​=VL​. 这是因为无税收无破产成本的情况下，所有的 EBIT 只在企业、股权所有者、债权所有者之间流通。 MM Proportion 2: 股权、债权、资产对风险的敏感程度的关系为 DVβD+EVβE=βA\\frac{D}{V}\\beta_D+\\frac{E}{V}\\beta_E=\\beta_AVD​βD​+VE​βE​=βA​ 股权、债权、资产的期望回报率 (Expected Return) 为 DVE[RD]+EVE[RE]=E[RA]\\frac{D}{V}\\mathbb E[R_D]+\\frac{E}{V}\\mathbb E[R_E]=\\mathbb E[R_A]VD​E[RD​]+VE​E[RE​]=E[RA​] 改变股权债权结构 不影响股价 平衡点：ENE=DND\\frac{E}{N_E}=\\frac{D}{N_D}NE​E​=ND​D​，即股权价格等于债权价格。 Handout11: Key Take-Away Without Tax and Backruptcy Costs 企业可以通过 发行股票 issue Equity 举债 debt, bonds, bank loan 进行融资。 Value of Firm=Value of Equity+Value of Debt \\text{Value of Firm}=\\text{Value of Equity}+\\text{Value of Debt} Value of Firm=Value of Equity+Value of Debt MM Proportion I 在无税收、无破产成本的情况下，公司价值 VVV 不受资本结构影响，即无杠杆公司的价值等于有杠杆的公司 VU=VL=OCF V_U=V_L=OCF VU​=VL​=OCF With Tax and Bankruptcy Costs MM Proportion I 在存在企业所得税的情况下，公司价值（VLV_LVL​）随财务杠杆增加而提升 ，主要因利息税盾（Interest Tax Shield）的税收优惠效应。 VL=VU+D×Tc V_L=V_U+D\\times T_c VL​=VU​+D×Tc​ MM Proportion II 在有税环境下，股权成本（RER_ERE​）仍随杠杆增加而上升 ，但公式需调整以反映税盾效应： RE=RU+(RU−RD)×ED×(1−Tc) R_E=R_U+(R_U−R_D)\\times E_D\\times (1−T_c) RE​=RU​+(RU​−RD​)×ED​×(1−Tc​)税收降低了债务的实际成本（因利息税盾），但股权风险和成本仍随杠杆上升而增加。"},{"title":"Investment Decision Rule","path":"/wiki/fina/invest-decision-rule.html","content":"Investment Decision Rule … based on Returns 就是看最后拿到的钱 RewardRewardReward 是不是初始投资 InvestInvestInvest 多就行， … based on NPV NPV定义 净现值（Net Present Value, NPV）是评估投资项目价值的核心指标，计算方式为项目所有现金流（含初始投资）按贴现率折算后的现值总和。公式为： NPV=C0+∑i=1TCi(1+R)iNPV=C_0+\\sum_{i=1}^T\\frac{C_i}{(1+R)^i}NPV=C0​+∑i=1T​(1+R)iCi​​ CtC_tCt​: 第 ttt 期的现金流（C0C_0C0​ 为初始成本，通常为负值）。 RRR: 贴现率（通常为金融市场可获得的回报率）。 决策规则 NPV 0：项目收益高于金融市场回报，应接受。 NPV 0：项目收益低于金融市场回报，应拒绝。 NPV = 0：项目收益与金融市场相当，决策无差异（可结合其他因素考虑）。 关键原则 基准比较：以金融市场回报率为基准，衡量项目是否创造额外价值。 现金流方向： 现金流入（收入）为正值（Ct0）。 现金流出（成本）为负值（Ct0）。 沉没成本忽略：过去已发生且不可逆的成本（如前期调研费用）不计入NPV，仅考虑未来现金流。 注意事项 时间价值：现金流的时点影响现值，需准确对应贴现期数。 零NPV的意义：项目回报率等于贴现率（金融市场回报率），而非无收益。 总结：NPV规则通过量化项目相对于金融市场的净收益，为投资决策提供清晰标准，强调未来现金流和沉没成本的正确处理。 … based on IRR 预设项目的最少回报 (Minimum Return Required)，然后判断项目的 IRR 是否高于预期。 Internal Rate of Return (IRR) 是将单笔资金拓展到多笔资金，IRRIRRIRR 是年利率 (Annual Rate of Return)，使得整个项目周期结束后的 NPV 为零 NPV=0=C0+∑i=1T(11+IRR)iCi NPV=0=C_0+\\sum_{i=1}^T \\Big(\\frac{1}{1+IRR}\\Big)^i C_i NPV=0=C0​+i=1∑T​(1+IRR1​)iCi​ NPV or IRR 实际上，IRR 和 NPV 大致成反比关系。 Notice the role of the return from the financial sector (e.g., interest rate on bank deposit) in the NPV rule and IRR rule: in the NPV rule, it is the discount rate in the IRR rule, it is the (minimum) required return of the real investment project under consideration. conventional: 初始投资都是 cash outflow，后续所有现金流都是入账 cash inflow non-conventional: 不符合 conventional 的现金流情况 IRR Rule 的问题 当同时考虑多个项目时，先计算 xxx，使得这几个项目在 Discount Rate 为 xxx 时的 NPV 相同。 Payback Rule 考虑多久可以回本（不考虑 discount） Discounted Payback Rule 先 discount cash flow，再应用 Payback Rule Profitability Index PI=PV of future cash flowsinitial cost PI=\\frac{\\text{PV of future cash flows}}{\\text{initial cost}} PI=initial costPV of future cash flows​"},{"title":"Agency Problem 和 Market Efficiency","path":"/wiki/fina/market-efficiency.html","content":"Agency Problem 什么是 Agency Problem? 代理问题指公司管理层（代理人）与股东（委托人）之间的利益冲突。管理层可能牺牲股东利益，追求自身利益最大化。 例如，接受负净现值（NPV）项目以获取回扣。 委托——代理关系 股东（委托人）委托管理层（代理人）管理公司，但代理人可能以牺牲委托人利益为代价谋取私利。 Compensation Package 薪酬结构 股权激励：将管理层薪酬与公司股票表现挂钩，使其利益与股东一致。 例子：若管理层持有公司股票，接受负NPV项目会导致其持股价值下降，从而抑制此类行为。 固定薪酬的局限性：固定工资无法激励管理层为公司价值最大化努力。 合伙制中的代理问题 普通合伙（无限责任）：合伙人需以个人财产承担公司债务，降低冒险行为。 有限合伙：部分合伙人仅承担有限责任，可能增加代理问题。 例子：普通合伙人在面临破产风险时更谨慎，因其个人资产可能被追偿。 Efficient Market Hypothesis 有效市场假设 Technical Analysis基于股票的历史交易数据从历史交易的价格检测出规律，并总结为交易策略 Fundamental Analysis基于股票的内在价值从企业的商业运营衡量股票的价值例如 Dividend Discount Model (DDM): PV=∑iDi(1+r)iPV=\\sum_i \\frac{D_i}{(1+r)^i}PV=∑i​(1+r)iDi​​常见做法：track the performance of actively managed mutual funds, 因为……这些基金由专家管理这些专家也只能通过公开数据、公开信息来对股票进行股价和选股CAPM 模型可以用来看是否可以 outperform market Fundamental Analysis 的暗示：有的时候连专家也无法保证持续从股票获利，那么为什么要交昂贵的管理费？于是诞生了指数 (index funds)，进一步演化为 EFT (Exchange Traded Funds) 使得一群股票可以像一支股票那样进行买卖。 Weak Form Efficiency 【定义】股价已反映所有历史交易信息（如价格、成交量），Technical Analysis 无效。 Semi Strong Form Efficiency 股价已反映所有公开信息（如财报、行业新闻），Fundamental Analysis 无效。 Strong Form Efficiency 股价反映所有信息（包括内幕信息），内幕交易也无法获利。 但内幕交易违法，现实中几乎不存在 IPO IPO (Initial Public Offering) 指公司首次向公众发行股票并在证券交易所上市的过程，是私有企业转变为上市公司的关键步骤。 IPO 流程与参与者 前期准备与风险投资 (Venture Capital, VC) VC的作用： 资金与指导：VC 在 IPO 前为公司提供资金和战略支持，帮助其扩展业务、优化管理。 持股与退出：VC 通过持有公司股份（通常为40%以上）并在 IPO 后出售股票获利。 投资银行 (Investment Banks) 核心职责： 承销 (Underwriting)：投行承诺购买全部新股并转售给公众，承担发行风险。从企业购入股票的价格会低于售卖给公众的价格，从中获利。 firm commitment underwriting: 买下企业全部的股票 定价：确定发行价，需平衡公司融资需求与市场接受度。 组建承销团 (Syndicate)：多家投行联合承销以分散风险（如阿里巴巴 IPO 由高盛、瑞银等牵头）。 费用结构： 承销费通常为融资额的 4%∼7%4\\%\\sim 7\\%4%∼7% Under-Pricing IPO 发行价低于首日收盘价，导致投资者首日可获得超额收益。 原因： 信息不对称：投行可能刻意低估发行价以确保发行成功（如吸引长期投资者）。 市场热度：高抑价可制造“赚钱效应”，吸引更多投资者参与后续 IPO。 风险补偿：新股不确定性高，抑价作为对投资者的风险补偿。 可能带来的损失： 公司损失：抑价导致公司融资额减少 投机行为：散户可能因追逐首日涨幅盲目申购，面临破发风险"},{"title":"PV, FV, NPV","path":"/wiki/fina/pv-fv-npv.html","content":"PV, NPV Present Value: 一笔在未来会获得的钱在当下的价值 Future Value: 一笔现在的钱在未来的价值 实际上 PV 和 FV 非常简单，主要就是考虑利滚利 (Compounding of Interest)，把指数算清楚即可。 某一笔钱的 PV/FV 钱的 PV 相当于说，如果某笔 TTT 年后的钱 QQQ，且利率 rrr 保持不变，那么现在要存 PVPVPV 块钱，这样经过 TTT 年的利滚利，这 PVPVPV 块钱就变成了 QQQ 块钱。 PV×(1+r)T=Q PV\\times (1+r)^T=Q PV×(1+r)T=Q反过来，Future Value 就是，现在存的 QQQ 块钱，在 TTT 年后会变成 FVFVFV 块钱。 FV=Q×(1+r)T FV=Q\\times(1+r)^T FV=Q×(1+r)T 现金流（多笔钱）的 PV/FV 本质就是多笔钱的 FV/PV 全部换算到当前的时间点， PVaggregate=∑PViFVaggregate=∑FVi PV_{aggregate}=\\sum PV_{i}\\\\ FV_{aggregate}=\\sum FV_{i}\\\\ PVaggregate​=∑PVi​FVaggregate​=∑FVi​由于每笔钱的利滚利时间不同，因此右式的和式通常又可以写成有限项等比数列求和的形式 Annuity, Perpetuity Perpetuity 表示永久持续的恒定现金流，用 CCC 表示每年的现金流（且第一笔现金在一年后） 必须满足两个条件： The cash flows and the interest rates are constant over time, The first cash flow occurs one year from today. 那么所有这些钱的 PV 就是 PV=∑i=1∞C×(11+r)i=Cr \\begin{aligned} PV=\\sum_{i=1}^{\\infin} C\\times(\\frac{1}{1+r})^i\\\\ =\\frac{C}{r} \\end{aligned} PV​=i=1∑∞​C×(1+r1​)i=rC​​ Present Value of Annuity 与 Perpetuity 的区别在于 Annuity 有时限。假设经过 TTT 年，那么 Annuity 的 PV 就等于 PV=C(11+r)+C(11+r)2+⋯+C(11+r)T=C×1−(11+r)Tr PV=C(\\frac{1}{1+r})+C(\\frac{1}{1+r})^2+\\dots+C(\\frac{1}{1+r})^T=C\\times\\frac{1-(\\frac{1}{1+r})^T}{r} PV=C(1+r1​)+C(1+r1​)2+⋯+C(1+r1​)T=C×r1−(1+r1​)T​在一笔现金的时候我们知道，(11+r)T(\\frac{1}{1+r})^T(1+r1​)T 就是 Present Value Factor，所以也可以直接代入写成 PV=1−Present Value Factor(T)rC PV=\\frac{1-\\text{Present Value Factor}(T)}{r}C PV=r1−Present Value Factor(T)​C Future Value of Annuity 如果以第 TTT 年的价值作为基准，把每一笔钱的 FV 都加起来，就能得到 FV=(1+r)T−1rC=Future Value Factor(T)−1rC FV=\\frac{(1+r)^T-1}{r}C=\\frac{\\text{Future Value Factor}(T)-1}{r}C FV=r(1+r)T−1​C=rFuture Value Factor(T)−1​C Effects of Inflation 由于通胀 (Inflation) 的存在，尽管手上的钞票数量变多了，但这并不意味着购买力 (Purchasing Power) 提升。 Purchasing Power the quantity of goods and services that we can buy with our money. … on Nominal/Real Interest Rate Fisher’s Equation Nominal IR RRR: interest rate in terms of money Real IR rrr: interest rate in terms of purchasing power 我们用 hhh 表示物价的变化，即通胀率 (Inflation Rate)，则可以得出他们之间的关系 (Fisher Equation) (1+r)(1+h)=(1+R) (1+r)(1+h)=(1+R) (1+r)(1+h)=(1+R) 证明 考虑一开始的钱 QQQ，一开始的物价 PPP，则今年的购买力为 QP\\frac{Q}{P}PQ​，明年的购买力（即经过一年的通胀）为 Q(1+R)P(1+h)\\frac{Q(1+R)}{P(1+h)}P(1+h)Q(1+R)​ 根据 Real Interest Rate 的定义，我们有 Purchase Poweri+1−Purchase PoweriPurchase Poweri=Real IRQ(1+R)P(1+h)QP=1+r(1+r)(1+h)=1+R \\begin{aligned} \\frac{\\text{Purchase Power}_{i+1}-\\text{Purchase Power}_i}{\\text{Purchase Power}_i}=\\text{Real IR}\\\\ \\frac{\\frac{Q(1+R)}{P(1+h)}}{\\frac{Q}{P}}=1+r\\\\ (1+r)(1+h)=1+R \\end{aligned} Purchase Poweri​Purchase Poweri+1​−Purchase Poweri​​PQ​P(1+h)Q(1+R)​​(1+r)(1+h)​=Real IR=1+r=1+R​ 由于这些值都很小，所以我们通常直接近似为 r≊R−hr\\approxeq R-hr≊R−h … on PV NPV Inflation 并不影响 PV 和 NPV。但必须 discount nominal cash at a nominal rate 或者 discount real cash at a real rate 证明 考虑 inflation rate 为 hhh，nominal interest rate 为 RRR，考虑 nnn 年后的一笔钱 CCC (nominal)，那么用 Nominal Interest Rate 算的话 PVnominal=C(1+R)n PV_{nominal}=\\frac{C}{(1+R)^n} PVnominal​=(1+R)nC​用购买力计算的话，nnn 年后的购买力为 C(1+h)n\\frac{C}{(1+h)^n}(1+h)nC​，因此 discount at a real rate 的话，即为 PVreal=C(1+h)n(1+r)n PV_{real}=\\frac{\\frac{C}{(1+h)^n}}{(1+r)^n} PVreal​=(1+r)n(1+h)nC​​代入 Fisher’s Equation 1+R=(1+h)(1+r)1+R=(1+h)(1+r)1+R=(1+h)(1+r) 可得 PVreal=PVnominal PV_{real}=PV_{nominal} PVreal​=PVnominal​因此其实并不影响 Present Value。对于 Net PV，由于基准年的金额有 Nominal=RealNominal=RealNominal=Real，因此也不影响 NPV"},{"title":"Risk of Return","path":"/wiki/fina/risk-and-return.html","content":"Risk of Return 现实世界中，未来的 Return 并非确定的，而是一个概率分布。 Probability 我们可以计算 Expected Return E[R]=∑P(r)⋅r=4.30% \\mathbb E[R]=\\sum P(r)\\cdot r=4.30\\% E[R]=∑P(r)⋅r=4.30%进一步的，可以计算其方差： Var(R)= Var(R)= Var(R)=我们可以把 return 的方差视为 volatility，方差越大，股价越不稳定。 Risk Free Risk Free 的意思就是方差为 000，通常只有国债才能做到。 Portfolio Portfolio 就是 a basket of assets，每一个 asset 都有一定的比重 Risk Systematic 经济体内的每一家企业都会遇到的风险（例如政治稳定、税收等等） Unsystematic 这类风险只会影响个别企业（例如舆论） Risk Diversification"},{"title":"Valuation of Stocks","path":"/wiki/fina/valuation-of-stocks.html","content":"Stock 股票 拥有对企业的所有权 Ownership 可以投票 以分红的形式分享利益 没有法律义务支付一定数额的分红 企业先付 bonds 产生的利息，再支付分红 支付的分红不能用于抵扣税款 Dividend Discount Model (DDM 模型) stock 会产生分红现金流， PV of Dividend Stream=∑i=1+∞Di(1+R)i \\text{PV of Dividend Stream}=\\sum_{i=1}^{+\\infin}\\frac{D_i}{(1+R)^i} PV of Dividend Stream=i=1∑+∞​(1+R)iDi​​也被称为 funcdamental value of stock. 在市场完全竞争的情况下，价格 P0=PV of Dividend StreamP_0=\\text{PV of Dividend Stream}P0​=PV of Dividend Stream. 这个模型被称为 Dividend Discount Model. Special Case 1: 恒常分红现金流 当 Di=DjD_i=D_jDi​=Dj​ 时，通过等比数列求和可以简便地计算 P0P_0P0​ P0=D1R P_0=\\frac{D_1}{R} P0​=RD1​​ Special Case 2: 现金流等比增长 也被称为 Gordon’s Growth Model，设分红以每年 ggg 的速度增长，即 Di=(1+g)Di−1D_i=(1+g)D_{i-1}Di​=(1+g)Di−1​，则有 P0=D1R−g P_0=\\frac{D_1}{R-g} P0​=R−gD1​​ 一些指标 Dividend Yield 计算产生的分红 Dividend Yield=D1P0 \\text{Dividend Yield}=\\frac{D_1}{P_0} Dividend Yield=P0​D1​​ Capital Gains Yield 如果在 ttt 时刻卖出 stock，可以计算获得的资本 Capital Gains Yield=Pt−P0P0 \\text{Capital Gains Yield}=\\frac{P_t-P_0}{P_0} Capital Gains Yield=P0​Pt​−P0​​ Common vs. Preferred Common stock 有投票权 Preferred stock 享有优先分红的权利 由于 Preferred stock 的期限是无穷的，我们可以直接按 Perpetual Bond 来看待。如果每年分红 DpD_pDp​，利率 RpR_pRp​，初始股价 P0P_0P0​，根据 DDM 模型，有 P0=DpRp P_0=\\frac{D_p}{R_p} P0​=Rp​Dp​​"},{"title":"RMSNorm","path":"/wiki/aitactics/RMSNorm.html","content":"372f7a9688aae3bb4ec6e6afab19ca439bb95e6d91164a50dda5c3f9a4f813ff Password is needed."},{"title":"LayerNorm","path":"/wiki/aitactics/layernorm.html","content":"LayerNorm 假设是在 CNN 的语境下，我们对一个 N×H×W×CN\\times H\\times W\\times CN×H×W×C 的 tensor 做 LayerNorm。 LayerNorm 做的事实际上是对于每一个 H×W×CH\\times W\\times CH×W×C 做 Normalization."},{"title":"MoE 架构","path":"/wiki/llm/Mixture-of-experts.html","content":"MoE 架构 MoE 代码实现：以 MiniMind 为例 Experts 首先定义专家模块，Experts 是 Experts(FeedForward) Code class FeedForward(nn.Module): def __init__(self, config: LMConfig): super().__init__() self.w1 = nn.Linear(config.dim, config.hidden_dim, bias=False) self.w2 = nn.Linear(config.dim, config.hidden_dim, bias=False) self.w3 = nn.Linear(config.hidden_dim, condig.dim, bias=False) self.dropout = nn.Dropout(config.dropout) def forward(self, x): return self.dropout(self.w3(F.silu(self.w1(x)) * self.w2(x))) Router 然后，我们来实现 Router 路由器。Router 接收一个 Token，计算出概率取 Top K 后转发给对应的专家。 Code class MoEGate(nn.Module): def __init__(self, config: LMConfig): super().__init__() self.config = config self.top_k = config.num_experts_per_token self.n_routed_experts = config.n_routed_experts self.scoring_func = config.scoring_func self.alpha = config.aux_loss_alpha self.seq_aux = config.seq_aux self.norm_topk_prob = config.norm_topk_prob self.gating_dim = config.dim self.weight = nn.Parameter( torch.empty((self.n_routed_experts, self.gating_dim)) ) self.reset_parameter() def reset_parameter(self) - None: import torch.nn.init as init init.kaiming_uniform_(self.weight, a=math.sqrt(5)) def forward(self, tokens: torch.Tensor): batch, seq_len, d = tokens.shape tokens = einops.rearrange(tokens, batch seq dim - (batch seq) dim) logits = F.linear(tokens, self.weight, None) if self.scoring_func == softmax: scores = logits.softmax(dim=-1) else: raise NotImplementedError( Unsupported scoring function, ) topk_weight, topk_idx = torch.topk( scores, k=self.top_k, dim=-1, sorted=False, ) if self.top_k 1 and self.norm_topk_prob: denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20 topk_weight = topk_weight / denominator if self.train and self.alpha 0.0: scores_for_aux = scores aux_topk = self.top_k topk_idx_for_aux_loss = topk_idx.view(batch, -1) if self.seq_aux: scores_for_seq_aux = scores_for_aux.view(batch, seq_len, -1) ce = torch.zeros( batch, self.n_routed_experts, device=tokens.device, ) ce.scatter_add_( 1, topk_idx_for_aux_loss, torch.ones( batch, seq_len * aux_topk, device=tokens.device ) ).div_(seq_len * aux_topk / self.n_routed_experts) aux_loss = ( (ce * scores_for_seq_aux.mean(dim=-1)) .sum(dim=1) .mean() * self.alpha ) else: mask_ce = F.one_hot( topk_idx_for_aux_loss.view(-1), num_classes=self.n_routed_experts, ) ce = mask_ce.float().mean(0) Pi = scores_for_aux.mean(0) fi = ce * self.n_routed_experts aux_loss = (Pi * fi).sum() * self.alpha else: aux_loss = 0.0 return topk_idx, topk_weight, aux_loss 以下分析约定： Batch size BBB，即一个 batch 包含 BBB 句句子 Sequence Length SSS，即一句句子包含 SSS 个 Token 词嵌入向量维度 HHH 专家数量 EEE 每个 Token 被转发到 Top KKK 个专家 参数含义 top_k 为每一个 Token 选择几个专家进行训练 n_routed_experts 为专家总数 scoring_func 用于计算 Token 和专家之间的评分 aux_loss_alpha 辅助损失的 alpha 参数 seq_aux 控制是否在序列级别上计算辅助损失 norm_topk_prob 是否对概率进行归一化 由于我们对每一个 Token 计算它被发送到某一个专家的概率，在 __init__() 函数里，我们令 Token 的维度为 d=d=d= self.gating_dim，专家数量为 n=n=n= self.n_routed_experts，那么我们希望 Router 输出的矩阵大小就是 Router:RB×S×H↦RB×S×E \\text{Router}:\\R^{B\\times S\\times H}\\mapsto \\R^{B\\times S\\times E} Router:RB×S×H↦RB×S×E因此这里的 self.weight 是 RH×E\\R^{H\\times E}RH×E 大小的矩阵，负责计算一个 Token 的被转发到 Expert 的概率。 前向传播 forward() Tensor 输入是 B×S×HB\\times S\\times HB×S×H，因为我们只关心 Token 发送到哪个专家，所以我们首先把 Tensor 拍成二维 BS×HBS\\times HBS×H，并用 self.weight 计算概率，用 softmax() 归一化。F.linear(x, A, bias=None) 计算 y=xA⊺ y=xA^\\intercal y=xA⊺因此，这里的 score 大小为 BS×EBS\\times EBS×E 接着，我们调用 torch.topk() 选取前 KKK 个专家，返回 score 中对应的权重和下标。此时 topk_weight, topk_idx 大小均为 BS×KBS\\times KBS×K 然后对选择出来的 KKK 个专家的权重再进行一次归一化（除以 denominator）。不过这一步是可选的 接着进入 if self.train and self.alpha 0.0: 判断，目的是为了平衡专家之间的负载。首先，代码确保只在训练模式以及需要平衡负载的时候才会启用。 如果需要计算 sequence level 的 loss，那么 topk_idx 首先被拍平成 (B,SK)(B,SK)(B,SK) 因为要对 sequence level 计算专家负载损失，所以我们先定义每一句句子上专家的负载损失，其大小为 (B,E)(B,E)(B,E). 然后遍历 topk_idx 中的每一个元素，用 scatter_add_() 将 topk_idx 中的每一个元素添加到对应的 sequence 里对应的专家中。 这个流程结束之后，ce (count experts) 保存的就是每一句句子的专家负载。随后，我们对每一句句子都归一化其专家负载：一句句子会产生 SKSKSK 个 counting，总数为 SKSKSK 我们沿 SSS 轴对 scores_for_seq_aux 计算平均值 scores.mean(dim=1) 这就表示每句句子与每个 expert 之间的得分（token 与 experts 得分的平均），其大小变为 (B,E)(B, E)(B,E). 然后再将 ce 与 scores.mean() 对应位置相乘，ce 可以理解为每句句子中 expert 的频率（这个 expert 在这句句子里总是被分配处理 token），scores.mean() 可以理解为每句句子中 expert 对于每个 token 的重要程度（这个 expert 总是被 MoEGate 认为与 token 关联很大），大小变为 (B,E)(B,E)(B,E)，但此时仍然是每句句子与 expert 的关联。 因此再对 EEE 求和取平均 .sum(dim=1).mean()，把 expert 与不同句子之间的频率与关联度整合起来，即对于这些句子 expert 的频率与关联度，也即 expert 的负载，其大小先变为 (E,)(E,)(E,)，再变为 (1,)(1,)(1,)。最后再乘上标量 self.alpha. 这就是我们的 sequence level 的 aux loss. 值得一提的是，这里还额外乘了一个 EEE，推测是为了让梯度不至于太小 Why It Makes Sense 因为我们的训练目标是让 Loss 尽可能的小，对于这个 Aux Loss 而言，如果某一个专家的负载特别大，那么就说明 每一个 Batch 内的所有 Tokens，这个专家对应的 scores 都会比较大 由于 torch.topk 总是都把这个专家选上，因此 ce 计算出来的加权也比较大 所以根据排序不等式，负载越不均衡，计算出来的 aux_loss 也就越大。那么反过来说，如果 aux_loss 越小，说明专家之间的负载越均衡。 如果不关心 sequence level 的 loss，那么我们直接把这一个 batch 的所有 token 拍到一起（总共 B⋅SB\\cdot SB⋅S 个 token，总共被分配 BSKBSKBSK 个专家），我们把 (BSK,1)(BSK, 1)(BSK,1) 的专家重新 encode 为 one-hot vector，即 (BSK,E)(BSK,E)(BSK,E) 那么我们对 BSKBSKBSK 轴求平均，fi 向量大小变为 (1,E)(1,E)(1,E)，就计算出来了某个专家处理的 token 数占所有 token 的比值（即频率）。因为 one-hot vector 不是 000 就是 111. 类似的，我们也对 scores (大小为 (BS,E)(BS,E)(BS,E)) 做 token level 的计算，直接按 BSBSBS 轴取平均即可，Pi 大小变为 (1,E)(1,E)(1,E)，即每个专家在所有 token 上的平均得分（关联度） 同样地，我们直接将 fi 与 Pi 对应位置相乘，求和乘上 self.alpha，这一步的目的和 sequence level 的 aux loss 是一样的。 MoE Feed Forward (MoEFFN) 然后我们来把他们组合到一起：首先，我们为 MoEFFN 定义好门控和专家，以及共享专家（无论如何都要处理 token） class MOEFeedForward(nn.Module): def __init__(self, config: LMConfig): super().__init__() self.config = config self.experts = nn.ModuleList( [ FeedForward(config) for _ in range(config.n_routed_experts) ] ) self.gate = MoEGate(config) if config.n_shared_experts is not None: self.shared_experts = FeedForward(config) 然后编写训练和推理。训练和推理的区别在于 推理模式下，Token 只转发给最优的 Expert。但在训练模式下，Token 会被转发给每一个 Expert Training 我们来考察一下训练时的代码。 这里，x.repeat_interleave() 重复输入数据，目的是让一个 token 可以多次被不同的 expert 处理，提升 expert 的泛化性 然后 y 就是计算 token 经过专家计算后输出的结果，并且将类型转为半精度浮点数 float16，此时的张量形状为 (BS×K,H)(BS\\times K, H)(BS×K,H)，经过 .view(*topk_weight.shape, -1) 之后变为 if self.training: # 训练模式下，重复输入数据 x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0) y = torch.empty_like(x, dtype=torch.float16) for i, expert in enumerate(self.experts): y[flat_topk_idx == i] = expert(x[flat_topk_idx == i]).to( y.dtype ) # 确保类型一致 y = (y.view(*topk_weight.shape, -1) * topk_weight.unsqueeze(-1)).sum(dim=1) y = y.view(*orig_shape) Inferencing 我们来考察前向传播过程。 提取出 x (输入的 Batch of sequence of tokens) 的维数信息之后，先经由 self.gate(x) 计算每一个 token 对应的专家，然后直接拍成 a sequence of tokens (BS,H)(BS,H)(BS,H)，topk_idx 则直接拍成 (BSK,)(BSK,)(BSK,) identity = xorig_shape = x.shapebsz, seq_len, _ = x.shape# 使用门控机制选择专家topk_idx, topk_weight, aux_loss = self.gate(x)x = x.view(-1, x.shape[-1])flat_topk_idx = topk_idx.view(-1) 随后进入 y = self.moe_infer(x, flat_topk_idx, topk_weight.view(-1, 1)).view(*orig_shape) 进行计算。 这里 .argsort() 的逻辑是：因为 flat_expert_indices 的值是专家的编号，通过 argsort()，idxs 把同一个专家要处理的 token 下标聚集到一起。 tokens_per_expert 以前缀和的方式，计算每一个专家处理的 token 下标的范围。 token_idxs 从 idxs 出发计算某一个 idxs[i] 对应的是第 token_idxs[i] 个 token。这是因为每一个 token 都会分给 top KKK 个专家，因此从下标来说，i∗K→(i+1)∗K−1i*K\\to (i+1)*K-1i∗K→(i+1)∗K−1 (idxs 保存的正好都是下标) 对应的都是第 iii 个 token，因此直接整数出除法可以计算出对应第几个 token. @torch.no_grad()def moe_infer(self, x, flat_expert_indices, flat_expert_weights): expert_cache = torch.zeros_like(x) idxs = flat_expert_indices.argsort() tokens_per_expert = flat_expert_indices.bincount().cpu().numpy().cumsum(0) token_idxs = idxs // self.config.num_experts_per_tok 接着，我们枚举每一个专家，拿出它需要处理的所有 tokens (即代码里的 token_idxs[start_idx : end_idx] 以及 x[exp_token_idx]) 我们把这些 token 经过 expert(expert_tokens) 计算、输出，得到 expert_out，乘上（对于这个 token 而言）每一个 expert 的权重。通过 scatter_add_()，expert_cache 包含了每个 token 位置的加权专家输出总和。 for i, end_idx in enumerate(tokens_per_expert): start_idx = 0 if i == 0 else tokens_per_expert[i - 1] if start_idx == end_idx: continue expert = self.experts[i] exp_token_idx = token_idxs[start_idx:end_idx] expert_tokens = x[exp_token_idx] expert_out = expert(expert_tokens).to(expert_cache.dtype) expert_out.mul_(flat_expert_weights[idxs[start_idx:end_idx]]) # 使用 scatter_add_ 进行 sum 操作 expert_cache.scatter_add_( 0, exp_token_idx.view(-1, 1).repeat(1, x.shape[-1]), expert_out )return expert_cache 除此之外，还需要加上共享专家的输出。不过这里的话，如果在推理模式，self.aux_loss 其实没作用 if self.config.n_shared_experts is not None: y = y + self.shared_experts(identity)self.aux_loss = aux_lossreturn y"},{"title":"Attention 中的 KV Cache","path":"/wiki/llm/attn-kv-cache.html","content":"KV Cache KV Cache（键值缓存）是 Transformer 模型推理优化中的核心技术，其核心思想是缓存 Attention 机制中已计算的 Key 和 Value 矩阵，避免重复计算，从而减少计算量并提升推理效率。"},{"title":"Transformer 架构","path":"/wiki/llm/classic-transformer.html","content":"Before Transformer: RNN, Attention 在 RNN 里，我们用一个隐藏变量 hth_{t}ht​ 记录前 ttt 个 Input Token 所代表的 Context。如果要生成第 t+1t+1t+1 个 Output Token 的话，就利用输入的第 t+1t+1t+1 个 Input Token 和代表了前文信息的隐藏状态 hth_{t}ht​ 做 Attention Alignment 计算出一个向量，再转换成 Word. 这样一个 AutoRegressive 模型有两个显著的问题 由于是时序生成（必须一个一个 Token 地生成），无法并行 由于时序生成，如果要保存很久远的信息，隐藏状态 hth_tht​ 的大小就必须非常大，导致了 Computation Memory Overhead Model Arch Transformer 总体是 Encoder-Decoder 架构。Encoder 将 Input 杂谈 其实 Transformer 的本质也是概率统计模型，其生成过程就是给定前 iii 个词的情况下生成第 i+1i+1i+1 个词；训练过程的 Loss Function 是 max⁡∑ilog⁡Pθ(wi∣wi−k,…,wi−1) \\max \\sum_i \\log P_\\theta(w_i|w_{i-k},\\dots,w_{i-1}) maxi∑​logPθ​(wi​∣wi−k​,…,wi−1​) 这里的 kkk 其实就是 Context Length 的意思。"},{"title":"Decoder-Only Transformer","path":"/wiki/llm/decoder-only-transformer.html","content":"Decoder-only Transformers Decoder-only 架构通常用于生成任务，代表作包括 GPT，GPT-2 等。 什么是生成任务 简单来说，Decoder-only 解决的生成任务是指：给定前 nnn 个单词 x1,x2,…,xnx_1,x_2,\\dots,x_nx1​,x2​,…,xn​，要求输出第 n+1n+1n+1 个单词。可以看出，这类生成任务的本质是 AutoRegressive 的。 近年来，出现了使用 Diffusion 作为 Language Model 的生成，达到了极快的生成速度。"},{"title":"Multi-Head Attention","path":"/wiki/llm/multi-head-attn.html","content":"Multi-Head Attention Initial 多头注意力就是将 query 的不同部分和 KV 的不同部分分别做 Attention 之后再拼接起来，达到“不同 Head 感受句子的不同信息”的效果。 Multi-Head Attention 的 Head 分的是什么 我们假定有 HHH 个 head，多头注意力做的事是将维度为 DDD 的 word embedding 分成 HHH 个维度为 DH\\frac{D}{H}HD​ 的子 embedding。 考虑输入的一批训练样本的大小为 (B,S,D)(B,S,D)(B,S,D)，那么实际上每一个 Head 计算的 Self-Attention 是 (B,S,DH)(B,S,\\frac{D}{H})(B,S,HD​). 如果硬要直观理解的话……我们可以假想 word embedding 的不同部分蕴含了不同的内在含义。例如，一个 DDD 的 word embedding 的前 D/8D/8D/8 维度编码了“数学”，接下来 D/8D/8D/8 编码了 “情感”……那么，我们 split head 就是想让 Attention 学习到这批样本所有 token 之间、不同领域上的关联。例如 Head 1 学习到数学层面的联系，Head 2 学习到情感层面的联系……"},{"title":"RoPE 旋转位置编码","path":"/wiki/llm/rotary-emb.html","content":"RoPE RoPE 的出发点是：通过绝对位置编码的方式实现相对位置编码 什么意思呢？比如说考虑某个英文词组总是以 A xx yy B 的形式出现，但是出现位置有可能是 1,2,3,4、也可能是 10,11,12,13，RoPE 就可以只通过这些单词的下标计算出代表相对位置的 embedding."},{"title":"Self Attention","path":"/wiki/llm/self-attn.html","content":"Self Attention 所谓的 Self Attention 其实只是 Q=K=VQ=K=VQ=K=V 的一种特例 Self-Attn(X)=Attention(X,X,X) \\text{Self-Attn}(X)=\\text{Attention}(X,X,X) Self-Attn(X)=Attention(X,X,X)直觉理解的话，可以认为是在句子的内部做 Attention，寻找句子内部的联系。"},{"title":"Tokenizer, BPE 算法","path":"/wiki/llm/tokenizer-bpe.html","content":"Byte-Pair Encoding 算法 Byte-Pair Encoding（BPE）算法是一种常用于分词器（Tokenizer）中的无监督分词方法，其主要思想是将文本中最常见的字符对（或子词对）不断合并，从而构建出一个词汇表。由于进行多轮 （假设 kkk 轮）合并，而每一次合并都会基于统计频率将一对 Token 合并为一个新 Token，因此在 kkk 轮迭代后，BPE 算法可以将长度为 kkk 的单词合并为一个 Token 将输入的文本转化为 UTF-8 Encoding 统计 Byte-Pair 的频率 计算频率最高的 Byte-Pair，合并为一个新的 Token 用新的 Token 替换旧 Byte-Pair 出现的位置 回到第 222 步，重新统计 Byte-Pair (Token-Pair) 频率 直到词汇表大小达到预设值 BPE 算法通过这种逐步合并的方式，不仅能有效地表示常见词汇，还能灵活处理低频词和新词，对于大型语言模型的分词和词表构建有很大的优势。 BPE 代码实现"},{"title":"Tokenizer 分词器","path":"/wiki/llm/tokenizer.html","content":"Tokenizer Tokenizer 在 LLM（大型语言模型）的上下文中指的是负责将输入文本分解成称为 tokens 的更小单元的组件。这些 tokens 是模型处理的基本元素（例如单词、子词或字符）。Tokenizer 将原始文本转换为模型可以处理的数字表示，并且在处理之后，还能将 tokens 转换回人类可读的文本。 Hugging Face Tokenizer: tokenizer.json"},{"title":"Attention 机制","path":"/wiki/llm/vanilla-attn.html","content":"Attention 机制 Attention 机制的直观理解就是：给定字典 D={K,V}D=\\{K,V\\}D={K,V}，和一个 Token QQQ。Attention 基于一个朴素的理解，例如英语里，如果两个单词长得差不多，那么语义应该也差不多（也就是词性变换）。如果可以量化出相似度，那么我们就可以一定程度上可以表示出这个单词的意思 对于 Token 而言，其都是长度为 ddd 的词向量，不同的相似度计算方式会产生不同的 Attention 计算公式。一般而言，可以写成下面这种形式 Attention(Q,K,V)=score(Q,K)⋅V \\text{Attention}(Q,K,V)=\\text{score}(Q,K)\\cdot V Attention(Q,K,V)=score(Q,K)⋅V这里，如果字典的大小为 mmm，查询 nnn 个词，每个词的 Embedding 维度为 ddd，代表“意义”的 Value 的维度为 dvd_vdv​，则这三个矩阵的大小分别为 Q∈Rn×dK∈Rm×dV∈Rm×dv Q\\in \\R^{n\\times d}\\\\ K\\in \\R^{m\\times d}\\\\ V\\in \\R^{m\\times d_v} Q∈Rn×dK∈Rm×dV∈Rm×dv​例如经典的 Scaled Dot Product Attention，其 score function 就是 softmax(QKTd)\\text{softmax}(\\frac{QK^T}{\\sqrt d})softmax(d​QKT​) Attention(Q,K,V)=softmax(QK⊺d)⋅V \\text{Attention}(Q,K,V)=\\text{softmax}\\Big( \\frac{QK^\\intercal}{\\sqrt{d}} \\Big)\\cdot V Attention(Q,K,V)=softmax(d​QK⊺​)⋅V"},{"title":"Comparative Statics","path":"/wiki/microecon/comparative-statics.html","content":"Change of Demand Curve 令 xxx 表示商品，那么其 Demand Curve 可以表示为 Qxd=a+bPxd+… Q^d_x=a+bP^d_x+\\dots Qxd​=a+bPxd​+… Increase in Demand 直线向右上移动，可以是向上平移，也可以是向右平移 Movement 影响因素：Income Normal Good Income↑ ⟹ Qxd↑\\text{Income}\\uparrow \\implies Q^d_x\\uparrowIncome↑⟹Qxd​↑ Inferior Good Income↑ ⟹ Qxd↓\\text{Income}\\uparrow \\implies Q^d_x\\downarrowIncome↑⟹Qxd​↓ 影响因素：Population 影响因素：Price of Substitutes Psubstitute↑ ⟹ Qxd↑ P_{\\text{substitute}}\\uparrow \\implies Q^d_x\\uparrow Psubstitute​↑⟹Qxd​↑当其他平替的价格上涨，消费者自然而然会转向价格更低的 xxx 影响因素：Price of Complement Pcomplement↑ ⟹ Qxd↓ P_{\\text{complement}}\\uparrow \\implies Q^d_x\\downarrow Pcomplement​↑⟹Qxd​↓ 影响因素：Expectation The expectation of a higher (lower) price for a good in the future increases (decreases) current demand for the good. 影响因素：Tastes Supply Movement of Supply Curve Increase in Supply Technology Entry implies more sellers in the market increasing supply. Exit implies fewer sellers in the market decreasing supply. Sellers will supply less of a good if the price of an alternate good using the same inputs rises (and vice versa). (-)"},{"title":"Cost Benefit Analysis","path":"/wiki/microecon/cost-benefit.html","content":"Rational Decision 对每一件事进行衡量，其付出为 C(x)C(x)C(x)，获得为 B(x)B(x)B(x)，那么定义 Economic Surplus ES(x)ES(x)ES(x) 为 ES(x)=B(x)−C(x) \\boxed{ES(x)=B(x)-C(x)} ES(x)=B(x)−C(x)​Cost Benefit Principle 当且仅当 ES(x)≥0ES(x)\\ge 0ES(x)≥0 时，才会做 xxx，此时收益大于付出；否则不做。 Cost/Benefit 只包含会影响被决策的 Cost 和 Benefit Sunk Cost Marginal(Additional) Cost/Benefit Only if Marginal Benefit≥Marginal Cost \\text{Marginal Benefit}\\ge\\text{Marginal Cost} Marginal Benefit≥Marginal Cost Allocation of Resources 优先分配给 Marginal Benefit 多的 Opportunity Cost 所有不选的选项里，Economic Surplus(不选的选项的 Cost = Benefit，没有这部分的开销了) 最大的那个"},{"title":"竞争下的 Cost 与 Profit Maximization","path":"/wiki/microecon/cost-maxprofit-under-competition.html","content":"大前提 在完全竞争的市场下，任何一家企业都无法操控市场价格，只能根据市场价格调整自身的产量。即市场价格决定每一家企业的定价。 逻辑 由于企业无法操控市场价格，如果企业的定价高于市场价格，那么消费者必然转向其替代品；如果企业定价低于市场价格 企业运行的驱动动力：利益 企业必然追求利益。当产量为 QQQ 时，利润 Profit π(Q)\\pi(Q)π(Q) 定义为 π(Q)=TR(Q)−TC(Q) \\boxed{\\pi(Q)=TR(Q)-TC(Q)} π(Q)=TR(Q)−TC(Q)​这里的 Total Cost TC(Q)TC(Q)TC(Q) 包含 explicit cost 和 implicit cost. Economic Profit 与 Accounting Profit 这两个概念会在分析长期行为时提到，简而言之 Economic Profit 需要包含 Implicit Cost Accounting Profit 则不需要包含 后文提到的 Profit 若无特殊说明都是指 Economic Profit. 利益最大化 根据 Cost-Benefit Analysis，利润最大化的时候对应的产量 Q∗Q^\\astQ∗ 必然有 MR(Q∗)=MC(Q∗) MR(Q^\\ast)=MC(Q^\\ast) MR(Q∗)=MC(Q∗)而且，此处的 MRMRMR 恒等于市场价格，即 MR(Q∗)=PMR(Q^\\ast)=PMR(Q∗)=P。因此，在完全竞争的市场下，总是有 P=MC(Q∗) \\boxed{P=MC(Q^\\ast)} P=MC(Q∗)​ 企业的成本 分为两种： 固定成本 (Fixed Cost)：短期内无法改变 Quantity，例如生产机器、办公楼等等 可变成本 (Variable Cost)：短期内可以改变 QUantity，例如劳动力、生产原料等等 在后面分析企业行为的时候也会用到这两个概念。简单来说，如果考察企业的短期行为，由于固定成本可以看作是已经产生费用，因此应当看作 Sunk Cost，不应参与短期行为决策；但对于长期行为而言，也应将固定成本考虑进去。 企业的市场行为 既然要赚取利益，那么企业是否继续参与市场必然与 Profit 有关。当 π(Q∗)0\\pi(Q^\\ast)\\lt 0π(Q∗)0 时，企业的利润小于零，企业会选择退出市场；否则就有利可图，会继续参与市场。 也可以理解为 π(Q=Q∗)\\pi(Q=Q^\\ast)π(Q=Q∗) 与 π(Q=0)\\pi(Q=0)π(Q=0) 之间进行比较 短期行为 从短期来看，企业的 Fixed Cost 不应计入决策过程，企业是否退出市场取决于利润 π(Q∗)≥π(0)TR(Q∗)−TC(Q∗)≥TR(0)−TC(0)PQ∗−TVC(Q∗)−TFC≥0−TVC(0)−TFCPQ∗≥TVC(Q∗) \\begin{aligned} \\pi(Q^\\ast)\\ge \\pi(0)\\\\ TR(Q^\\ast)-TC(Q^\\ast)\\ge TR(0)-TC(0)\\\\ PQ^\\ast-TVC(Q^\\ast)-TFC\\ge 0-TVC(0)-TFC\\\\ PQ^\\ast\\ge TVC(Q^\\ast)\\\\ \\end{aligned} π(Q∗)TR(Q∗)−TC(Q∗)PQ∗−TVC(Q∗)−TFCPQ∗​≥π(0)≥TR(0)−TC(0)≥0−TVC(0)−TFC≥TVC(Q∗)​所以有 P≥AVC(Q∗) \\boxed{P\\ge AVC(Q^\\ast)} P≥AVC(Q∗)​ 因此从短期来看，企业是否会退出市场，取决于市场价格（会影响收入）和自身生产的平均可变成本 (Average Variable Cost, AVC)。此时的利润为 π(Q∗)=(P−AC)×Q∗ \\boxed{\\pi(Q^\\ast)=\\Big( P-\\textcolor{red}{AC} \\Big)\\times Q^\\ast} π(Q∗)=(P−AC)×Q∗​ 注意！ 计算利润时要注意包含 Fixed Cost。只有在做决策时才不计算 FC. 曲线的性质 MC 曲线总是和 AVC 曲线交于 AVC 曲线的最低点。 证明 略。 进一步的，我们可以推断，企业短期内的生产曲线 Supply Curve 由 MC Curve 和 Shutdown Decision 共同决定。 Supply Curve 长期行为 对于长期而言，Fixed Cost 此时也应该算入（例如办公楼续约费可以视为支出）。类似的，应有 P≥AC(Q∗) \\boxed{P\\ge AC(Q^\\ast)} P≥AC(Q∗)​当且仅当市场价格高于平均成本（含固定成本），企业才会考虑进入市场（不然无法收回固定成本）。 合并来看，Supply Curve 差不多长这样： Aggregate Supply Curve 例题 2020 Spring Final 题目 There are many identical firms (having the same production costs and producing the same product) in a competitive market. Their product is assumed to be perfectly divisible. When one firm produces qqq units, its marginal cost is MC(q)=qMC(q) = qMC(q)=qand the corresponding total variable cost is TVC(q)=0.5q2TVC(q) = 0.5q^2TVC(q)=0.5q2The demand curve in the market is Q=9000−90PQ = 9000 - 90PQ=9000−90PMoreover, assume that the industry is constant cost. The market is currently in a long-run equilibrium, and the equilibrium price is $45\\$45$45 per unit. Using the information, we conclude that there are [ Answer36 ] firms in the market. Using this information, we conclude that the fixed cost of an individual firm is [ Answer37 ] dollars. Using this information, we conclude that the short-run supply curve of this industry is of the form Q=A+B×PQ = A + B \\times PQ=A+B×Pwhere AAA is equal to [ Answer38A ], and BBB is equal to [ Answer38B ] Suppose now the market demand has become Q=15300−153PQ = 15300 - 153PQ=15300−153PUsing the information, we can calculate that in the long run, an individual firm will produce [ Answer39 ] units. 解答 Question Bank Q1 Determine whether the following statements about a perfectly competitive market are TRUE OR FALSE. Statement 1: The demand curve facing the market is horizontal. Statement 2: Entry of new firms will drive the accounting profits of the existing firms to zero in the long run. 【解答】False, False 【解说】对于 Statement 1，完全竞争市场的 Demand Curve 应该是 Downward Sloping 的。陈述错误。 对于 Statement 2，长期来看，完全竞争市场下，企业的 Economic profit 会趋于 000，但是 Accounting profit 相比 Economic profit 无须考虑 implicit cost，因此即使 economic profit 为 000，accounting profit 依旧可能 0\\gt 00。因此陈述错误。 Q2 Determine whether the following statements are TRUE OR FALSE. Statement 1: Competitive firms can still earn positive accounting profits in the long run. Statement 2: A higher total cost to a firm must imply a lower level of output. 【解答】True, False 对 Statement 1 的解说见 Q1 statement 2 对于 statement 2，因为 Total cost=Variable Cost+Fixed Cost \\text{Total cost}=\\text{Variable Cost}+\\text{Fixed Cost} Total cost=Variable Cost+Fixed Cost 如果 Fixed cost 增加，那么短期内企业仍然会维持产量。因此陈述错误。 Q3 Suppose the government reduces a commercial registration fee for all firms in a competitive industry. Evaluate whether the following statements are TRUE or FALSE. Statement 1: A typical firm of the industry will raise its output in the short run. Statement 2: A typical firm of the industry will see an increase in producer surplus in the short run. 题目减少的是 fixed cost，对于 short run 没有影响。 生产者剩余=总收益−可变成本 \\text{生产者剩余}=\\text{总收益}-\\text{可变成本} 生产者剩余=总收益−可变成本 Q4 The government invested $10 million in an infrastructure project two years ago. The amount of money was spent on wages and material costs, which cannot be recovered. This year, the government realizes that she needs to invest another $4 million in the project because of construction delays, while the $11 million expected revenue remains unchanged and can only be generated upon the completion of the project. The government should ________ the additional budget because ________. Select one: a. approve; the project can generate positive revenue. b. approve; marginal benefit of the extra payment is greater than its marginal cost. c. disapprove; the cost is higher than expected d. disapprove; the government will suffer from a $3 million loss. 【解答】B 【解说】A 为什么错误呢？分析应该基于边际成本与收益。 Q5 There are 100100100 firms in a perfectly competitive decreasing cost industry at the long-run equilibrium. A typical firm faces the following cost curves (qqq is firm quantity, nnn is number of firms): Average Variable Cost ($): AVC=q+50AVC=q+50AVC=q+50 Marginal Cost ($): MC=2q+50MC=2q+50MC=2q+50 Fixed Cost ($): FC=810000/nFC=810000/nFC=810000/n What are the market price and market quantity? Select one: a. $230\\$230$230, 900090009000 units b. $15.5\\$15.5$15.5, 810081008100 units c. $155\\$155$155, 818181 units d. $230\\$230$230, 909090 units 长期平衡的话，考虑 TC=TVC+TFCTC=TVC+TFCTC=TVC+TFC，AC=AVC+AFCAC=AVC+AFCAC=AVC+AFC。这里，单个企业的 Total Fixed Cost 为 8.1×105100=8100\\frac{8.1\\times 10^5}{100}=81001008.1×105​=8100，因此平均固定成本为 AFC=8100qAFC=\\frac{8100}{q}AFC=q8100​，因此平均总成本为 AC=AVC+AFC=q+50+8100q AC=AVC+AFC=q+50+\\frac{8100}{q} AC=AVC+AFC=q+50+q8100​ 考虑市场价格，对于单个企业有 MC=ACMC=ACMC=AC，因此 q=90,p=230q=90,p=230q=90,p=230。 这里要求市场上总的产量，因此 Q=nq=9000Q=nq=9000Q=nq=9000 units. Q6 (2020 Spring Final Exam Q31-34) The market of fortune cookies is perfectly competitive. John’s Fortune Cookies is one of the many perfectly competitive firms. Assume that the only variable input required for the production is workers. The following table shows the relationship between the number of workers hired and the total output (packs of fortune cookies produced). # of workers Total output (packs) 1 98 2 183 3 264 4 330 5 391 6 441 7 482 8 517 9 536 10 552 John pays a fixed cost of $380 per day, and to each employee a wage of $54 per day. The price of fortune cookies is $1.14 per pack. Given such information, we can calculate the marginal product due to the 3-rd worker is [Answer1]. Given such information, in the short run, John should hire [Answer2] workers. Given such information, in the short run, John should [Answer3] packs of fortune cookies. Consequently, in the short run, John will make a profit of [Answer4] dollars. Suppose the fixed cost has become $570 per day. In the short run, John should hire [Answer5] workers. 【解答】 决定产量的时候不需要考虑 fixed cost，但是计算利润的时候还是要计算固定成本的。 Q7 (2020 Spring Final Exam Q36-39) There are many identical firms (having the same production costs and producing the same product) in a competitive market. Their product is assumed to be perfectly divisible. When one firm produces qqq units, its marginal cost is MC(q)=qMC(q) = qMC(q)=qand the corresponding total variable cost is TVC(q)=0.5q2TVC(q) = 0.5q^2TVC(q)=0.5q2The demand curve in the market is Q=9180−90PQ = 9180 - 90PQ=9180−90PMoreover, assume that the industry is constant cost. The market is currently in a long-run equilibrium, and the equilibrium price is $45 per unit. Using the information, we conclude that there are [Answer1] firms in the market. Using this information, we conclude that the fixed cost of an individual firm is [Answer2] dollars. Using this information, we conclude that the short-run supply curve of this industry is of the form Q=A+B×PQ = A + B \\times PQ=A+B×P AAA is equal to [Answer3]. BBB is equal to [Answer4]. Suppose now the market demand has become Q=15300−150PQ = 15300 - 150PQ=15300−150P Using the information, we can calculate that in the long run, an individual firm will produce [Answer5] units. 【解答】 对于单个企业而言，其 supply curve 就是 MC curve，也就是单个企业的 P−QP-QP−Q 曲线为 q=0+1Pq=0+1Pq=0+1P，但是这里题目要的是社会总产量 Q=nqQ=nqQ=nq，因此 Q=0+114PQ=0+114PQ=0+114P Q8 In a perfectly competitive constant cost industry, all firms are identical. A typical firm possesses the cost curves: TC=q2+80q+100TC=q^2+80q+100TC=q2+80q+100 MC=2q+80MC=2q+80MC=2q+80 (q is firm quantity). The market demand is P=1100−QP=1100-QP=1100−Q (Q is market quantity). The industry is initially at the long-run equilibrium. (a) A typical firm will shut-down and produce zero quantity in the short run if the price is lower than [Answer1] dollars. (Hint: P_shutdown = min AVC) A typical firm will exit from the industry in the long run if the price is lower than [Answer2] dollars. (Hint: P_exit = min AC, where MC=AC). (b) At initial long-run equilibrium, the price is [Answer3] dollars. (Hint: What is the relationship between long-run equilibrium price and firm exit price?) At initial long-run equilibrium, each firm produces [Answer4] units. © At initial long-run equilibrium, there are [Answer5] firms in the market. (Hint: number of firms = market quantity / firm quantity.) (d) Because of change in environmental regulation that affects production cost, the marginal cost is reduced by 17 dollars but the fixed cost is increased by 300 dollars permanently. The post-regulation short-run supply curve of this industry is: P=A+B×QP = A + B \\times QP=A+B×Qwhere A is equal to [Answer6]. (Hint: What will be the new MC curve?) Continue above, B is equal to [Answer7]. (e) At the post-regulation short-run equilibrium, the price is [Answer8] dollars. At the post-regulation short-run equilibrium, each hotel produces [Answer9] units. At the post-regulation short-run equilibrium, each firm earns a profit of [Answer10] dollars. (Hint: What will be the new TC curve?) (f) With free entry and exit of firms, at the post-regulation long-run equilibrium, the price will be [Answer11] dollars. At the post-regulation long-run equilibrium, there will be [Answer12] firms in the market. (h) Suppose the government wants to induce the market to generate a post-regulation long-run equilibrium quantity the same as the pre-regulation long-run equilibrium quantity. The government should impose per-unit subsidy of [Answer13] dollars. 【解答】 分析 MCMCMC 的表达式，发现当 shutdown 即 q=0q=0q=0 时，对应的价格为 808080，也就是说当价格低于 808080 企业就会选择不生产。因此短期 shutdown price 为 808080. exit price 的话只需要列出方程即可，即 AVC=MCAVC=MCAVC=MC q2+80q+100q=2q+80 \\frac{q^2+80q+100}{q}=2q+80 qq2+80q+100​=2q+80 解得 q=10q=10q=10，所以 exit price 为 2q+80=1002q+80=1002q+80=100. 长期平衡价格等于企业 exit price 上面刚刚解出来的 q=10q=10q=10 代入 Demand curve 可知，市场总产量为 Q=1000Q=1000Q=1000，因此，总共有 n=Qq=100n=\\frac{Q}{q}=100n=qQ​=100 家企业 (7.) Marginal Cost 降低 171717 而 fixed cost 增加 300300300，于是，MCMCMC 和 TCTCTC 变成了 MC=2q+63TC=q2+63q+400 MC=2q+63\\\\ TC=q^2+63q+400 MC=2q+63TC=q2+63q+400 从 P=MC=2q+63P=MC=2q+63P=MC=2q+63 和 q=Qnq=\\frac{Q}{n}q=nQ​，可知 P=0.02Q+63P=0.02Q+63P=0.02Q+63，所以 A=63A=63A=63 B=0.02B=0.02B=0.02 联立 P=0.02Q+63P=0.02Q+63P=0.02Q+63 和 P=1100−QP=1100-QP=1100−Q，可知 Q=1017.67,P=83.33Q=1017.67,P=83.33Q=1017.67,P=83.33 由于 Q=1017.67Q=1017.67Q=1017.67，由于市场刚刚变化，因此此时还不会有企业退出，所以 q=Q/n=10.17q=Q/n=10.17q=Q/n=10.17 把 q=10.17q=10.17q=10.17 代入，Profit=Pq−TC=−296.67\\text{Profit}=Pq-TC=-296.67Profit=Pq−TC=−296.67 h. 最后可以分析出来原来的长期市场价格为 100100100 而当前为 103103103，在 Demand Curve 上我们可以发现，要想让 Q′QQ′ 恢复到 QQQ 的水平，长期均衡的价格必须是 100100100，但是企业现在的 ACACAC 最低点是 103103103，需要降到 100100100. 因此需要补贴 103−100=3103-100=3103−100=3 块钱。"},{"title":"Elasticity 弹性","path":"/wiki/microecon/elasticity.html","content":"弹性 量化某个变量随着另一个变量的变化而变化的程度 弹性大：因变量对自变量的变化很敏感 弹性小：因变量对自变量的变化不怎么敏感 Price Elasticity of Demand 量化 PED=percentage change in Quantity demandedpercentage change in Price=%ΔQd%ΔP PED=\\frac{\\textbf{percentage}\\text{ change in Quantity demanded}} {\\textbf{percentage}\\text{ change in Price}}=\\boxed{\\frac{\\%\\Delta Q^d}{\\%\\Delta P}} PED=percentage change in Pricepercentage change in Quantity demanded​=%ΔP%ΔQd​​ 两点 PED 计算公式 我们用中点代为计算 Percentage Change Percentage=New−Old(New+Old)/2 \\text{Percentage}=\\frac{\\text{New}-\\text{Old}}{(\\text{New+Old})/2} Percentage=(New+Old)/2New−Old​那么 PEDPEDPED 的计算公式可以改写成 PED=Qnewd−QolddPnew−Pold×Pnew+PoldQnewd+Qoldd \\boxed{ PED=\\frac{Q^d_{new}-Q^d_{old}}{P_{new}-P_{old}}\\times \\frac{P_{new}+P_{old}}{Q^d_{new}+Q^d_{old}} } PED=Pnew​−Pold​Qnewd​−Qoldd​​×Qnewd​+Qoldd​Pnew​+Pold​​​当某个点 oldoldold 已经被固定了的时候，考虑其差值 ΔQ→0\\Delta Q\\to 0ΔQ→0，就有 PED=ΔQdΔP×2Pold+ΔP2Qoldd+ΔQd→ΔQdΔP×PoldQoldd→PQd×1slope \\begin{aligned} PED=\\frac{\\Delta Q^d}{\\Delta P}\\times \\frac{2P_{old}+\\Delta P}{2Q^d_{old}+\\Delta Q^d}\\\\ \\to \\frac{\\Delta Q^d}{\\Delta P}\\times\\frac{P_{old}}{Q^d_{old}}\\\\ \\to \\boxed{\\frac{P}{Q^d}\\times \\frac{1}{\\text{slope}}} \\end{aligned} PED​=ΔPΔQd​×2Qoldd​+ΔQd2Pold​+ΔP​→ΔPΔQd​×Qoldd​Pold​​→QdP​×slope1​​​ Observation Price Elasticity 随着点在 Quantity of Demand 曲线上的移动而变化；并且在中点处为 −1-1−1，往上 −1\\lt -1−1，往下 −1\\gt -1−1 如果两条 QdQ^dQd 曲线有交点，那么更加平缓的那条直线在这个点上的弹性更大。 Elasticity 与 Revenue 收入基本公式 Revenue=Quantity×Price \\text{Revenue}=\\text{Quantity}\\times\\text{Price} Revenue=Quantity×Price因此考虑 Elasticity 的话，Revenue 是关于 Price 的二次函数，并且在 PED=−1PED=-1PED=−1 的时候，取到最大值 弹性需求（∣η∣1∣\\eta∣1∣η∣1）：降价增加总收益（需求量增幅 价格降幅）。 非弹性需求（∣η∣1∣\\eta∣1∣η∣1）：降价减少总收益（需求量增幅 价格降幅）。 单位弹性（∣η∣=1∣\\eta∣=1∣η∣=1）：总收益最大。 也可以在 QdQ^dQd 直线上直观地进行比较：找到点变化前后对应的矩形变化面积。更一般的，如果点在中点上方，则总收益一定增加；在下方则总收益减少。 Constant Elasticity 如果一条曲线在每一个点的 PPP Elasticity of QQQ 都相等为 −k-k−k，那么其曲线可以表示为 f(P,Q):PkQ=C f(P,Q):\\boxed{P^{\\textcolor{red}{k}}Q=C} f(P,Q):PkQ=C​ 证明（不考） 考虑 Q-P 曲线 fff 在这一个点的 Elasticity，用点斜式即为 Elasticity=−k=PQ×dQdP \\text{Elasticity}=-k=\\frac{P}{Q}\\times\\frac{dQ}{dP} Elasticity=−k=QP​×dPdQ​把 xdxx\\mathop{dx}xdx 放到一起： −kPdP=1QdQ -\\frac{k}{P}\\mathop{dP}=\\frac{1}{Q}\\mathop{dQ} −Pk​dP=Q1​dQ两边积分 −kln⁡P+CP=ln⁡Q+CQln⁡Q+kln⁡P=cPkQ=C \\begin{aligned} -k\\ln{P}+C_P=\\ln{Q}+C_Q\\\\ \\ln Q+k\\ln P=c\\\\ P^kQ=C \\end{aligned} −klnP+CP​lnQ+klnPPkQ​=lnQ+CQ​=c=C​ 左右取对数，曲线方程也可以写作 ln⁡Q=−kln⁡P+c \\boxed{ \\ln Q=\\textcolor{red}{-k} \\ln P+c } lnQ=−klnP+c​ 影响 Price Elasticity of Demand 的因素 Availability of Substitutes Time Horizon 产品有效期 Category of product (specific or broad) Necessities vs. Luxuries Purchase Size Substitutes Fewer substitutes makes it harder for consumers to adjust QQQ when PPP changes… so demand is more inelastic. Many substitutes? Switching brands when prices change is easy, so demand is more elastic. Time Horizon Category 另外两种 Elasticity Cross-Elasticity Exy=%ΔQd of X%ΔP of Y=PyQxd×ΔQxΔPy \\begin{aligned} E_{xy}=\\frac{\\%\\Delta Q^d \\text{ of X}}{\\%\\Delta P \\text{ of Y}}\\\\ =\\boxed{\\frac{P_y}{Q^d_x}\\times\\frac{\\Delta Q_x}{\\Delta P_y}} \\end{aligned} Exy​​=%ΔP of Y%ΔQd of X​=Qxd​Py​​×ΔPy​ΔQx​​​​Exy0E_{xy}0Exy​0 说明是 Substitute ；反之，说明是 complement Income Elasticity EI=%ΔQd%ΔIncome=IQx×ΔQxΔI \\begin{aligned} E_I=\\frac{\\%\\Delta Q^d}{\\%\\Delta \\text{Income}}\\\\ =\\boxed{\\frac{I}{Q_x}\\times\\frac{\\Delta Q_x}{\\Delta I}} \\end{aligned} EI​​=%ΔIncome%ΔQd​=Qx​I​×ΔIΔQx​​​​ EI1E_I\\gt 1EI​1 说明是 Luxury EI0E_I\\gt 0EI​0 说明是 Normal Goods EI0E_I\\lt 0EI​0 为 Inferior Goods Price Elasticity of Supply 类似的，也有中点公式和点斜公式 性质 PES0PES\\gt 0PES0 若截距 0\\gt 00，那么随着 QsQ^sQs 增加，PESPESPES 降低，但永远 1\\gt 11 若过原点，则 PES≡1PES\\equiv 1PES≡1 影响因素 Change in Per-Unit Costs with Increased Production Time Horizon Share of Market for Inputs Geographic Scope Elasticity and Quick Predictions 把基准点放在 Equilibrium Point，记 ηs\\eta_sηs​ 为 Price Elasticity of Supply，ηd\\eta_dηd​ 为 Price Elasticity of Demand，则有 % change in Price from a shift in Demand ΔQd=% change in Demand ΔQdηs+∣ηd∣% change in Price from a shift in Supply ΔQs=−% change in Supply ΔQsηs+∣ηd∣ \\begin{array}{rll} \\text{\\% change in \\textbf{Price} from a shift in \\textbf{Demand} }\\Delta Q^d\\\\ =\\frac{\\text{\\% change in \\textbf{Demand} }\\Delta Q^d}{\\eta_s+|\\eta_d|}\\\\ \\text{\\% change in \\textbf{Price} from a shift in \\textbf{Supply} }\\Delta Q^s \\\\ =\\textcolor{red}{-}\\frac{\\text{\\% change in \\textbf{Supply} }\\Delta Q^s}{\\eta_s+|\\eta_d|} \\end{array} % change in Price from a shift in Demand ΔQd=ηs​+∣ηd​∣% change in Demand ΔQd​% change in Price from a shift in Supply ΔQs=−ηs​+∣ηd​∣% change in Supply ΔQs​​ 例题 2024 Summer Suppose there is only an initial shift of demand or supply (but not both), and we observe that the equilibrium price of computers increases by 2.1%2.1\\%2.1%, and the equilibrium quantity increases by 8.1%8.1\\%8.1%, we conclude that: A) The price elasticity of demand is elastic. B) The price elasticity of demand is inelastic. C) The price elasticity of supply is elastic. D) The price elasticity of supply is inelastic. 解答 Price 和 Quantity 都上涨了，因此这一定是 Demand 增加引起的 (Demand Curve right shift)。 上涨的幅度由 Supply Curve 的斜率决定，这里 %ΔP=2.1%,%ΔQ=8.1%\\% \\Delta P=2.1\\%,\\% \\Delta Q=8.1\\%%ΔP=2.1%,%ΔQ=8.1%，所以 price elasitcity of supply 等于 Es=%ΔQ%ΔP1 E_s=\\frac{\\% \\Delta Q}{\\% \\Delta P}\\gt 1 Es​=%ΔP%ΔQ​1 由此，price elasticity of supply 是 elastic 的。"},{"title":"externalities","path":"/wiki/microecon/externalities.html","content":"Market Failure total surplus (both of the consumer and producer) is maximized in free markets. The market equilibrium price and quantity are socially optimal… (1) when all relevant production costs are incurred by sellers (2) when all relevant consumption benefits accrue to buyers. Sometimes costs or benefits that result from an activity accrue to people not directly involved in the activity Ex ternal cost = a cost paid by people other than the consumer or the producer trading in the market Social cost = the cost to everyone o Social cost = private cost + external cost Deadw eight Loss is the welfare loss because of quantity traded deviating from social optimal level"},{"title":"Monopoly 垄断","path":"/wiki/microecon/monopoly.html","content":"垄断定义 拥有定价权 价格不受 Demand Quantity 影响 Profit Maximizing Rule 依然遵循利益最大化原则，对于垄断企业来说，只需要考虑在 Demand Curve 上找到 Marginal Revenue =0=0=0 的那个点即可。 Marginal Revenue 对于离散的 Quantity，有 MR(Q)=TR(Q)−TR(Q−1) MR(Q)=TR(Q)-TR(Q-1) MR(Q)=TR(Q)−TR(Q−1) 对于连续的 Quantity，有 P(Q)=a−bQ ⟹ MR(Q)=a−2bQ P(Q)=a-bQ\\implies MR(Q)=a-2bQ P(Q)=a−bQ⟹MR(Q)=a−2bQ 证明 由于是连续性变量，考虑极小值 ΔQ\\Delta QΔQ 的 Marginal Revenue，则有 MR(Q)=lim⁡ΔQ→0(a−b(Q+ΔQ))(Q+ΔQ)−(a−bQ)QΔQ=a−2bQ \\begin{aligned} MR(Q)=\\lim_{\\Delta Q\\to 0}\\frac{\\Big(a-b(Q+\\Delta Q)\\Big)(Q+\\Delta Q)-(a-bQ)Q}{\\Delta Q}\\\\ =a-2bQ \\end{aligned} MR(Q)​=ΔQ→0lim​ΔQ(a−b(Q+ΔQ))(Q+ΔQ)−(a−bQ)Q​=a−2bQ​ Select Price 选定 Quantity 之后，根据 Demand Curve 计算 Price。 Price Elasticity of Demand 与 Monopoly Markup 完全竞争的市场里，无法有溢价，即必须满足 P=MCP=MCP=MC。但垄断市场里可以有溢价： P=ϵ1+ϵMCMarkup=P−MCMC=−11+ϵ P=\\frac{\\epsilon}{1+\\epsilon}MC\\\\ Markup=\\frac{P-MC}{MC}=\\frac{-1}{1+\\epsilon} P=1+ϵϵ​MCMarkup=MCP−MC​=1+ϵ−1​The More Inelastic the Demand Curve the More the Monopolist Raises Price Above Marginal Cost Welfare Analysis Producer Surplus FC=0 Monopoly Profit FC0 政府干预 垄断企业相比其他企业，由于生产规模更大，其平均生产成本也更低，此时一定会将新入行的企业（其平均生产成本更高）挤出市场。 若政府设置价格上限…… 如果设置在 MCMCMC，垄断企业会退出市场，因为 ACMCACMCACMC。所以如果要让垄断企业留在行业内，最低的上限必须设置为 ACACAC. the price that corresponding to the same quantity Q∗Q^\\astQ∗ depends on the demand. 例题 Q1 A monopolist possesses the cost curves: TC=Q2+80Q+90000TC = Q^2 + 80Q + 90000TC=Q2+80Q+90000, MC=2Q+80MC = 2Q + 80MC=2Q+80. The market demand is P=1100−QP = 1100 - QP=1100−Q. (a) To maximize profit, the monopolist charges [Answer1]. (b) Continue above, the deadweight loss is [Answer2]. © If the government imposes a price ceiling at $700. The monopoly will earn a profit of [Answer3]. (Hint: Be careful with where MR(Q)=MC(Q)MR(Q) = MC(Q)MR(Q)=MC(Q) in price ceiling case.) (d) Suppose the government instead imposes a price ceiling to completely eliminate deadweight loss in the short run. The monopolist will earn a profit of [Answer4]. (e) Ignore price ceiling. Suppose the marginal cost of production is increased by $20 per unit sold. The monopolist will earn a profit of [Answer5]. (Hint: What are the new TC and MC curves?) (f) Suppose instead only the fixed cost of production is increased by $20000. The monopolist will produce earn a profit of [Answer6]. (Hint: What are the new TC and MC curves?) (g) Suppose instead the government provides a 20 per-unit subsidy to the monopoly. The monopolist will earn a profit of $[Answer7]. (Hint: What are the new TC and MC curves?) (h) Suppose instead the government provides a 20 per-unit subsidy to consumers. The monopolist will earn a profit of $[Answer8]. (i) Suppose instead the government wants to completely eliminate deadweight loss. It can do so by providing per-unit subsidy of [Answer9][Answer9][Answer9] to consumers."},{"title":"Price Ceiling/Floor","path":"/wiki/microecon/price-ceiling-floor.html","content":"Price Ceiling 价格上限在 P−QP-QP−Q 图像上表示为一条水平线 Ceiling Line 黑色实线为坐标轴，纵轴 PPP，横轴为 QQQ 橙色线表示 P=F(Qs)P=F(Q^s)P=F(Qs) 黄色线表示 P=F(Qd)P=F(Q^d)P=F(Qd) 蓝色线表示 Price Ceiling， 注意！ 只有当蓝色低于 Equilibrium 点的时候才需要额外分析，如果价格上限高于市场价，那么对于市场完全没有影响 于是此时我们可以看到，需求量远远大于供给量，于是在供给量固定的情况下，市场上的买家需要争夺这些稀缺的供给……我们来看四种情况 Bribery 第一种解决办法，加价/拍卖。Total Value of bribery 会计入 Surplus 中 Waiting in Line Total Value of Time，会算作损失 例题 2024 Summer Suppose the demand and supply of Cannabis in Neverland community are given as follows: Demand: P=930−589QP = 930 - 589QP=930−589Q Supply: P=180+2.5QP = 180 + 2.5QP=180+2.5Q In which PPP is the price per kg in thousand dollars and QQQ is the quantity in thousand kgs. Note: 1 thousand×(1 thousand)=1 million1 \\text{ thousand} \\times (1 \\text{ thousand}) = 1 \\text{ million}1 thousand×(1 thousand)=1 million 18. The unregulated equilibrium Cannabis price is [ Answer18A ] thousand dollars per kg and the equilibrium quantity is [ Answer18B ] thousand kgs. 19. Suppose the government purchases 101010 thousand kgs of Cannabis back from the market regardless of price. After the government enters the market, on the new market demand curve, when the price is 555555555 thousand dollars per kg, the total quantity demanded is [ Answer19 ] thousand kgs. 20. Suppose the government wants to use a buyback program to reduce the quantity of Cannabis traded in the community to zero. The government will need to buy at least [ Answer20A ] thousand kgs from the market and spend at least [ Answer20B ] million dollars. 21. Suppose the government wants to use a price floor to reduce the quantity of Cannabis traded in the community to zero. The government will need to implement a price floor of [ Answer21A ] (A. at most; B. at least) [ Answer21B ] thousand dollars per kg. 22. Knowing that the government is determined to reduce the quantity of Cannabis traded in the community to zero through either buyback or price floor, the Cannabis suppliers are actively thinking of lobbying the government for a policy they favor. The Cannabis suppliers as a group are willing to spend up to [ Answer22 ] million dollars to lobby the government to adopt the policy they favor. 解答 这一小问的核心在于，生产商是从这两个政策里选出最利于自己（收益最大化）的政策，也就是贿赂政府选择 buyback 政策。而 buyback 政策下，Producer Surplus 为 0.5×(930−180)×300=1125000.5\\times (930-180)\\times 300=1125000.5×(930−180)×300=112500 million，因此最多可以支付 112500112500112500 来贿赂政府。"},{"title":"Price Discrimination 价格歧视","path":"/wiki/microecon/price-discrimination.html","content":"Single-Price Monopoly Single-Price Monopoly（单一价格垄断）是指垄断企业在同一市场中对所有消费者收取相同价格的行为 仅考虑企业自身的利润，不考虑 Social Surplus. 价格歧视 本质目的还是为了提升利润。 First Degree (Perfect) Price Discrimination PPD: 企业对每位消费者收取不同的价格，等于其保留价格，完全提取消费者剩余（Consumer Surplus），将其转化为企业利润。此时 DWL=0DWL=0DWL=0 Q↑ ⟹ MB↓ ⟹ P↓ Q\\uparrow \\implies MB\\downarrow \\implies P\\downarrow Q↑⟹MB↓⟹P↓ Secodn Degree Price Discrimination (using Hurdles) Discrimination using Hurdles（使用障碍进行价格歧视） 是一种通过设置特定门槛或障碍，将市场分割为不同子市场，并对各子市场实施差异化定价的策略。 并非 Social Efficient，但是比 Single-Price Monopoly 好很多 DWL 分析 DWL Third Degree PD 根据可观察特征（如年龄、地区）划分市场，对不同子市场设定不同价格. 例题"},{"title":"Public Goods","path":"/wiki/microecon/public-goods.html","content":"Excludability and Rivalry Excludability: 排他性 Non-excludable goods: Cannot exclude non-payers (e.g., national defense, radio signals). Excludable goods: Can exclude non-payers (e.g., jeans, paid e-books). Rivalry: 竞争性 Rival goods : Use by one person reduces availability for others. Non-rival goods : One person’s use does not diminish availability for others. Excludability Rivalry Type Examples Excludable Rival Private Goods Jeans, hamburgers, gasoline Excludable Non-rival Nonrival Private Wi-Fi, satellite TV Non-excludable Rival Common Resources Timber in public land, bluefin tuna Non-excludable Non-rival Public Goods National defense, lighthouses 四种商品类型 Private Goods 可以直接高效地进入竞争市场 因为 rival，所以 excludable 不导致 inefficiency 因为 excludable，所以有 incentive 去消费和生产 经典微观经济学的假设： 所有生产成本由生产者承担 所有消费收益由消费者享受 Public Goods Under-provision : Free-rider problem leads to insufficient supply. Collective Action : Difficult to negotiate joint purchases, especially with large groups. 需要政府介入 Taxes to buy public goods (e.g., streetlights, highways). Optimal quantity determined where Marginal Social Benefit (MSB) = Marginal Social Cost (MSC) . MSB curve: 分段加和 Non-rival Private Common Resources 私人决策的低效性 个人在做决策的时候，指针方针仅考虑私人的边际收益和私人的边际成本 MB(Qi)≥MC(Qi) MB(Q_i)\\ge MC(Q_i) MB(Qi​)≥MC(Qi​)然而从社会的角度看，社会成本 === 私人成本 +++ 外部成本，导致市场均衡产量 QmktQ_{mkt}Qmkt​ 远大于社会均衡产量 QsocQ_{soc}Qsoc​. 为防止 Tragedy of the Commons 需要政府干预： 核心思想 当前的 quantity 为 QmktQ_{mkt}Qmkt​，其满足 MB(Qmkt)=MC(Qmkt) MB(Q_{mkt})=MC(Q_{mkt}) MB(Qmkt​)=MC(Qmkt​)既然想让它达到 QsocQ_{soc}Qsoc​ 的水平，那么我们可以列出两个式子 {MB(Qsoc)=MC(Qsoc)MC(Qsoc)=MC(Qmkt)+Extra Cost \\begin{cases} MB(Q_{soc})=MC(Q_{soc})\\\\ MC(Q_{soc})=MC(Q_{mkt})+\\text{Extra Cost} \\end{cases} {MB(Qsoc​)=MC(Qsoc​)MC(Qsoc​)=MC(Qmkt​)+Extra Cost​那么我们只需要限制 Extra Cost\\text{Extra Cost}Extra Cost 的范围即可。 通过税收干预 通过 Tradable Permit 干预 例题 2020 Fall Final Q1 Please refer to the background information below to answer the following three questions. Factory X produces 101010 tons of wastewater every day. Each ton of wastewater caused 17.517.517.5 (in thousand dollars) worth of damage to nearby residents. It costs the factory n2n^2n2 (in thousand dollars) to remove pollutants from nnn tons of wastewater. Suppose nnn can only take integer numbers. If pollution is unregulated and negotiation is impossible, Factory X will discharge [ Answer38 ] tons of untreated wastewater to the sea every day. It is socially efficient for Factory X to treat [ Answer39 ] tons of wastewater. Consider levying a tax on wastewater. In order to achieve the socially efficient outcome, the minimum amount of tax should be [ Answer40 ] thousand dollars per ton of wastewater. 【解答】 我们不妨 38. 由于工厂并不对环境污染负责，所以其 cost 仅由废水处理构成 arg min⁡xx2\\argmin_x x^2argminx​x2，所以 x=0x=0x=0，即不处理任何污水 考虑社会环境效益，假设工厂处理 xxx 吨废水，则社会的成本由废水处理和废水污染构成，即 arg min⁡xx2+17.5(10−x)\\argmin_x x^2+17.5(10-x)argminx​x2+17.5(10−x)，因为 x∈Zx\\in \\Zx∈Z 故 x=9x=9x=9 仅考虑工厂的成本，工厂的成本由废水处理和税构成，假设处理 xxx 吨废水其余排放，每吨废水收税 ppp，则成本为 arg min⁡xx2+p(10−x)\\argmin_x x^2+p(10-x)argminx​x2+p(10−x)，现在要求达到社会最大效益，即这个二次函数的最小值在 x=9x=9x=9 取到。所以解得 p≥17p\\ge 17p≥17 Q2 Please refer to the background information below to answer the following three questions. There is a small public beach in Utopia. The residents of Utopia, in total 20 of them, love to go to the beach but prefer not to when it is too crowded. In particular, if n people share the beach together, each individual gets an economic surplus of 10.5 − n dollars. Suppose n can only take integer numbers. If the residents make their decisions individually, then [ ] residents will go to the beach in equilibrium. The socially optimal number of beach occupants is [ ]. If the government charges an entrance fee of $2.62, [ ] residents will go to the beach in equilibrium."},{"title":"Supply Demand","path":"/wiki/microecon/supply-demand.html","content":"Demand Curve Normal good: When we have more income, we choose to buy more of the good. Inferior good: When we have more income, we choose to buy less of the good. Combination of Demand Curves Qtotald=Q1d+Q2d Q^d_{total}=Q^d_{1}+Q^d_2 Qtotald​=Q1d​+Q2d​ Supply Curve Horizontally: How many suppliers are willing and able to sell at a certain price. Vertically: The minimum price for which suppliers are willing to sell a certain quantity. Combination of Supply Curves Qtotals=Q1s+Q2s Q^s_{total}=Q^s_{1}+Q^s_2 Qtotals​=Q1s​+Q2s​ 计算 Equilibrium: Supply Curve 与 Demand Curve 的交点 Economic Surplus 这个 Surplus 可以这样理解：如果我预期 100100100 元买下，而我实际只花了 606060，那么其实我会觉得我赚了 100−60=40100-60=40100−60=40。 而在 Equilibrium 的情况下，交易价为 Equi Price，在 Demand Curve 上不同预期价（Price，纵坐标）有对应的人数（Quantity，横坐标，实际上应该是 ΔQ\\Delta QΔQ），因此对于这个预期价而言，他们获得的“赚了”感是 P×ΔQP\\times \\Delta QP×ΔQ 因此在下图的公式里，所有的 Surplus 是一个三角形 Total Economic Surplus=Consumer Surplus+Producer Surplus \\text{Total Economic Surplus}=\\text{Consumer Surplus}+\\text{Producer Surplus} Total Economic Surplus=Consumer Surplus+Producer Surplus如果 Economic Surplus 0\\lt 00 那么交易就不会发生 红色部分就是 Total Economic Surplus 分别计算 Consumer 和 Producer 的 Surplus"},{"title":"Tax and Subsidy","path":"/wiki/microecon/tax-subsidy.html","content":"本章最核心的理论是，无论税收或者补贴，假设为 TTT 元，那么当达到新的平衡点时，消费者支付价格与生产者成本价格之间的差值必为 TTT 元 Pd−Ps=T \\boxed{P^d-P^s=T} Pd−Ps=T​Tax Subsidy"},{"title":"Trading 交易与分工","path":"/wiki/microecon/trading.html","content":"Unit Requirement Table 与 Unit Productivity Table Requirement 和 Productivity Table 最重要的区别就是：前者给出生产一个物品需要的资源，后者给出在限定资源的情况下能生产多少物品。 有一个简单的转化： 1Requirement=Productivity \\frac{1}{\\text{Requirement}}=\\text{Productivity} Requirement1​=Productivity Opportunity Cost Opportunity Cost Copp()C_{opp}()Copp​() 描述某个人在生产某件物品的时候，能够生产多少的其他物品；直观理解就是这个人生产这件物品有多 efficient 重要公式 Copp(A)=Time of ATime of B=Productivity of BProductivity of A C_{opp}(A)=\\frac{\\text{Time of }A}{\\text{Time of }B} =\\frac{\\text{Productivity of }B}{\\text{Productivity of }A} Copp​(A)=Time of BTime of A​=Productivity of AProductivity of B​ 这里的 Quantity 是在一段长度确定的时间内的。并且可以注意到 Copp(A)=1Copp(B)C_{opp}(A)=\\frac{1}{C_{opp}(B)}Copp​(A)=Copp​(B)1​ 如果对于两个人 X,YX,YX,Y，如果 Copp,X(A)Copp,Y(A)C_{opp,X}(A)\\lt C_{opp,Y}(A)Copp,X​(A)Copp,Y​(A)，即 XXX 在 AAA 上的 Opportunity Cost 更小，我们称 XXX 在 AAA 上有 Comparative Advantage. Specialization 分工 一个经济体里肯定会有分工，理性经济体里的分工由 Opportunity Cost 的大小来决定：让 Copp(A)C_{opp}(A)Copp​(A) 最小的人来负责这件 AAA （总是让最高效的人来处理这件事） 分工的存在，也可以让经济体达到 1+121+1\\gt 21+12 的效果。 Term of Trade (TOT) TOTTOTTOT 描述交易时的换算比例（例如 1.11.11.1 Tea/Cake 说明 111 个蛋糕能交易 1.11.11.1 包茶） 因为交易的双方都需要从交易中获利（否则根本不会进行交易），此时 Term of Trade 叫需要满足一些条件，使得双方都能获利。这里的获利的意思是，我从你这里买东西比我自己生产这个东西要好（你更加熟练，需要的资源更少）。 一个重要的公式就是，对于交易的双方，TOTAperBTOT_{A\\mathop{per}B}TOTAperB​（表示 1 unit B=TOT unit A\\text{\\(1\\) unit \\(B=TOT\\) unit \\(A\\)}1 unit B=TOT unit A）应该满足 Copp,X(A)≤TOTAperB≤Copp,Y(A) C_{opp,X}(A)\\le TOT_{A\\mathop{per}B}\\le C_{opp,Y}(A) Copp,X​(A)≤TOTAperB​≤Copp,Y​(A) 证明 我们假设经济体只生产 A,BA,BA,B 两件物品，且生产 AAA 的 XXX 与生产 BBB 的 YYY 进行交易。那么我们首先知道，根据分工，有 Copp,X(A)Copp,Y(A)Copp,X(B)Copp,Y(B) C_{opp,X}(A)\\lt C_{opp,Y}(A)\\\\ C_{opp,X}(B)\\gt C_{opp,Y}(B) Copp,X​(A)Copp,Y​(A)Copp,X​(B)Copp,Y​(B)交易双方判断能否获利的准则是： （Requirement）生产相同数量时，相比自己生产，能否获得资源上的节约？ （Quantity）拥有相同数量资源时，相比自己生产，能否获得产品数量上的提升？ 现在假设 TOT 的计算是 111 个单位 BBB 能交易 TOTTOTTOT 单位的 AAA（那么其单位就是 A/BA/BA/B），用资源的节省量推导。 那么，对 XXX 而言，他生产的是 AAA，购买的是 BBB，那么他生产的 111 个单位的 AAA 能换来 1TOT\\frac{1}{TOT}TOT1​ 的 BBB，理论应该节约 RX(B)−1TOTRX(A)≥0 ⟺ TOT≥RX(A)RX(B)=Copp,X(A) \\begin{aligned} R_{X}(B)-\\frac{1}{TOT}R_X(A)\\ge 0\\\\ \\iff TOT\\ge \\frac{R_X(A)}{R_X(B)}=C_{opp,X}(A) \\end{aligned} ⟺​RX​(B)−TOT1​RX​(A)≥0TOT≥RX​(B)RX​(A)​=Copp,X​(A)​同理，对 YYY 而言，他生产的每单位 BBB 能换 TOTTOTTOT 单位的 AAA，理论上，生产 AAA 可以节约 RY(A)−TOT×RY(B)≥0 ⟺ TOT≤RY(A)RY(B)=Copp,Y(A) \\begin{aligned} R_Y(A)-TOT\\times R_Y(B)\\ge 0\\\\ \\iffTOT\\le\\frac{ R_Y(A)}{R_Y(B)}=C_{opp,Y}(A) \\end{aligned} ⟺​RY​(A)−TOT×RY​(B)≥0TOT≤RY​(B)RY​(A)​=Copp,Y​(A)​所以，对于交易的双方，TOTAperBTOT_{A\\mathop{per}B}TOTAperB​（表示 1 unit B=TOT unit A\\text{\\(1\\) unit \\(B=TOT\\) unit \\(A\\)}1 unit B=TOT unit A）应该满足 Copp,X(A)≤TOTAperB≤Copp,Y(A) C_{opp,X}(A)\\le TOT_{A\\mathop{per}B}\\le C_{opp,Y}(A) Copp,X​(A)≤TOTAperB​≤Copp,Y​(A) (PPC) Production Possibility Curve PPC 上的一条直线 我们假设纵轴表示 AAA 的生产，横轴表示 BBB 的生产，那么截距表示全力生产某一样物品的情况下，该物品的产量。 我们来考察这条直线的斜率 kkk，则有 k=−Productivity of AProductivity of B=−Copp(B)=−1Copp(A) \\begin{aligned} k=-\\frac{\\text{Productivity of }A}{\\text{Productivity of }B}\\\\ =-C_{opp}(B)\\\\ =-\\frac{1}{C_{opp}(A)} \\end{aligned} k​=−Productivity of BProductivity of A​=−Copp​(B)=−Copp​(A)1​​我们更关心 ∣k∣|k|∣k∣，这个绝对值的意义更加鲜明：多生产 111 个单位的 BBB 的 Opportunity Cost 为 ∣k∣|k|∣k∣ 的单位的 AAA.，而 ∣k∣=Copp(B)|k|=C_{opp}(B)∣k∣=Copp​(B) 多条直线：分工 Low-Hanging-Fruit Principle 这个原理描述一个经济体内多人合作分工时，若要扩大生产，一定先让 Lowest Opportunity Cost 的人去做（因为最高效） 如果扩大的是 BBB 的生产（横轴），那么从左向右斜率的绝对值越来越大，越来越陡峭；图像呈现向上凸。 如果扩大的是 AAA 的生产（纵轴），那么从下往上直线的斜率的绝对值越来越小，越来越平缓（因为 Copp(A)C_{opp}(A)Copp​(A) 与 Copp(B)C_{opp}(B)Copp​(B) 成反比）；不过图像仍是上凸的。 直线的相交位置：(Bi−1,Ai)(B_{i-1},A_i)(Bi−1​,Ai​) 影响 PPC 的因素 资源增多 科技进步 总结 通常来说，如果交易双方的 Copp()C_{opp}()Copp​() 差距越大，那么双方交易带来的资源节省和产能提升也会越大。 (CPC) Consumption Probalitity Curve Closed Economy: 无开放贸易 在无开放贸易的情况下，一个经济体的 CPC 和 PPC 是重合的。因为除了这几个人没有人需要生产的物品，因此这些人生产出来的东西只能被自己消耗。 有浪费会趋于减产，有不够会趋于增产，最终都会回归到 PPC 上，因此 CPC 与 PPC 重合。 Open Economy and Open Trade 我们可以从几个角度来看 Open Trade 对 Production 和 Consumption 的影响，然后来看一看相关的计算。 以下假设假设贸易市场上 AAA 的价格为 aaa，BBB 的价格为 bbb，假设 TOTTOTTOT 用 AperBA\\mathop{per} BAperB 计算，此时对于贸易市场来说，TOT=TOTAperB=abTOT=TOT_{A\\mathop{per}B}=\\frac{a}{b}TOT=TOTAperB​=ba​ 例子：如何生产使得收益最大化 贸易市场的价格可以用一根斜率确定、截距不定的直线在 PPC 图像（纵轴为 AAA，横轴为 BBB）上表示出来，这条直线的斜率就是 −TOT-TOT−TOT 为了让利益最大化，我们平移这条直线，让他和 PPC 产生交点，对于每一个交点计算收益，取最大值即可。 从交点倒推 TOT 和市场价格 一个很 tricky 的点是，图像上 PPC 的斜率是 −Copp(B)-C_{opp}(B)−Copp​(B)，但市场的直线的斜率是 −TOTAperB-TOT_{A\\mathop{per}B}−TOTAperB​。记得取倒数。 最大化组合消费 通常会问，若消费 nnn 单位的 AAA，那么最多能消费多少 BBB？ 我们把这个过程转化为，X,YX,YX,Y 两人先生产，通过贸易市场换成钱，再用钱在市场上买所需的物品。这里不考虑成“生产后的东西先拿出一部分满足消费”，是因为这两个思路是等价的 计算：在这个市场下，通过交易最多能赚多少钱 通过计算 TOTTOTTOT，判断出每一个应该生产什么（贸易市场的 TOTAperBTOT_{A\\mathbb{per}B}TOTAperB​ 更大，则生产 AAA；否则生产 BBB） 把生产出来的东西卖成钱 先购买需要消费的东西 然后就能计算最多能买多少了"},{"title":"PCA","path":"/wiki/ml/PCA.html","content":"PCA 主成分分析 PCA 是一种无监督的线性降维方法，它的核心思想是通过线性变换将原始的高维数据投影到一个新的低维空间中，使得投影后的数据在新的坐标轴上的方差最大。这样可以保留数据中最主要的特征信息，同时去除噪声和冗余信息 数学原理 既然需要保留最多的方差，那么首先我们要计算出数据集的协方差矩阵 C[i,j]=Cov(xi,xj)C[i,j]=Cov(x_i,x_j)C[i,j]=Cov(xi​,xj​)"},{"title":"RANSAC 线性回归算法","path":"/wiki/ml/RANSAC.html","content":"RANSAC 线性回归算法 用于解决普通线性回归算法里对 outlier 敏感的缺点 随机挑选出 mmm 个点 用这 mmm 个点拟合一个线性模型 对剩下的所有点，计算模型误差，筛选出在 tolerance ϵ\\epsilonϵ 内的点 用新挑选出来的点对模型进行重新拟合（例如最小二乘法） 最后选择性能最好的模型"},{"title":"Bagging","path":"/wiki/ml/bagging.html","content":"Bagging Algorithm 核心思想：对原数据集采样多次（可以有漏，可以有重）分别用于训练"},{"title":"Bisecting K-Means","path":"/wiki/ml/bisecting-k-means.html","content":"Bisecting K-Means"},{"title":"Cluster Analysis","path":"/wiki/ml/clustering.html","content":"Cluster 分类 Cluster Type Description Well-Separated 簇间完全分离，无重叠。 Center-Based 每个簇由中心点（如均值、中位数）定义，数据点靠近所属簇中心。 Contiguity-Based 基于空间邻近性，形成连通区域。 Density-Based 高密度区域形成簇，低密度区域分隔簇（如DBSCAN）。 Objective Function 通过优化目标函数（如最小化误差平方和）划分簇。 Partitional Clustering Hierarchical Clustering 聚合式 (Agglomerative): 自底向上，初始每个点为独立簇，逐步合并最近簇。 分裂式 (Divisive): 自顶向下，初始一个簇，逐步分裂为更小簇。 Agglomerative 使用 Distance/Similarity/Proximity Matrix Space: O(n2)O(n^2)O(n2) Time: O(n3)O(n^3)O(n3), O(n2log⁡n)O(n^2\\log n)O(n2logn) 计算 Proximity Matrix 循环，直到只剩一个 cluster 合并两个最近的 cluster 更新 Proximity Matrix 如何更新 Proximity Matrix? MIN: 簇间距离取最近点距离，易处理非球形簇但易受噪声影响。 MAX: 取最远点距离，抗噪但易分裂大簇. Biased towards globular clusters Group Avg: 取所有点对的平均距离，平衡抗噪与形状适应性。Biased towards globular clusters Distance between Centroids Ward’s Method: 最小化簇内误差平方和 Divisive: 最小生成树 构建 MST 计算点对之间的距离矩阵 生成 Proximity Graph 的 MST 分裂 MST 从 MST 中选出一条边，断开。 形成的两个连通块对应两个不同的 cluster"},{"title":"Evaluating Regression","path":"/wiki/ml/eval-regression.html","content":"SSE R2 R2=1−SSESST R^2=1-\\frac{SSE}{SST} R2=1−SSTSSE​"},{"title":"K-Means","path":"/wiki/ml/k-means.html","content":"K-Means Partitional Clustering 每一个 cluster 与一个 centroid 相联系，每一个点分配到其最近的 centroid KKK 是超参数 算法流程 选定初始的 KKK 个点作为 centroid 循环，直到 KKK 个 centroid 不再发生变化 把所有点分配到这 KKK 个 cluster 重新计算 KKK 个 centroid O(n×K×I×d)O(n×K×I×d)O(n×K×I×d)，其中 nnn 为数据量，KKK 为簇数，III 为迭代次数，ddd 为特征数。 我们用平均距离衡量 K-Means 算法的优劣： SSE=∑i=1K∑x∈Cidist2(x,mi) SSE=\\sum_{i=1}^K \\sum_{x \\in C_i} \\text{dist}^2(x, m_i) SSE=i=1∑K​x∈Ci​∑​dist2(x,mi​) 局限性 对初始中心的选取很敏感 需要预先指定 KKK 可能出现空聚类 针对这些问题，我们可以作出一些改进： 多运行几次算法，增加找到最优解的概率 用 Hierarchical Clustering 优化初始点的选取 K-Means 算出多于 KKK 个 centroid，再从其中选出 KKK 个 后处理 Bisecting K-Means 空聚类的处理 将距离所有簇中心最远的样本点选为空簇的新中心。该点对当前聚类的误差（SSE）贡献最大，重新分配它有助于优化整体聚类效果。 从SSE（误差平方和）最高的非空簇中选择一个点，将其作为空簇的新中心。该簇的误差较大，说明其内部数据分布分散，分裂或调整其中心可能改善聚类质量。 If there are several empty clusters, the above can be repeated several times. 增量法更新 不会产生空簇 前、后处理 前处理： Normalization 去除异常值 后处理： 去除小聚类（可能由异常值组成） 分裂稀疏、分散的聚类 合并close, 紧密的聚类 使用 ISODATA"},{"title":"Particle Filter 粒子滤波","path":"/wiki/ml/particle-filter.html","content":"Particle Filter: Overview 现实世界里，xxx 的维度太大、数量太多，计算的时间复杂度爆炸。我们很难精确计算出 P(X)P(X)P(X) 的 closed form，一个比较经典的方法就是利用蒙特卡罗方法，化连续为离散，用若干个点近似 P(X)P(X)P(X)，这就是粒子滤波的思想。 我们通过对这些点进行追踪，从而得到大致的分布。粒子的平均值代表对 state 的近似，粒子的分布代表对 state distribution 的近似 HMM view of PF PF 对于粒子滤波而言有这么几个东西比较重要： 粒子滤波算法流程 粒子滤波的流程大致可以分为这么几步 获得 observation oto_tot​，得到每一个粒子对真实 state 的近似程度 对粒子进行重采样 (resample)，越近似的粒子比重越大。重采样将近似程度低的粒子替换为近似程度高的粒子 sample：对每一个粒子进行状态转移，即 xt+1=sample(P(Xt+1∣Xt=xt))x_{t+1}=\\text{sample}(P(X_{t+1}|X_t=x_t))xt+1​=sample(P(Xt+1​∣Xt​=xt​))"},{"title":"Plurality Majority Voting","path":"/wiki/ml/plurality-majority.html","content":"Majority Voting 多个模型分别输出预测结果，然后取投票最多的那个标签作为最后的输出。 Voting 用数学语言描述就是 y^=model{Ci(x)},1≤i≤m \\hat y=model\\Big\\{ C_i(x) \\Big\\},1\\le i\\le m y^​=model{Ci​(x)},1≤i≤m其中 CiC_iCi​ 表示训练的第 iii 个 Classifier 多个模型组合带来准确率提升 考虑训练了 2n+12n+12n+1 个分类器，每一个分类器的准确率为 rrr，那么组合后，由于需要超过半数投票，因此正确分类的概率为 ∑k=n+12n+1(kn)rk(1−r)2n+1−k \\sum_{k=n+1}^{2n+1} \\binom{k}{n}r^k(1-r)^{2n+1-k} k=n+1∑2n+1​(nk​)rk(1−r)2n+1−k当 n=5,r=0.7n=5,r=0.7n=5,r=0.7 时，这个值约为 0.92180.92180.9218，可以看到，准确率有很大提升。 Weighted Majority Vote 在此基础上，给每一个模型的预测结果添加权重 y^=arg max⁡i∈A∑j=1mwj[Cj(x)=i] \\hat y=\\argmax_{i\\in A} \\sum_{j=1}^m w_j \\Big[ C_j(\\bold{x})=i \\Big] y^​=i∈Aargmax​j=1∑m​wj​[Cj​(x)=i]其中 AAA 是所有的标签，方括号函数表示如果第 jjj 的分类器对于样本 x\\bold xx 给出的预测结果是 iii 类别的话则为 111，否则为 000. 因此，Weighted Vote 就相当于是枚举标签，然后看每一个模型预测结果的加权平均，取均值最大的那个对应的标签。 Soft Voting 有的模型可以输出概率，所以我们也可以对概率进行加权，最后取最高 y^=arg max⁡i∈A∑j=1mwj⋅Pj(i) \\hat y=\\argmax_{i\\in A}\\sum_{j=1}^m w_j\\cdot P_{j}(i) y^​=i∈Aargmax​j=1∑m​wj​⋅Pj​(i) 代码实现 下面的代码实现了一个 Majority Vote Classifier (vote='classlabel') 和 Soft Vote (vote='probability') from sklearn.base import BaseEstimator, ClassifierMixin, clonefrom sklearn.preprocessing import LabelEncoderfrom sklearn.pipeline import _name_estimatorsimport numpy as npimport operatorclass MajorityVoteClassifier(BaseEstimator, ClassifierMixin): def __init__(self, classifiers, vote=classlabel, weights=None): __init__ 函数接收分类器列表，进行初始化 vote 表示投票方法 self.classifiers = classifiers self.named_classifiers = key: value for key, value in _name_estimators(classifiers) self.vote = vote self.weights = weights def fit(self, X, y): fit() 根据输入的数据 + 标签， 对标签进行 encoding（方便 Soft Vote 获取概率） 然后对 classifier 模型进行训练，并存起来 self.label_enc = LabelEncoder() self.label_enc.fit(y) self.classes = self.label_enc.classes_ self.trained_classifiers = [] for classifier in self.classifiers: trained_clf = clone(classifier).fit( X, self.label_enc.transform(y), ) self.trained_classifiers.append(trained_clf) return self def predict(self, X): probability 部分比较容易理解 classlabel 部分的话，我们首先获取每一个模型的输出结果（`predictions`）， 然后 if self.vote == probability: maj_vote = np.argmax(self.predict_proba(X), axis=1) else: predictions = np.asarray( [ clf.predict(X) for clf in self.trained_classifiers ] ).T maj_vote = np.apply_along_axis( lambda x: np.argmax(np.bincount(x, weights=self.weights)), axis=1, arr=predictions) maj_vote = self.label_enc.inverse_transform(maj_vote) return maj_vote def predict_proba(self, X): probas = np.asarray([clf.predict_proba(X) for clf in self.classifiers_]) avg_proba = np.average(probas, axis=0, weights=self.weights) return avg_proba"},{"title":"Regression","path":"/wiki/ml/regression.html","content":"Linear Regression Polynomial Regression 对每一个 feature xix_ixi​ 都映射到 xi1,xi2,…xidx_i^1,x_i^2,\\dots x_i^dxi1​,xi2​,…xid​，以及可能的交叉项。 Decision Tree (Random Forest) Regression Random Forest: 视为 ensemble of 多个 linear functions Impurity for continuous variables I(t)=MSE(t)=1Nt⋅∑i∈Dt(y(i)−y^t)2 I(t)=MSE(t)=\\frac{1}{N_t}\\cdot \\sum_{i\\in D_t}\\Big( y^{(i)}-\\hat y_t \\Big)^2 I(t)=MSE(t)=Nt​1​⋅i∈Dt​∑​(y(i)−y^​t​)2其中 NtN_tNt​ 是节点 ttt 内的样本数量，DtD_tDt​ 代表这个节点对应的所有样本，y^t\\hat y_ty^​t​ 代表预测样本值（其实是 sample mean），y(i)y^{(i)}y(i) 表示样本真实值 y^t=1Nt∑i∈Dty(i) \\hat y_t=\\frac{1}{N_t}\\sum_{i\\in D_t}y^{(i)} y^​t​=Nt​1​i∈Dt​∑​y(i)使用随机森林时，在构建单棵决策树的时候，predicted target variable is calculated as the average prediction over all decision trees"},{"title":"Regularization","path":"/wiki/ml/regularization.html","content":"Regularization shrink the parameter values L2 正则化: Ridge 在原始的损失函数上加上 L2\\mathcal{L}_2L2​ Penalty JRidge=J(θ)+α⋅∥θ∥22 J_{Ridge}=J(\\theta)+\\alpha\\cdot\\|\\theta\\|_2^2 JRidge​=J(θ)+α⋅∥θ∥22​ L1 正则化: LASSO 加上 L1\\cal L_1L1​ Penalty JLASSO=J(θ)+α⋅∥θ∥ J_{LASSO}=J(\\theta)+\\alpha\\cdot\\|\\theta\\| JLASSO​=J(θ)+α⋅∥θ∥ 有些权重容易变成 000 折中: Elastic Net λ\\lambdaλ 控制比例，α\\alphaα 控制正则程度 JElastic=J(θ)+α⋅[λ∥θ∥2+(1−λ)∥θ∥] J_{Elastic}=J(\\theta)+\\alpha\\cdot\\Bigg[ \\lambda\\|\\theta\\|^2+(1-\\lambda)\\|\\theta\\| \\Bigg] JElastic​=J(θ)+α⋅[λ∥θ∥2+(1−λ)∥θ∥]"},{"title":"Support Vector Machine","path":"/wiki/ml/svm.html","content":"Linear SVM Kernel SVM 通过映射函数 ϕ()\\phi()ϕ()，将低维的特征向量 feature vector xxx 映射到高维空间中 vxv_xvx​，以期望在低维空间不可线性分割的 feature vector 在高维空间可以被线性分割。 但是当映射到高维空间之后，高维向量之间的点乘运算比较耗时，因此利用核函数 Kernel Function K(v1,v2)\\mathcal{K}(v_1,v_2)K(v1​,v2​) 替换点乘运算。例如常见的做法是 K(x(i),x(j))=exp⁡(−γ∥x(i)−x(j)∥2) \\mathcal{K}(x^{(i)},x^{(j)})=\\exp\\Big( -\\gamma\\|x^{(i)}-x^{(j)} \\|^2 \\Big) K(x(i),x(j))=exp(−γ∥x(i)−x(j)∥2)当 γ=12σ2\\gamma=\\frac{1}{2\\sigma^2}γ=2σ21​ 时，就是高斯核函数。"},{"title":"ViT","path":"/wiki/multimodal/ViT.html","content":"372f7a9688aae3bb4ec6e6afab19ca439bb95e6d91164a50dda5c3f9a4f813ff Password is needed."},{"title":"VAE: Variational AutoEncoders","path":"/wiki/multimodal/VAE.html","content":"Representation 图像生成模型的本质是一个概率模型：如果我们知道了真实图像 xxx 的分布规律 p(x)p(x)p(x)，那么我们只需要从这个分布里随便采样 x′∼p(x)x\\sim p(x)x′∼p(x)，那么 x′xx′ 就是我们想要生成的图像。 不过通常，p(x)p(x)p(x) 很难表示和学习。我们考虑通过两个步骤生成图像： 先生成图片的特征，例如想要生成二次元图片，就先指定 tags 例如发色、动作等等 在根据特征，去生成图像 我们用 zzz 表示图像的“特征” (latent variable)，那么这样的过程就是如同下面所示 z⟶guidex \\boxed{z}\\overset{\\text{guide}}{\\longrightarrow} \\boxed{x} z​⟶guide​x​用数学语言描述就是这样一个恒等式 p(x)=∑zp(x∣z)⋅p(z) p(x)=\\sum_z p(x|z)\\cdot p(z) p(x)=z∑​p(x∣z)⋅p(z)VAE 的推理从数学的角度也就变成了 Sample zzz from p(z)p(z)p(z) Sample xxx from p(x∣z)p(x|z)p(x∣z) 当然，由于我们的目的是简化 p(x)p(x)p(x) 的建模，因此我们通常假设 p(z)∼N(0,1)p(x∣z)∼N(μθ(z),Σθ(z)) \\begin{aligned} p(z)\\sim \\mathcal N(0,1)\\\\ p(x|z)\\sim \\mathcal N(\\mu_\\theta(z),\\Sigma_\\theta(z)) \\end{aligned} p(z)p(x∣z)​∼N(0,1)∼N(μθ​(z),Σθ​(z))​其中 μθ(⋅),Σθ(⋅)\\mu_\\theta(\\cdot),\\Sigma_\\theta(\\cdot)μθ​(⋅),Σθ​(⋅) 是神经网络。 这里也不一定非得是正态分布，其他容易计算的分布也可以。简单起见直接用正态分布了 Inference Inferencing Objective Function 给定一个数据集 D={x1,x2,…,xm}\\mathcal D=\\{x^{1}, x^{2}, \\dots, x^{m}\\}D={x1,x2,…,xm}，模型训练目标就是，从数据集 D\\mathcal DD 里训练的图像分布 pθ(x)p_\\theta(x)pθ​(x) 和真实的图像分布 p(x)p(x)p(x) 尽可能接近。衡量两个分布接近程度可以用 KL 散度，即训练目标为最小化 KL 散度： min⁡θDKL(pθ(x)∥p(x)) \\min_\\theta D_{KL}\\Big( p_\\theta(x) \\big\\| p(x) \\Big) θmin​DKL​(pθ​(x)​p(x))最小化 KL 散度等同于最大化 Marginal Log-Likelilhood log⁡pθ(x)\\log p_\\theta(x)logpθ​(x) over D\\mathcal DD max⁡θ∑xi∈Dlog⁡pθ(xi)=max⁡θ∑xi∈Dlog⁡(∑zpθ(xi,z)) \\begin{aligned} \\max_\\theta \\sum_{x^{i} \\in\\mathcal D} \\log p_\\theta(x^{i})\\\\ =\\max_\\theta \\sum_{x^{i} \\in\\mathcal D} \\log \\Big(\\sum_z p_\\theta(x^{i},z)\\Big) \\end{aligned} =​θmax​xi∈D∑​logpθ​(xi)θmax​xi∈D∑​log(z∑​pθ​(xi,z))​然而，zzz 是高维空间的隐变量，∑z\\sum_z∑z​ 需要遍历所有可能的 zzz、计算 pθ(xi,z)p_\\theta(x^{i},z)pθ​(xi,z)、再相加，几乎是不可能做到的，我们只能用各种方法去近似求解 log-likelihood via Monte Carlo 我们随机采样一些 zi∼p(z)z^{i} \\sim p(z)zi∼p(z)，用这些采样的 ziz^{i}zi 计算平均值： log⁡pθ(x)≈log⁡1k∑i=1kp(x∣zi),zi∼p(z) \\log p_\\theta(x)\\approx \\log \\frac{1}{k}\\sum_{i=1}^k p(x|z^{i}), \\quad z^{i}\\sim p(z) logpθ​(x)≈logk1​i=1∑k​p(x∣zi),zi∼p(z)尽管理论上，蒙特卡洛估计方法是 no-bias 的，但是在实战中，用蒙特卡洛计算出来的梯度具有很大的方差。 via Importance Sampling 比起直接 maximize 目标，我们也可以构造出目标的 lower bound 然后通过 maximize 这个 lower bound 从而 maximize 目标。 此处，log⁡pθ(x)\\log p_\\theta(x)logpθ​(x) 的一个下界被称为 ELBO (Evidence Lower Bound) pθ(x)=∑zq(z)q(z)pθ(x,z)=∑zq(z)⋅pθ(x,z)q(z)=Ez∼q(z)[pθ(x,z)q(z)]log⁡pθ(x)=log⁡Ez∼q(z)[pθ(x,z)q(z)]=log⁡∑zq(z)⋅pθ(x,z)q(z)≥∑zq(z)⋅log⁡pθ(x,z)q(z)‾by Jensen’s Inequality=Ez∼q(z)[log⁡pθ(x,z)q(z)]≔ELBO(x;θ)=Lθ(x) \\begin{aligned} p_\\theta(x) =\\sum_z \\frac{q(z)}{q(z)}p_\\theta(x,z)\\\\ =\\sum_z q(z)\\cdot \\frac{p_\\theta(x,z)}{q(z)}\\\\ =\\mathbb E_{z\\sim q(z)}\\Big[\\frac{p_\\theta(x,z)}{q(z)}\\Big]\\\\ \\log p_\\theta(x)=\\log \\mathbb E_{z\\sim q(z)}\\Big[ \\frac{p_\\theta(x,z)}{q(z)} \\Big]\\\\ = \\log \\sum_z q(z)\\cdot \\frac{p_\\theta(x,z)}{q(z)}\\\\ \\ge \\underset{\\scriptsize\\text{by Jensens Inequality}}{\\underline{\\sum_z q(z)\\cdot \\log\\frac{p_\\theta(x,z)}{q(z)}}}\\\\ =\\mathbb E_{z\\sim q(z)}\\Big[ \\log\\frac{p_\\theta(x,z)}{q(z)} \\Big]\\\\ \\coloneqq \\text{ELBO}(x;\\theta)=\\mathcal L_{\\theta}(x) \\end{aligned} pθ​(x)logpθ​(x)​=z∑​q(z)q(z)​pθ​(x,z)=z∑​q(z)⋅q(z)pθ​(x,z)​=Ez∼q(z)​[q(z)pθ​(x,z)​]=logEz∼q(z)​[q(z)pθ​(x,z)​]=logz∑​q(z)⋅q(z)pθ​(x,z)​≥by Jensen’s Inequalityz∑​q(z)⋅logq(z)pθ​(x,z)​​​=Ez∼q(z)​[logq(z)pθ​(x,z)​]:=ELBO(x;θ)=Lθ​(x)​ 从 KL 散度的视角理解 ELBO 而实际上 log⁡pθ(x)=Ez∼q(z)[log⁡pθ(x,z)]+H(q)entropy of q(z)‾ \\log p_\\theta(x)=\\mathbb E_{z\\sim q(z)}\\Big[ \\log p_\\theta(x,z) \\Big] + \\underset{\\overline{\\scriptsize\\text{entropy of }q(z)}}{H(q)} logpθ​(x)=Ez∼q(z)​[logpθ​(x,z)]+entropy of q(z)​H(q)​直觉上理解，我们选取的 q(z)q(z)q(z) 应该同模型从图像出发对特征的预测接近，即 DKL(q(z)∥pθ(z∣x)) D_{KL}\\Big( q(z) \\big\\| p_\\theta(z|x) \\Big) DKL​(q(z)​pθ​(z∣x))越小越好。而 DKL()D_{KL}()DKL​() 具有非负性，移项后便是 ELBO 的形式。一般形式的，也有 log⁡pθ(x)=ELBO+DKL(q(z)∥pθ(z∣x)) \\log p_\\theta(x)=\\text{ELBO}+D_{KL}\\Big( q(z)\\big\\|p_\\theta(z|x) \\Big) logpθ​(x)=ELBO+DKL​(q(z)​pθ​(z∣x)) 然后我们就又可以用 Monte Carlo 方法估计 ELBO 了。 ELBO(x;θ)≈1k∑i=1klog⁡pθ(x,zi)q(zi),zi∼q(z) \\text{ELBO}(x;\\theta)\\approx \\frac{1}{k}\\sum_{i=1}^k \\log\\frac{p_\\theta(x,z^{i})}{q(z^{i})},\\quad z^{i}\\sim q(z) ELBO(x;θ)≈k1​i=1∑k​logq(zi)pθ​(x,zi)​,zi∼q(z) VAE: Decoder 与 Encoder from Decoder to Encoder: Variational Inference 到目前位置，我们实际上只讨论了 Decoder 部分：pθ(x∣z)p_\\theta(x|z)pθ​(x∣z)。为了训练模型，我们肯定还需要 x→zx\\to zx→z 的推理与训练。这就是 VAE 里 Encoder 的作用。 Encoder 负责的就是 pθ(z∣x)p_\\theta(z|x)pθ​(z∣x)，但是 pθ(z∣x)p_\\theta(z|x)pθ​(z∣x) 很难从神经网络模型中推导出来。不过，根据上文对 ELBO 与 KL 散度 DKL(q(z)∥pθ(z∣x))D_{KL}\\Big( q(z)\\big\\| p_\\theta(z|x) \\Big)DKL​(q(z)​pθ​(z∣x)) 的分析，我们其实也可以通过优化 q(z)q(z)q(z) 让其近似 pθ(z∣x)p_\\theta(z|x)pθ​(z∣x) 来达成相同的目的。 所以，我们把 q(z)q(z)q(z) 也用神经网络建模为 qϕ(z)q_\\phi(z)qϕ​(z)，其中 ϕ\\phiϕ 为 Encoder 模型的参数，此时 ELBO 改写为 ELBO=Lθ,ϕ(x)=∑zqϕ(z)log⁡pθ(z,x)+H(qϕ(z)) \\text{ELBO}=\\mathcal L_{\\theta,\\phi}(x)=\\sum_z q_\\phi(z)\\log p_\\theta(z,x)+H(q_\\phi(z)) ELBO=Lθ,ϕ​(x)=z∑​qϕ​(z)logpθ​(z,x)+H(qϕ​(z)) Amortized Inference 注意：这里的 Decoder 与 Encoder 本质上是对分布进行建模，即给定张量，输出一个分布。 如果 Encoder 部分我们为每一个输入的图像都训练一个 Encoder qϕ(z)q_\\phi(z)qϕ​(z)，计算代价无法承受。 因此，我们用神经网络对分布进行拟合，即 gλ:xi↦qϕi(z)g_\\lambda:x^{i} \\mapsto q_{\\phi^{i}}(z)gλ​:xi↦qϕi​(z)，这样就可以避免反复求解 ϕi\\phi^{i}ϕi 了。 而对 Decoder 部分就不用了，因为 pθ(x∣z)p_\\theta(x|z)pθ​(x∣z) 的 zzz 是由 Encoder 完成的，而每一个而 Encoder 总是输出的 qϕ(z)≈pθ(z∣x)q_\\phi(z)\\approx p_\\theta(z|x)qϕ​(z)≈pθ​(z∣x) 总是映射到同一个 random variable space 里. Training VAE 有一个 Encoder 架构，负责将图像 xxx 编码为 latent variable zzz；Decoder 架构则负责从 latent variable zzz 生成出图像 xxx. VAE 上文的 ELBO 则为我们优化 VAE 模型提供了一个良好的目标函数：（其实应该是求解上文的 λ\\lambdaλ） max⁡θ,ϕELBO=max⁡θ∑x∈Dmax⁡ϕEqϕ(z)[log⁡pθ(z,x)qϕ(z)]⇒max⁡θ,λ∑x∈Dmax⁡λEgλ(x)[log⁡pθ(z,x)gλ(x)] \\begin{aligned} \\max_{\\theta,\\phi}\\text{ELBO}=\\max_{\\theta}\\sum_{x\\in\\mathcal D}\\max_{\\phi}\\mathbb E_{q_\\phi(z)}\\Bigg[ \\log\\frac{p_\\theta(z,x)}{q_\\phi(z)} \\Bigg]\\\\ \\Rightarrow\\max_{\\theta,\\lambda}\\sum_{x\\in\\mathcal D}\\max_\\lambda\\mathbb E_{g_\\lambda(x)}\\Bigg[ \\log\\frac{p_\\theta(z,x)}{g_\\lambda(x)} \\Bigg] \\end{aligned} θ,ϕmax​ELBO​=θmax​x∈D∑​ϕmax​Eqϕ​(z)​[logqϕ​(z)pθ​(z,x)​]⇒θ,λmax​x∈D∑​λmax​Egλ​(x)​[loggλ​(x)pθ​(z,x)​]​ Stochastic Variational Inference 用随机梯度下降法进行学习 初始化 θ,ϕ1…m\\theta,\\phi^{1\\dots m}θ,ϕ1…m 随机一个 xi∈Dx^{i} \\in\\mathcal Dxi∈D 先优化 ϕi\\phi^{i}ϕi： ϕi←ϕi+η∇ϕiLθ,ϕ(xi)\\phi^{i}\\gets \\phi^{i}+\\eta abla_{\\phi^{i}}\\mathcal L_{\\theta,\\phi}(x^{i})ϕi←ϕi+η∇ϕi​Lθ,ϕ​(xi) 直到收敛为止 更新 θ\\thetaθ：θ←θ+η∇θLθ,ϕi(xi)\\theta\\gets\\theta+\\eta abla_{\\theta}\\mathcal L_{\\theta,\\phi^{i}}(x^{i})θ←θ+η∇θ​Lθ,ϕi​(xi)。回到 step 2 继续执行。 那么我们如何计算梯度呢？因为很有可能这个式子并不存在 closed form，我们依然采用 Monte Carlo 的方法解决问题，即 Eqϕ(z)[log⁡pθ(z,x)−log⁡qϕ(z)]≈1K∑i=1Klog⁡pθ(zi,x)−log⁡qϕ(zi) \\mathbb E_{q_\\phi(z)}\\Bigg[ \\log p_\\theta(z,x)-\\log q_\\phi(z) \\Bigg]\\approx \\frac{1}{K}\\sum_{i=1}^{K}\\log p_\\theta(z^{i},x)-\\log q_\\phi(z^{i}) Eqϕ​(z)​[logpθ​(z,x)−logqϕ​(z)]≈K1​i=1∑K​logpθ​(zi,x)−logqϕ​(zi)其中 qϕ(z)q_\\phi(z)qϕ​(z) 应该容易采样和计算。由此，ELBO 关于 θ\\thetaθ 的导数即为 ∇θEqϕ(z)[log⁡pθ(z,x)−log⁡qϕ(z)]≈1K∑i=1K∇θlog⁡pθ(zi,x) abla_\\theta \\mathbb E_{q_\\phi(z)}\\Bigg[ \\log p_\\theta(z,x)-\\log q_\\phi(z) \\Bigg]\\approx \\frac{1}{K}\\sum_{i=1}^K abla_\\theta \\log p_\\theta(z^{i},x) ∇θ​Eqϕ​(z)​[logpθ​(z,x)−logqϕ​(z)]≈K1​i=1∑K​∇θ​logpθ​(zi,x)然而 ELBO 关于 ϕi\\phi^iϕi 的导数不那么好算，因为期望本身依赖于这个参数。一般而言，可以使用强化学习的方法进行学习，也可以使用 Reparameterization 的方法。 Reparam 我们把 qϕ(z)∼N(μ,σ2I)q_\\phi(z)\\sim \\mathcal N(\\mu, \\sigma^2 I)qϕ​(z)∼N(μ,σ2I)，即 ϕi=(μ,σ)\\phi^i=(\\mu,\\sigma)ϕi=(μ,σ)，那么从这个正态分布采样就等同于 ϵ∼N(0,1)z=μ+σϵ=gϕ(ϵ) \\epsilon\\sim \\mathcal N(0,1)\\\\ z=\\mu+\\sigma\\epsilon=g_\\phi(\\epsilon) ϵ∼N(0,1)z=μ+σϵ=gϕ​(ϵ)借用这个想法，我们可以改写 ELBO，这里先让 r(z)=log⁡qϕ(z)r(z)=\\log q_\\phi(z)r(z)=logqϕ​(z) 简化计算，稍后再代入 Ez∼qϕ(z)[r(z)]=∑zqϕ(z)r(z)=Eϵ∼N(0,1)[r(gϕ(ϵ))]=∫N(ϵ)r(μ+σϵ)dϵ∇ϕEqϕ(z)[r(z)]=∇ϕEϵ[r(gϕ(ϵ))]=Eϵ[∇ϕr(gϕ(ϵ))]≈1K∑i=1Kr(gϕ(ϵi))‾Monte Carlo Estim \\begin{aligned} \\mathbb E_{z\\sim q_\\phi(z)}[r(z)]=\\sum_z q_\\phi(z)r(z)\\\\ =\\mathbb E_{\\epsilon\\sim\\mathcal N(0,1)}[r(g_\\phi(\\epsilon))]\\\\ =\\int \\mathcal N(\\epsilon) r(\\mu+\\sigma\\epsilon) d\\epsilon\\\\ abla_\\phi \\mathbb E_{q_\\phi(z)}[r(z)]= abla_\\phi \\mathbb E_\\epsilon [r(g_\\phi(\\epsilon))]\\\\ =\\mathbb E_{\\epsilon}[ abla_\\phi r(g_\\phi(\\epsilon))]\\\\ \\approx \\underset{\\text{Monte Carlo Estim}}{\\underline{\\frac{1}{K}\\sum_{i=1}^K r(g_\\phi(\\epsilon^i))}} \\end{aligned} Ez∼qϕ​(z)​[r(z)]∇ϕ​Eqϕ​(z)​[r(z)]​=z∑​qϕ​(z)r(z)=Eϵ∼N(0,1)​[r(gϕ​(ϵ))]=∫N(ϵ)r(μ+σϵ)dϵ=∇ϕ​Eϵ​[r(gϕ​(ϵ))]=Eϵ​[∇ϕ​r(gϕ​(ϵ))]≈Monte Carlo EstimK1​i=1∑K​r(gϕ​(ϵi))​​​"},{"title":"LightRAG","path":"/wiki/rag/LightRAG.html","content":"LightRAG Overview LightRAG 是由 香港大学研究团队 推出的一种轻量级检索增强生成（Retrieval-Augmented Generation, RAG）系统，通过整合 图结构索引 和 双层检索机制，显著提升了大型语言模型（LLM）在信息检索中的准确性与效率 。其核心特点是： 轻量化设计 相较于传统的 GraphRAG，LightRAG 通过简化知识图谱构建流程和减少冗余计算，大幅降低了资源消耗和 API 调用成本，在动态数据环境中更新速度更快、维护更灵活 。 双层检索架构 LightRAG 结合关键词匹配与语义向量检索：第一层快速定位实体及关联关系，第二层通过向量数据库（如 Milvus 或 TiDB Vector）补充相关片段，兼顾效率与精度 。 动态数据支持 系统支持增量更新，能够高效处理实时变化的数据源，适应动态环境下的检索需求 。 成本效益 实验表明，LightRAG 在保证检索质量的同时，能显著降低大模型问答的计算成本，适用于高并发或预算受限的场景 。 LightRAG"},{"title":"基于 Nano GraphRAG 的二次开发","path":"/wiki/rag/build-on-nano-graphrag.html","content":"本地部署 LLM"},{"title":"Dify 中使用 HTTP Request 配合 Flask 进行高级数据处理","path":"/wiki/rag/custom-data-process.html","content":"Flask Server"},{"title":"GraphRAG 解读","path":"/wiki/rag/graph-rag.html","content":"算法流程 GraphRAG WorkFlow Overview NanoGraphRAG 工作流程 Chunk Documents graph LR A(Document) --> B(Text Chunk 1) A --> C(Text Chunk 2) 像普通的 VectorRAG 一样，提取出来的 Text Chunk 可以用于后续 LLM 的知识来源。 NanoGraphRAG 源码解读 Extract Entity and Relationships Graph Indexing Graph Decomposition"},{"title":"NanoGraphRAG项目思路（一）：文档分块","path":"/wiki/rag/nano-graph-rag-p1.html","content":"Phase 1: 文档预处理 (论文 3.1.1) 在 Nano GraphRAG 里，预处理文档的任务在 GraphRAG.ainsert() 中完成。GraphRAG.ainsert() 由 GraphRAG.insert() 调用，并且使用了 asyncio.get_event_loop() 保证执行完毕。 文档去重 ainsert() 首先计算文档的 MD5 哈希值，检查文档是否已经添加过了。如果已经添加过了，那么就不用再插入数据库。 new_docs 首先准备所有待检查的字符串，计算其哈希值。 new_docs = compute_mdhash_id(c.strip(), prefix=doc-): content: c.strip() for c in string_or_strings 然后丢入 self.full_docs 检查是否有重复的字符串并剔除 _add_doc_keys = await self.full_docs.filter_keys(list(new_docs.keys())) 剔除完成后的字符串拼接成 hash: str 形式的字典重新赋值回 new_docs. new_docs = k: v for k, v in new_docs.items() if k in _add_doc_keys 如果没有新文档，直接退出。然后输出一点调试日志 if not len(new_docs): logger.warning(fAll docs are already in the storage) returnlogger.info(f[New Docs] inserting len(new_docs) docs) 文档分块 Text Chunk 否则，准备插入文档。先对文档 get_chunk() 拆分成若干个 text chunks inserting_chunks = get_chunks( new_docs=new_docs, # 要切分的文档 chunk_func=self.chunk_func, # chunk 切分方法，默认按 token 数量 overlap_token_size=self.chunk_overlap_token_size, # chunk 之间重叠的 token 数量，充当上下文的作用 max_token_size=self.chunk_token_size, # 一个 chunk 最多多少 token) 然后 get_chunk() 加载 Tokenizer (这里调用的是 OpenAI tiktoken 的库)，将 docs（纯文本）转化为 Tokens，再调用 chunk_func 进行切块，每一块都标注成 tokens: token 个数, content: token 内容（已经不是文本了）, chunk_order_index: 是第几个 chunk, full_doc_id: 所属文档的哈希值, 标注完成后，再为每一个 text chunk 计算哈希值 chunk-xxxxx，放入 inserting_chunk 并返回 Text Chunk 去重 对 text chunks 也进行去重 _add_chunk_keys = await self.text_chunks.filter_keys( list(inserting_chunks.keys()))inserting_chunks = k: v for k, v in inserting_chunks.items() if k in _add_chunk_keysif not len(inserting_chunks): logger.warning(fAll chunks are already in the storage) returnlogger.info(f[New Chunks] inserting len(inserting_chunks) chunks)if self.enable_naive_rag: logger.info(Insert chunks for naive RAG) await self.chunks_vdb.upsert(inserting_chunks) 和文档去重的逻辑很相似，就是在 KVStorage 里按哈希值查找，去重 更新数据库 这里有另一点比较重要的是，如果需要插入新的 chunk，那么首先需要把 self.community_reports 清空。因为插入新块后，可能这个文档的 community 就会改变 await self.community_reports.drop()"},{"title":"基础","path":"/wiki/rustbook/basics.html","content":"流程控制"},{"title":"泛型、模板","path":"/wiki/rustbook/generic.html","content":"泛型（对标 C++ 模板） 函数、方法、结构体、枚举都可以使用泛型。 Type Annotation 明确指出泛型的类型需要满足什么条件。 函数泛型/函数模板 fn funcT(a: T) template typename Tvoid function(T a) 方法泛型/方法模板 可以包含其他的类型 implT PointT fn funcU(self) - T // 等价于 ...... template typename Tclass Point template typename U T func() const 泛型、模板参数 对应 C++ 中 template int N 这样的模板参数。 fn funcT, const N: usize(arr: [T; N]) template typename T, size_t Nvoid func(std::arrayT, N arr) 针对 const 泛型做检查 这个在 C++ 里应该需要使用 require 做检查，我还没有研究过。Rust 里使用 Assert: IsTrue 泛型限制即可。 fn somethingT(val: T)where Assert core::mem::size_of::T() 768 : IsTrue, // ^ 这里是一个 const 表达式，换成其它的 const 表达式也可以 // const fn 对应 C++ 的 constexpr，在编译期求值。"},{"title":"深入特征","path":"/wiki/rustbook/more-traits.html","content":"关联类型 pub trait Iterator type Item; fn next(mut self) - OptionSelf::Item; why not Generics? 使用泛型：trait ContainerA, B fn contains(self, a: A, b: B) - bool;fn differenceA, B, C(container: C) - i32 where C : ContainerA, B ... 使用关联类型trait Container type A; type B; fn contains(self, a: Self::A, b: Self::B) - bool;fn differenceC: Container(container: C) 特征和类型都有同名方法 优先调用类型上的方法 显示通过特征调用方法 // for example ...Trait.method(object); 用完全限定语法，让类型使用某个特征的方法（也可适用于 2） // like thisType as Trait::function(receiver_if_method, other_arguments...); 特征定义中的特征约束 trait X: Y 如果你想要实现 X 特征，首先你需要实现 Y 特征。 外部类型上实现外部特征"},{"title":"所有权","path":"/wiki/rustbook/ownership.html","content":"所有权"},{"title":"特征、特征对象","path":"/wiki/rustbook/traits.html","content":"Traits 特征定义了一组可以被共享的行为，只要实现了特征，你就能使用这组行为。 Trait 实现：孤儿规则 如果你想要为类型 A 实现特征 T，那么 A 或者 T 至少有一个是在当前作用域中定义的！ 使用特征作为函数参数 意义：所有实现了 Trait 这个特征的类型 fn func(item: impl Trait) 本质是泛型的语法糖： fn funcT: Trait(item: T) derive 派生 特征作为返回值 返回一个对象，这个对象实现了 Summary 特征 fn func(...) - impl Summary 问题：最终 impl Summary 只能对应一个类型，如果想做到 if .. return A else return B 是不行的 在 C++ 里，我们可以用继承的方式，返回父类指针。在 Rust 里应该怎么做呢？ 在 Rust 里，我们使用 Boxdyn Trait 或者 dyn Trait，两者的区别在于：前者是通过 Box::new() 在堆上创建的新变量，后者是对已有的变量的引用。 特征对象的动态分发 当使用特征对象时，Rust 必须使用动态分发。编译器无法知晓所有可能用于特征对象代码的类型，所以它也不知道应该调用哪个类型的哪个方法实现。为此，Rust 在运行时使用特征对象中的指针来知晓需要调用哪个方法。动态分发也阻止编译器有选择的内联方法代码，这会相应的禁用一些优化。 trait object 虽然特征对象没有固定大小，但它的引用类型的大小是固定的，它由两个指针组成（ptr 和 vptr），因此占用两个指针大小 一个指针 ptr 指向实现了特征 Draw 的具体类型的实例，也就是当作特征 Draw 来用的类型的实例，比如类型 Button 的实例、类型 SelectBox 的实例 另一个指针 vptr 指向一个虚表 vtable，vtable 中保存了类型 Button 或类型 SelectBox 的实例对于可以调用的实现于特征 Draw 的方法。当调用方法时，直接从 vtable 中找到方法并调用。之所以要使用一个 vtable 来保存各实例的方法，是因为实现了特征 Draw 的类型有多种，这些类型拥有的方法各不相同，当将这些类型的实例都当作特征 Draw 来使用时(此时，它们全都看作是特征 Draw 类型的实例)，有必要区分这些实例各自有哪些方法可调用 但是虚表里只会保存 Draw 特征的方法 struct A impl A fn foo(self) println!(A::foo); struct B impl B fn foo(self) println!(B::foo); trait Bar fn bar(self);impl Bar for A fn bar(self) println!(A::bar); impl Bar for B fn bar(self) println!(B::bar); fn main() let mut v = Vec::Boxdyn Bar::new(); v.push(Box::new(A)); v.push(Box::new(B)); for item in v.iter() item.bar(); // item.foo(); // This line will cause a compile-time error 特征对象的限制 必须满足下列所有条件： 返回类型不能是 Self 如果特征方法返回了具体的 Self 类型，但是特征对象忘记了其真正的类型（特征对象只知道这个类型实现了某个 Trait），那这个 Self 就非常尴尬，因为没人知道它是谁了。 没有泛型参数 对于泛型类型参数来说，当使用特征时其会放入具体的类型参数：此具体类型变成了实现该特征的类型的一部分。而当使用特征对象时其具体类型被抹去了，故而无从得知放入泛型参数类型到底是什么。 针对第一点的解释 考虑这么一段代码 trait Trait fn foo(self) - Self;fn call_foo(x: Boxdyn Trait) let y = x.foo(); // What type is y? // ... 我们只能推断出 y 的类型是某个实现了 Trait 特征的类型， 针对第二点的解释 trait Trait fn fooT(self, on: T); // more methodsimpl Trait for String fn fooT(self, on: T) // implementation 1 impl Trait for u8 fn fooT(self, on: T) // implementation 2 // 8 more implementations 考虑这样的调用 fn call_foo(thing: Boxdyn Trait) thing.foo(true); // this could be any one of the 8 types above thing.foo(1); thing.foo(hello); Rust 针对泛型的处理又是单态化，这意味着这样一结合，Rust 代码会出现 303030 种实现，全都是泛型的单态化展开。"},{"title":"强化学习研究的是什么？","path":"/wiki/rl/what-is-rl.html","content":"强化学习研究的是什么？ 强化学习的研究目标可以概括为一句话：在特定的环境 (Environment) 下，智能体 (Agent) 如何通过和环境的交互，学习一个策略 (Policy)，使其在长期内获得最大的累计奖励。 强化学习的目标 .typst-text { pointer-events: bounding-box; } .tsel span, .tsel { left: 0; position: fixed; text-align: justify; white-space: nowrap; width: 100%; height: 100%; text-align-last: justify; color: transparent; white-space: pre; } .tsel span::-moz-selection, .tsel::-moz-selection { color: transparent; background: #7db9dea0; } .tsel span::selection, .tsel::selection { color: transparent; background: #7db9dea0; } .pseudo-link { fill: transparent; cursor: pointer; pointer-events: all; } svg { fill: none; } .outline_glyph path, path.outline_glyph { fill: var(--glyph_fill); stroke: var(--glyph_stroke); } .outline_glyph path, path.outline_glyph { transition: 0.2s fill stroke; } .hover .typst-text { --glyph_fill: #66bab7; --glyph_stroke: #66bab7; } .typst-jump-ripple, .typst-debug-react-ripple { width: 0; height: 0; background-color: transparent; position: absolute; border-radius: 50%; } .typst-jump-ripple { border: 1px solid #66bab7; } .typst-debug-react-ripple { border: 1px solid #cb1b45; } @keyframes typst-jump-ripple-effect { to { width: 10vw; height: 10vw; opacity: 0.01; margin: -5vw; } } @keyframes typst-debug-react-ripple-effect { to { width: 3vw; height: 3vw; opacity: 0.01; margin: -1.5vw; } } •State 𝑠∈𝑆, 表示 agent 在环境中所有可能的状态•Action 𝑎∈𝐴, 表示 agent 在环境中所有可以进行的动作 因此，从环境的视角来看，agent 实际上不断地在产生 State, Action, State, Action, … 的数据：agent 每一次行动都会导致自己的 state 发生变化。 (s0,a0,s1,a1,s2,a2,… ) (s_0,a_0,s_1,a_1,s_2,a_2,\\dots) (s0​,a0​,s1​,a1​,s2​,a2​,…)agent 通过和环境的交互学习如何根据环境和状态决定自己下一步的动作，也就是 policy. 我们可以把 policy 看成是一个函数：接收当前 state 作为参数，输出 action 表示当前 state 下应该怎么行动 Policyπ:S↦A \\text{Policy}\\quad \\pi: S \\mapsto A Policyπ:S↦A那么我们怎么让 agent 学习 action 呢？如果我们希望 agent 按照我们预想的那样，在特定的 state 下学习到应该走出特定的步骤，我们该怎么诱导 agent 呢？就是利用 Reward 机制，诱导 agent 向着 Reward 更大的方向行动，就好像我们玩游戏的时候也是朝着让自己更加“强大”的方向去打怪升级。 于是我们就需要给 agent 的每一次行动进行“打分”，告诉 agent 你这一步棋走的好不好。我们可以把 Reward 也看成一个函数，给定 sss 当前的状态、aaa 采取的行动、s′ss′ 行动完后的状态，来判断这个行动是不是 OK. R(s′∣s,a)∈R R(s\\vert s,a) \\in \\R R(s′∣s,a)∈R于是考虑上 reward 的话，agent 的动作 sequence 就变成了 (s0,a0,r0,s1,a1,r1,s2,a2,r2,… ) (s_0,a_0,r_0,s_1,a_1,r_1,s_2,a_2,r_2,\\dots) (s0​,a0​,r0​,s1​,a1​,r1​,s2​,a2​,r2​,…) 传统强化学习算法: Deterministic Approach 传统强化学习算法简而言之就是 现代强化学习: Vision + Language + Action"},{"title":"Isaac Lab(Sim) 简介","path":"/wiki/simulation/isaac-lab-brief.html","content":"Assets Isaac Sim Has more built-in scenes and robots available Classic: Cartpole, Humanoid, Ant Fixed-Arm and Hands: UR10, Franka, Allegro, Shadow Hand Quadrupeds: Anybotics Anymal-B, Anymal-C, Anymal-D, Unitree A1, Unitree Go1, Unitree Go2, Boston Dynamics Spot Humanoids: Unitree H1, Unitree G1 Quadcopter: Crazyflie Procedure Isaac Lab: Manager Method: More specified control Direct Method: Similar to Maniskill. Example of Direct Method: def _get_rewards(self) - torch.Tensor: total_reward = compute_rewards( self.cfg.rew_scale_alive, self.cfg.rew_scale_terminated, self.cfg.rew_scale_pole_pos, self.cfg.rew_scale_cart_vel, self.cfg.rew_scale_pole_vel, self.joint_pos[:, self._pole_dof_idx[0]], self.joint_vel[:, self._pole_dof_idx[0]], self.joint_pos[:, self._cart_dof_idx[0]], self.joint_vel[:, self._cart_dof_idx[0]], self.reset_terminated, ) return total_reward@torch.jit.scriptdef compute_rewards( rew_scale_alive: float, rew_scale_terminated: float, rew_scale_pole_pos: float, rew_scale_cart_vel: float, rew_scale_pole_vel: float, pole_pos: torch.Tensor, pole_vel: torch.Tensor, cart_pos: torch.Tensor, cart_vel: torch.Tensor, reset_terminated: torch.Tensor,): rew_alive = rew_scale_alive * (1.0 - reset_terminated.float()) rew_termination = rew_scale_terminated * reset_terminated.float() rew_pole_pos = rew_scale_pole_pos * torch.sum(torch.square(pole_pos).unsqueeze(dim=1), dim=-1) rew_cart_vel = rew_scale_cart_vel * torch.sum(torch.abs(cart_vel).unsqueeze(dim=1), dim=-1) rew_pole_vel = rew_scale_pole_vel * torch.sum(torch.abs(pole_vel).unsqueeze(dim=1), dim=-1) total_reward = rew_alive + rew_termination + rew_pole_pos + rew_cart_vel + rew_pole_vel return total_reward Tasks"},{"title":"ManiSkill 物理仿真：编写 Tasks for RL","path":"/wiki/simulation/maniskill-testcase.html","content":"Task Components 较为繁琐的说法 Setting up the Task Class Loading (Robots, Assets, Sensors, etc.) (run once) Episode initialization / Randomization (run every env.reset) Success/Failure Condition (run every env.step) Extra Observations (run every env.step) (Optional) Dense Reward Function (run every env.step) (Optional) Setting up cameras/sensors for observations and rendering/recording (run once) 最简工作流示例 env init env.step 负责根据 Action，然后在物理仿真，模拟无理式解的变化，并计算 Reward。 env.step 简单来说，一个类包含这些元素： @register_env() 方便外部调用 class CustomEnv(BaseEnv) 使用继承，快速开发新 Testcase （成员变量）SUPPORTED_ROBOTS = [] 定义该 Testcase 里使用的 Robot （成员变量）agent: Union[...] Robot，也即 Agent Environment Class 首先，我们定义一个类继承 BaseEnv，这个类是我们初始化 Environment 的入口。同时需要调用 mani_skill.utils.registeration.register_env() 函数进行“注册”（主要是定义名称和限定最大迭代步数） @register_env(CustomEnv-v1, max_episode_steps=200)class CustomEnv(BaseEnv): 然后我们在这个环境里定义我们需要的 Agent 定义物体 位置与朝向 建议在 _load_scene() 的时候就设置一次位置与朝向，然后在 _initialize_episode() 中"},{"title":"django 表单","path":"/wiki/web_fullstack/django-forms.html","content":"django.forms.Form 类 在 Form 里定义需要填写的栏目，这样就可以直接在 template 里渲染了"},{"title":"Django 模板","path":"/wiki/web_fullstack/django-template.html","content":"Django 模板 Django 模板的作用是，可以根据数据动态地展示网页。例如，可以根据用户的权限，选择向用户展示 dashboard 或者登录界面。 与 views.py 交互"},{"title":"llama.cpp 常用 API","path":"/wiki/usefulAPIs/llama-cpp.html","content":"Preliminary: 启动一个 llama.cpp server llama-server 的 API"},{"title":"vllm 服务器 API 与示例","path":"/wiki/usefulAPIs/vllm-server.html","content":"vllm 部署 embedding 模型 vllm 部署 Vision Language Model, LLM 选用的是 Qwen/Qwen2.5-VL-3B-Instruct vllm 部署命令： $"},{"title":"Quadric Surfaces","path":"/notes/cs/graphics/quadric-surfaces.html","content":"Quadric Surfaces Quadric Surfaces 是由三元二次方程确定的曲面：ax2+by2+cz2+dxy+exz+fyz+gx+hy+lz+m=0ax^2+by^2+cz^2+dxy+exz+fyz+gx+hy+lz+m=0ax2+by2+cz2+dxy+exz+fyz+gx+hy+lz+m=0 Quadric Surfaces 包含很多特殊情况，例如椭圆体（包括球体）、椭圆抛物面、双曲抛物面、单叶双曲面、双叶双曲面、圆锥、椭圆柱面（包括圆柱体）、双曲柱面、抛物柱面。我们先研究其中的几个。 Sphere 球体 隐函数形式：x2+y2+z2−r2=0x^2+y^2+z^2-r^2=0x2+y2+z2−r2=0 参数方程： x=rcos⁡ϕsin⁡θy=rsin⁡ϕz=rcos⁡ϕcos⁡θϕ∈[−π2,π2],θ∈[−π,π] x=r\\cos\\phi\\sin\\theta\\\\ y=r\\sin\\phi\\\\ z=r\\cos\\phi\\cos\\theta\\\\ \\phi\\in[-\\frac{\\pi}{2},\\frac{\\pi}{2}], \\theta\\in[-\\pi, \\pi] x=rcosϕsinθy=rsinϕz=rcosϕcosθϕ∈[−2π​,2π​],θ∈[−π,π] 只需要看准 θ,ϕ\\theta, \\phiθ,ϕ 分别对应的是什么角即可 Epllisoid 椭圆体 隐函数形式：(xa)2+(yb)2+(zc)2−1=0(\\frac{x}{a})^2+(\\frac{y}{b})^2+(\\frac{z}{c})^2-1=0(ax​)2+(by​)2+(cz​)2−1=0 参数方程形式（相比于球体，就是把 rrr 换成三个轴上的轴长 a∗,b∗,c∗a^\\ast,b^\\ast,c^\\asta∗,b∗,c∗） x=a∗cos⁡ϕsin⁡θy=b∗sin⁡ϕz=c∗cos⁡ϕcos⁡θϕ∈[−π2,π2],θ∈[−π,π] x=a^\\ast \\cos\\phi\\sin\\theta\\\\ y=b^\\ast \\sin\\phi\\\\ z=c^\\ast \\cos\\phi\\cos\\theta\\\\ \\phi\\in[-\\frac{\\pi}{2},\\frac{\\pi}{2}], \\theta\\in[-\\pi, \\pi] x=a∗cosϕsinθy=b∗sinϕz=c∗cosϕcosθϕ∈[−2π​,2π​],θ∈[−π,π]","tags":[null]},{"title":"平面、双线性曲面","path":"/notes/cs/graphics/polygons-planes-bilinear-surfaces.html","content":"Surfaces Polygonal Representation 通常包含以下几个要素： List of vertices 顶点顺序很重要（逆时针定义，右手定则） List of polygons formed by the vertices (Optional) List of normal vectors built at the vertices Analytical Representation 如何定义一个平面 隐函数形式 Ax+By+Cz+D=0Ax+By+Cz+D=0Ax+By+Cz+D=0，法向量为 N=(A,B,C)N=(A,B,C)N=(A,B,C) 截距形式 xa+yb+zc=1\\frac{x}{a}+\\frac{y}{b}+\\frac{z}{c}=1ax​+by​+cz​=1 法向量、点式 N⋅(r−r0)=0\\bold{N}\\cdot (r-r_0)=0N⋅(r−r0​)=0，即法向量与平面上任意向量的夹角为 90°90\\degree90°. 三点式 P=P1+u(P2−P1)+v(P3−P1)P=P_1+u(P_2-P_1)+v(P_3-P_1)P=P1​+u(P2​−P1​)+v(P3​−P1​) Bilinear Surface Bilinear Surface 可以看作是两条不共面的直线所形成的曲面。 假设 Pi,i={ 1,2,3,4 }P_i,i=\\set{1,2,3,4}Pi​,i={1,2,3,4} 是四个点，令 P1,P2P_1,P_2P1​,P2​ 取自直线 aaa，P3,P4P_3,P_4P3​,P4​ 取自直线 bbb，其核心公式是 P=P1+u(P2−P1)+v(P3−P1+u(P4−P3−(P2−P1))) P=P_1+u(P_2-P_1)+v(P_3-P_1+u(P_4-P_3-(P_2-P_1))) P=P1​+u(P2​−P1​)+v(P3​−P1​+u(P4​−P3​−(P2​−P1​)))注意：双线性曲面不能完全用于定义一般平面，因为这与四个点如何选取息息相关；但是可以表示三角形（令其中的两个点重合即可）。","tags":[null]},{"title":"二维曲线与图形（一）","path":"/notes/cs/graphics/shapes-and-curves-p1.html","content":"前情提要：函数 我们有三种方式可以表达函数： Implicit Function 如 f(x,y)=0f(x,y)=0f(x,y)=0 Explicit Function 如 y=f(x),x=g(y)y=f(x), x=g(y)y=f(x),x=g(y) Parametric Function 如 x=f(u),y=g(u)x=f(u), y=g(u)x=f(u),y=g(u) 函数是如何绘制的？ 显示屏是一个个像素 直线 圆","tags":[null]},{"title":"Sweeping Surfaces","path":"/notes/cs/graphics/sweeping-surfaces.html","content":"Sweeping Curve 可以看作 Sweeping a point with 111 degree of freedom Plane 可以看作 Sweeping a point with 222 degree of freedom Bilinear 可以看作 Sweeping a segment 或者 a point with 222 degree of freedom Quadric Surface 可以看作 Sweeping/Rotating a circle Sweeping 的两个特例是 Translation,Rotation\\texttt{Translation},\\texttt{Rotation}Translation,Rotation，更加复杂的 Sweeping 需要使用矩阵进行定义。 解题通常只需要找到对应的 degree of freedom 即可，通常来说，一个可以给到待旋转的平面图形，另一个可以给到平面图形中心的运动轨迹。 然后分别找出其定义域，再 Linear Interpolate 到 [0,1][0,1][0,1] 上即可。","tags":[null]},{"title":"Canny 边缘检测算法","path":"/notes/cs/vision/canny-algorithm.html","content":"Derivatives of Gaussian Non-Maximal Suppression Hypothesis Thresholding","tags":[null]},{"title":"边缘处理","path":"/notes/cs/vision/edge-processing.html","content":"为什么需要进行 edge detection? 从像素进阶到 features edge 意味着变化，而变化意味着信息 什么是 Edge? gradient (梯度) becomes large, and curvature (曲率) becomes small 通常包含几个步骤： Edge Filtering Edge Detection Edge Linking Prewitt 算子与 Sobel 算子 Prewitt 算子和 Sobel 算子是卷积核，可以有效提取 Gradient 较大的区域 Prewitt 算子： Prewitt1=[−1−1−1000111]Prewitt2=[−101−101−101] \\text{Prewitt}_1=\\begin{bmatrix} -1 -1 -1\\\\ 0 0 0\\\\ 1 1 1 \\end{bmatrix}\\\\ \\text{Prewitt}_2=\\begin{bmatrix} -1 0 1\\\\ -1 0 1\\\\ -1 0 1 \\end{bmatrix} Prewitt1​=​−101​−101​−101​​Prewitt2​=​−1−1−1​000​111​​Sobel 算子： Sobel1=[−1−2−1000121]Sobel2=[−101−202−201] \\text{Sobel}_1=\\begin{bmatrix} -1 -2 -1\\\\ 0 0 0\\\\ 1 2 1 \\end{bmatrix}\\\\ \\text{Sobel}_2=\\begin{bmatrix} -1 0 1\\\\ -2 0 2\\\\ -2 0 1 \\end{bmatrix} Sobel1​=​−101​−202​−101​​Sobel2​=​−1−2−2​000​121​​ 优势 简单有效 劣势 对噪声敏感 边缘粗细不一致、strength 不一致 Laplacian of Gaussian (LoG) 我们先对图像应用 Gaussian Blur 进行平滑处理以降低 Laplacian 操作对于噪声的敏感性 我们首先对图像进行 Gaussian Blur，式子可以写作 h(r)=−exp⁡(−r22σ2),r2=x2+y2 h(r)=-\\exp\\Big(-\\frac{r^2}{2\\sigma^2}\\Big), r^2=x^2+y^2 h(r)=−exp(−2σ2r2​),r2=x2+y2接着，对 Gaussian Blur 过的图像进行 Laplacian 操作 ∇2h(r)=∂2h∂x2+∂2h∂y2=−r2−2σ2σ4⋅exp⁡(−r22σ2) abla^2 h(r)=\\frac{\\partial^2 h}{\\partial x^2}+\\frac{\\partial^2 h}{\\partial y^2}=-\\frac{r^2-2\\sigma^2}{\\sigma^4}\\cdot \\exp\\Big(-\\frac{r^2}{2\\sigma^2}\\Big) ∇2h(r)=∂x2∂2h​+∂y2∂2h​=−σ4r2−2σ2​⋅exp(−2σ2r2​)不过真正在计算的时候，我们通常用一个卷积核来近似 LoG 操作，例如 LoG1=[0−10−14−10−10]LoG2=[−1−1−1−18−1−1−1−1] \\text{LoG}_1=\\begin{bmatrix} 0 -1 0\\\\ -1 4 -1 \\\\ 0 -1 0 \\end{bmatrix}\\quad \\text{LoG}_2=\\begin{bmatrix} -1 -1 -1\\\\ -1 8 -1 \\\\ -1 -1 -1 \\end{bmatrix} LoG1​=​0−10​−14−1​0−10​​LoG2​=​−1−1−1​−18−1​−1−1−1​​ Canny Edge Detector"},{"title":"Hough 变换算法","path":"/notes/cs/vision/hough-transform.html","content":"直线的极座标表示 考虑一条直线 ax+by+c=0ax+by+c=0ax+by+c=0，过原点 OOO 做这条直线的垂线交于 NNN。不妨令 NNN 点的极座标为 (rN,θN)(r_N, \\theta_N)(rN​,θN​)，再任取直线上的一点 P(x,y)P(x,y)P(x,y)，显然应该有 ON⊥NPON\\perp NPON⊥NP，所以两向量垂直，故 ON⃗⋅NP⃗=0 \\vec{ON}\\cdot \\vec{NP}=0 ON⋅NP=0代入 ON⃗=(rNcos⁡(θN),rNsin⁡(θN)),NP⃗=(x−rNcos⁡(θN),y−rNsin⁡(θN))\\vec{ON}=(r_N\\cos(\\theta_N), r_N\\sin(\\theta_N)), \\vec{NP}=(x-r_N\\cos(\\theta_N), y-r_N\\sin(\\theta_N))ON=(rN​cos(θN​),rN​sin(θN​)),NP=(x−rN​cos(θN​),y−rN​sin(θN​))，化简并代入点乘公式有 rNcos⁡(θN)⋅(x−rNcos⁡(θN))+rNsin⁡(θN)⋅(y−rNsin⁡(θN))=0x⋅rNcos⁡(θN)−rN2cos⁡2(θN)+y⋅rNsin⁡(θN)−rN2sin⁡2(θN)=0x⋅rNcos⁡(θN)+y⋅rNsin⁡(θN)=rN2xcos⁡(θN)+ysin⁡(θN)=rN \\begin{aligned} r_N \\cos(\\theta_N)\\cdot\\Big( x-r_N\\cos(\\theta_N) \\Big) + r_N\\sin(\\theta_N)\\cdot\\Big( y-r_N\\sin(\\theta_N) \\Big)=0\\\\ x\\cdot r_N\\cos(\\theta_N)-r_N^2\\cos^2(\\theta_N) + y\\cdot r_N\\sin(\\theta_N) - r_N^2\\sin^2(\\theta_N)=0\\\\ x\\cdot r_N\\cos(\\theta_N) + y\\cdot r_N\\sin(\\theta_N)=r_N^2\\\\ x\\cos(\\theta_N) + y\\sin(\\theta_N)=r_N \\end{aligned} rN​cos(θN​)⋅(x−rN​cos(θN​))+rN​sin(θN​)⋅(y−rN​sin(θN​))x⋅rN​cos(θN​)−rN2​cos2(θN​)+y⋅rN​sin(θN​)−rN2​sin2(θN​)x⋅rN​cos(θN​)+y⋅rN​sin(θN​)xcos(θN​)+ysin(θN​)​=0=0=rN2​=rN​​所以直线的表达式是 xcos⁡θ+ysin⁡θ=r \\boxed{x\\cos\\theta + y\\sin\\theta=r} xcosθ+ysinθ=r​ Transform from Image Space to Hough Space 令 (x0,y0)(x_0,y_0)(x0​,y0​) 为图像上一点，对于图像空间来说，横轴为 xxx，纵轴为 yyy. 我们找出所有 (θ,r)(\\theta,r)(θ,r) 满足 x0cos⁡θ+y0sin⁡θ=r x_0\\cos\\theta + y_0\\sin\\theta=r x0​cosθ+y0​sinθ=r把这些点标在 Hough Space 中。Hough Space 的横轴为 θ\\thetaθ，纵轴为 rrr. 那么满足条件的 (θ,r)(\\theta,r)(θ,r) 在 Hough Space 上的图像为正弦函数的一部分。 这是因为，我们可以把直线的极座标表达式转化成 1x02+y02sin⁡(θ+φ)=r\\frac{1}{\\sqrt{x_0^2+y_0^2}}\\sin(\\theta+\\varphi)=rx02​+y02​​1​sin(θ+φ)=r所以 Hough Space 上的曲线是正弦函数. Curve Voting 对于每一个点 (x,y)(x,y)(x,y)，计算所有 (θ,r)(\\theta,r)(θ,r)，然后在计数器 AAA 里 +1，即 A(θ,r)←A(θ,r)+1A(\\theta, r)\\gets A(\\theta, r)+1A(θ,r)←A(θ,r)+1，最后，我们取得票数最多的 (θ,r)(\\theta, r)(θ,r) 视作 Linked Edge. 这个就类似于，我们对每一个点计算它可能属于什么直线，然后对所有点找出那条公共的直线。"},{"title":"Imaging System 成像系统","path":"/notes/cs/vision/imaging-system.html","content":"人眼与相机 棱镜成像 基本公式，令焦距 Focal Length 为 fff，物距（到棱镜中心的距离）为 Z^\\hat ZZ^，像距为 z^\\hat zz^，则有 1Z^+1z^=1f\\frac{1}{\\hat Z}+\\frac{1}{\\hat z}=\\frac{1}{f}Z^1​+z^1​=f1​同时，令物体高度为 YYY，宽度为 XXX，像的高度为 yyy，宽度为 xxx，则有 XZ^=xz^YZ^=yz^ \\boxed{\\frac{X}{\\hat Z}=\\frac{x}{\\hat z}}\\\\ \\boxed{\\frac{Y}{\\hat Z}=\\frac{y}{\\hat z}} Z^X​=z^x​​Z^Y​=z^y​​ Depth of Field 景深 Refers to the range of depths that scene objects can be at for acceptable imaging. 如下图，考虑固定物体和棱镜，如果成像平面没有摆在 ideal image point，那么物体 PPP 上的某一个点，经过棱镜折射之后，在平面上就会形成一段区间。区间越大，成像越模糊。 所以我们希望调整我们的成像平面的位置，使得对于所有物体，其模糊程度都在可以接受的范围内。 相机成像 Charged-Coupled Device (CCD) Array 相机的传感器是一个二维的矩阵，每一个小格子上装有滤镜（拜耳滤镜，蓝绿绿红）和光电二极管。当光线射入的时候，每一个格子能够过滤出特定颜色的光线，并且输出电压，电压的大小与曝光成正比。例如图中的蓝色格子可以输出电压，从而反应蓝光的强度。 图中的拜耳滤镜是为了输出彩色图片；也可以使用 3CCD 系统得到更好的质量。 3CCD 3CCD 是首先利用分光镜将光线分成红、绿、蓝三股，然后用 333 片 CCD 分别处理这三色光。可以得到更好的质量。之所以说拜耳滤镜的图像质量不太行，是因为因为每个像素只有单个颜色光的亮度，所以为了计算 RGB，我们必须使用 interpolation 算法计算得出其他颜色在这一个像素点的值，这就导致了误差。 Spatial Sampling 先假设 x,yx,yx,y 是连续的，那么光强关于坐标的函数 f(x,y)f(x,y)f(x,y) 应该是连续的。但是像素是离散的，于是令 x,y∈Nx,y\\in \\Nx,y∈N，这个过程可以看作是在连续的函数图像上取离散的点，因此被称作 Spatial Sampling。那么可以推出，像素越多，越接近连续采样，清晰度也就越高。 Grey-Level Quantization 电压值可能是浮点数，我们把它转化成整数，例如 888 位整数。位数更高，分辨率也越高。 Image Matrix 就像 OpenCV 储存图像那样~ Image File Formats","tags":[null]},{"title":"Image Enhancement in Frequency Domain 频域上的图像处理","path":"/notes/cs/vision/img-enhancement-in-frequency-domain.html","content":"频域上的图像分解 Why Another Domain? Fourier Transform f(x,y)=1MN∑v=0N−1∑u=0M−1F(u,v)exp⁡(j⋅2π(xuM+yvN)) f(x,y)=\\frac{1}{MN}\\sum_{v=0}^{N-1}\\sum_{u=0}^{M-1} F(u,v) \\exp\\Big( j\\cdot 2\\pi (\\frac{xu}{M}+\\frac{yv}{N}) \\Big) f(x,y)=MN1​v=0∑N−1​u=0∑M−1​F(u,v)exp(j⋅2π(Mxu​+Nyv​)) Spectrum 光谱 我们称 F(u,v)F(u,v)F(u,v) 为 Spectrum，其中 Low/Large (u,v)(u,v)(u,v) 表示 Low Frequency Median (u,v)(u,v)(u,v) 表示 High Frequency 低频高频意味着什么？ Component Reflect… Low Freq Slow changing part; Trend; Overall Information High Freq Details; Jump; Sudden change Signal (usually) strong low freq Noise All freq FT 衍生出来的图片处理框架 graph TB A[\"Input Image f(x,y)\"] --> B[Pre-processing] --> C[\"Forward FT, F(u,v)\"] --> D[\"Filter Function, H(u,v)\"] D --> E[\"Convolution, H(u,v)F(u,v)\"] --> F[Inverse FT] --> G[Post-processing] --> H[\"Enhanced Image g(x,y)\"] Low-Pass Filter 低通滤波 High-Pass Filter 高通滤波","tags":[null]},{"title":"Image Enhancement in Spatial Domain 空间域上的图像增强","path":"/notes/cs/vision/img-enhancement-in-spatial-domain.html","content":"Pointwise Processing 像素处理 Contrast Stretching 对比度拉伸 通过线性插值，将设定的 [rmin⁡,rmax⁡][r_{\\min}, r_{\\max}][rmin​,rmax​] 拉伸到 [0,255][0,255][0,255]。常用于相机设置出错、光源不足等情况 r′={0if r≤rmin⁡255(r−rmin⁡)rmax⁡−rmin⁡if rmin⁡rrmax⁡255if rmax⁡≤r r=\\begin{cases} 0\\text{if \\(r\\le r_{\\min}\\)}\\\\ \\frac{255(r-r_{\\min})}{r_{\\max}-r_{\\min}} \\text{if \\(r_{\\min}\\lt r\\lt r_{\\max}\\)}\\\\ 255\\text{if \\(r_{\\max}\\le r\\)} \\end{cases} r′=⎩⎨⎧​0rmax​−rmin​255(r−rmin​)​255​if r≤rmin​if rmin​rrmax​if rmax​≤r​这个插值函数在 [rmin⁡,rmax⁡][r_{\\min}, r_{\\max}][rmin​,rmax​] 区间内获得更多的对比度变化，但是 [0,rmin⁡],[rmax⁡,255][0,r_{\\min}],[r_{\\max},255][0,rmin​],[rmax​,255] 之间的灰度直接被抹除了，失去了这些灰度区间内的对比度信息。 Power-Law Transformations 函数表示为 r′=c⋅rγ r=c\\cdot r^\\gamma r′=c⋅rγ其中 c,γc,\\gammac,γ 为常数。假设我们知道了 r,r′r,rr,r′ 的值域，我们还可以将 ccc 表示为 γ\\gammaγ 和值域 LLL 的函数。 通常来说，当 γ\\gammaγ 越大，图像越暗，牺牲了暗部的辨识度，提高亮部的辨识度；当 γ\\gammaγ 越小，图像越亮，牺牲亮部的辨识度，提升暗部的辨识度。 Histogram Equalization Spatial Filters 空间滤波 Convolution 卷积 Smoothing Filter 平滑滤波 Median Filter 中值滤波","tags":[null,null]},{"title":"极角排序","path":"/notes/acm/geometry/angular-sorting.html","content":"极角排序 方法一：使用角度排序 方法二：上下平面 + 叉积排序 相比第一种方法而言，这种方法的精度更高，因为不涉及浮点数之间的运算，也更加符合点的坐标都是整数的情况。 具体地来说，我们把向量分成两部分： 上半面：y0y\\gt 0y0 或者 y=0 and x≥0y=0 \\texttt{ and } x\\ge 0y=0 and x≥0 下半面：y0y\\lt 0y0 或者 y=0 and x0y=0 \\texttt{ and } x\\lt 0y=0 and x0 对于每一个部分，我们用叉积判断向量 aaa 是否在 bbb 的逆时针位置，然后将排完序后的下半面向量与上半面向量直接 concat 起来. 叉积法极角排序 int is_neg(vect a) return (a.y 0) || (a.y == 0 a.x 0);// 这里的 cmp 函数表示，从 theta=0 转到 theta=2pibool cmp(vect a, vect b) int wa = is_neg(a), wb = is_neg(b); if (wa != wb) return wa wb; else return a.cross(b) = 0;","tags":[null]},{"title":"WBLT 平衡树","path":"/notes/acm/ds/weight-balanced-leafy-tree.html","content":"Weight Balanced Leafy Tree Template Code Simple Code for Maintaining Sequence 指针风格（容量大、但速度慢） class WBLT using T = int; public: struct node int size; T val, sum; std::arraystd::unique_ptrnode, 2 ch; node() : size0, val0, sum0, chnullptr, nullptr node(T v) : size1, valv, sumv, chnullptr, nullptr ; using pointer = std::unique_ptrnode; public: friend WBLT merge(WBLT a, WBLT b) WBLT res; res.root = a.merge(a.root, b.root); return res; void merge(WBLT b) root = merge(root, b.root); /** * @brief Split the current WBLT into two WBLTs, with size k (returned) and n-k (current) */ WBLT split(int k) WBLT res; std::tie(res.root, root) = split(root, k); return res; /** * @brief Insert a value into the WBLT by its rank, ans assume its the k-th element. (1-indexed) */ void insertByRank(int k, T val) if (k == 1 || root == nullptr) auto p = newLeaf(val); root = merge(p, root); return; else if (k == root-size + 1) auto p = newLeaf(val); root = merge(root, p); return; assert(k = root-size + 1 k = 1); auto [left, right] = split(root, k - 1); auto p = newLeaf(val); root = merge(left, p = merge(p, right)); /** * @brief Get a value from the WBLT by its rank. (1-indexed) */ T getByRank(int kth) assert(kth = root-size kth = 1); auto [left, right] = split(root, kth - 1); auto [mid, newRight] = split(right, 1); T f = mid-val; root = merge(left, right = merge(mid, newRight)); return f; /** * @brief Build WBLT from a vector or array */ template typename U void build(const U vector, int l, int r) _build(vector, l, r, root); /** * @brief Get the root of WBLT */ pointer getRoot() return root; protected: pointer newNode() return std::make_uniquenode(); void deleteNode(pointer p) p = nullptr; pointer newLeaf(T val) return std::make_uniquenode(val); pointer join(pointer left, pointer right) pointer root = newNode(); root-ch[0] = std::move(left), root-ch[1] = std::move(right); pushUp(root); return root; std::pairpointer, pointer cut(pointer p) if (p == nullptr) return nullptr, nullptr; pointer left = std::move(p-ch[0]), right = std::move(p-ch[1]); deleteNode(p); return std::move(left), std::move(right); void rotate(pointer rt, int r) auto [a, b] = cut(rt); if (r) auto [c, d] = cut(b); rt = join(b = join(a, c), d); else auto [c, d] = cut(a); rt = join(c, a = join(d, b)); void pushUp(pointer p) assert(p-ch[0] p-ch[1]); p-size = p-ch[0]-size + p-ch[1]-size; p-val = p-ch[1]-val; p-sum = p-ch[0]-sum + p-ch[1]-sum; // functions to help WBLT keep balanced bool heavy(int sx, int sy) return sx sy * 3; bool doubleRotationRequired(pointer rt, int r) return rt-ch[!r]-size rt-ch[r]-size * 2; void balance(pointer rt) if (rt-size == 1) return; if (heavy(rt-ch[0]-size, rt-ch[1]-size) || heavy(rt-ch[1]-size, rt-ch[0]-size)) auto [a, b] = cut(rt); rt = merge(a, b); // functions to build WBLT template typename U void _build(const U vector, int l, int r, pointer rt) if (rt == nullptr) rt = newNode(); if (l == r) rt = newLeaf(vector[l]); return; int mid = l + ((r - l) 1); _build(vector, l, mid, rt-ch[0]); _build(vector, mid + 1, r, rt-ch[1]); pushUp(rt); /** * @brief Split the WBLT into two WBLTs, with size k and n-k. * @warning This operation invalidates the current root. */ std::pairpointer, pointer split(pointer rt, int k) if (rt == nullptr) return nullptr, nullptr; if (k = 0) return nullptr, std::move(rt); if (k = rt-size) return std::move(rt), nullptr; auto [left, right] = cut(rt); if (k = left-size) auto [l, r] = split(left, k); return std::move(l), merge(r, right); else auto [l, r] = split(right, k - left-size); return merge(left, l), std::move(r); /** * @brief Merge two WBLTs into one WBLT (without re-ordering) */ pointer merge(pointer left, pointer right) if (left == nullptr || right == nullptr) return left == nullptr ? std::move(right) : std::move(left); if (heavy(left-size, right-size)) auto [a, b] = cut(left); if (heavy(b-size + right-size, a-size)) auto [c, d] = cut(b); return merge(left = merge(a, c), b = merge(d, right)); else return merge(a, left = merge(b, right)); else if (heavy(right-size, left-size)) auto [a, b] = cut(right); if (heavy(a-size + left-size, b-size)) auto [c, d] = cut(a); return merge(right = merge(left, c), a = merge(d, b)); else return merge(right = merge(left, a), b); else return std::move(join(left, right)); private: pointer rootnullptr;; 数组风格（速度更快，但是 std::array 受内存制约，容量小） class WBLT using T = int; using index = int; static constexpr int CacheSize = 10e5 + 10; public: struct node int size; T val, sum; std::arrayindex, 2 ch; node() : size0, val0, sum0, ch0, 0 node(T v) : size1, valv, sumv, ch0, 0 ; public: /// @brief Access the specific element in the tree node operator[](index i) return pool[i]; /// @brief Split the current WBLT into two WBLTs, with size k (returned) and n-k (current) index split(int k) int res; std::tie(res, root) = split(root, k); return res; index merge(index x, index y) if (x == 0 || y == 0) return x == 0 ? y : x; if (heavy(pool[x].size, pool[y].size)) auto [a, b] = cut(x); if (heavy(pool[b].size + pool[y].size, pool[a].size)) auto [c, d] = cut(b); return merge(merge(a, c), merge(d, y)); else return merge(a, merge(b, y)); else if (heavy(pool[y].size, pool[x].size)) auto [a, b] = cut(y); if (heavy(pool[a].size + pool[x].size, pool[b].size)) auto [c, d] = cut(a); return merge(merge(x, c), merge(d, b)); else return merge(merge(x, a), b); else return join(x, y); void merge(int b) root = merge(root, b); index getRoot() const return root; void insertByRank(int k, T val) if (k == 1 || root == 0) auto p = newLeaf(val); root = merge(p, root); return; else if (k == pool[root].size + 1) auto p = newLeaf(val); root = merge(root, p); return; assert(k = pool[root].size + 1 k = 1); auto [left, right] = split(root, k - 1); auto p = newLeaf(val); root = merge(left, merge(p, right)); T getByRank(int kth) assert(kth = pool[root].size kth = 1); auto [left, right] = split(root, kth - 1); auto [mid, newRight] = split(right, 1); T f = pool[mid].val; root = merge(left, merge(mid, newRight)); return f; template typename U void build(U vector, int l, int r) root = _build(vector, l, r); protected: index newNode() if (trash.empty()) return ++poolid; else int res = trash.back(); trash.pop_back(); return res; void deleteNode(index p) trash.push_back(p), pool[p] = node(), p = 0; index newLeaf(T val) index p = newNode(); pool[p] = node(val); return p; /// @brief Attach two nodes to a new root index join(index left, index right) index nroot = newNode(); pool[nroot].ch[0] = left, pool[nroot].ch[1] = right; pushUp(nroot); return nroot; /// @brief Remove the parent node and return its children std::pairindex, index cut(index p) auto [y, z] = pool[p].ch; deleteNode(p); return y, z; /// @brief Single-rotation, r = 0 for left, r = 1 for right. void rotate(index rt, int r) auto [a, b] = cut(rt); if (r) auto [c, d] = cut(b); rt = join(join(a, c), d); else auto [c, d] = cut(a); rt = join(c, join(d, b)); bool heavy(int sx, int sy) return sx sy * 3; bool doubleRotationRequired(index rt, int r) return pool[rt].ch[!r] pool[pool[rt].ch[!r]].size pool[pool[rt].ch[r]].size * 2; void balance(index rt) if (pool[rt].size == 1) return; if (heavy(pool[pool[rt].ch[0]].size, pool[pool[rt].ch[1]].size) || heavy(pool[pool[rt].ch[1]].size, pool[pool[rt].ch[0]].size)) auto [a, b] = cut(rt); rt = merge(a, b); /// @brief Maintain the supporting info in WBLT non-leaf node. void pushUp(index p) assert(pool[p].ch[0] pool[p].ch[1]); index ls = pool[p].ch[0], rs = pool[p].ch[1]; pool[p].size = pool[ls].size + pool[rs].size, pool[p].val = pool[rs].val; pool[p].sum = pool[ls].sum + pool[rs].sum; std::pairindex, index split(int x, int k) if (x == 0) return 0, 0; if (k = 0) return 0, x; if (k = pool[x].size) return x, 0; auto [left, right] = cut(x); if (k = pool[left].size) auto [l, r] = split(left, k); return l, merge(r, right); else auto [l, r] = split(right, k - pool[left].size); return merge(left, l), r; // helper function to build WBLT template typename U index _build(const U vector, int l, int r) if (l == r) return newLeaf(vector[l]); int mid = l + ((r - l) 1); return join(_build(vector, l, mid), _build(vector, mid + 1, r)); private: index root0, poolid0; // std::arraynode, CacheSize pool; std::vectornode pool = std::vectornode(CacheSize); std::vectorint trash;;","tags":[null]},{"title":"FFT","path":"/notes/acm/polynomials/FFT.html","content":"Fast Fourier Transform 多项式的点值表示法 Fact nnn 个点可以唯一表示 degree 为 n−1n-1n−1 的多项式 f(x)=a0+a1x+a2x2+⋯+an−1xn−1f(x)=a_0+a_1x+a_2x^2+\\dots +a_{n-1}x^{n-1}f(x)=a0​+a1​x+a2​x2+⋯+an−1​xn−1. 所以，两个度数为 n−1n-1n−1 的多项式 f(x),g(x)f(x),g(x)f(x),g(x)，其乘积 h(x)h(x)h(x) 为 2n−22n-22n−2 度的多项式 h(x)=∑i=02n−2bixih(x)=\\sum_{i=0}^{2n-2} b_ix^ih(x)=∑i=02n−2​bi​xi h(x)h(x)h(x) 的点值表示法需要 2n−12n-12n−1 个点，那么为了从 f(x),g(x)f(x),g(x)f(x),g(x) 计算出 h(x)h(x)h(x)，我们也需要在 f(x),g(x)f(x),g(x)f(x),g(x) 上取 2n−12n-12n−1 个点。 而朴素的多项式乘法需要 O(n2)O(n^2)O(n2)，所以，只要我们可以用比 O(n2)O(n^2)O(n2) 的方式计算出 f(x),g(x)f(x),g(x)f(x),g(x) 上的 2n−12n-12n−1 个点 (xi,f(xi)),(xi,g(xi))(x_i,f(x_i)),(x_i,g(x_i))(xi​,f(xi​)),(xi​,g(xi​))，然后再以 O(n)O(n)O(n) 把点乘起来 (xi,f(xi)⋅g(xi))(x_i,f(x_i)\\cdot g(x_i))(xi​,f(xi​)⋅g(xi​)) 得到 (xi,h(xi))(x_i,h(x_i))(xi​,h(xi​))，再将 h(x)h(x)h(x) 上的 2n−12n-12n−1 个点转化成系数表示法，就大功告成了！ 所以我们的思路是 graph LR; A[Coefficient Representation] -->|FFT Forward| B[Point-Value Representation] B -->|FFT Inverse| A 用线性对数时间复杂度计算 Point-Value Representation 怎么以 O(nlog⁡n)O(n\\log n)O(nlogn) 计算 f(x)f(x)f(x) 的 2n−12n-12n−1 个点值呢？考虑分治，令 f(x)f(x)f(x) 的 Coefficient Representation 为 f(x)=a0+a1x+a2x2+a3x3+…an−1xn−1 f(x)=a_0+a_1x+a_2x^2+a_3x^3+\\dots a_{n-1}x^{n-1} f(x)=a0​+a1​x+a2​x2+a3​x3+…an−1​xn−1把 f(x)f(x)f(x) 的系数分成偶数项和奇数项： (a0,a2,…,an−2) ⟹ f0(x)=a0+a2x+a4x2+⋯+an−2xn2−1(a1,a3,…,an−1) ⟹ f1(x)=a1+a3x+a5x2+⋯+an−1xn2−1 (a_0,a_2,\\dots,a_{n-2})\\implies f_0(x)=a_0+a_2x+a_4x^2+\\dots +a_{n-2}x^{\\frac{n}{2}-1}\\\\ (a_1,a_3,\\dots,a_{n-1})\\implies f_1(x)=a_1+a_3x+a_5x^2+\\dots +a_{n-1}x^{\\frac{n}{2}-1} (a0​,a2​,…,an−2​)⟹f0​(x)=a0​+a2​x+a4​x2+⋯+an−2​x2n​−1(a1​,a3​,…,an−1​)⟹f1​(x)=a1​+a3​x+a5​x2+⋯+an−1​x2n​−1所以我们有 f(x)=f0(x2)+x⋅f1(x2) f(x) = f_0(x^2) + x\\cdot f_1(x^2) f(x)=f0​(x2)+x⋅f1​(x2)考虑两个点 x1,x2x_1,x_2x1​,x2​，如果我们不用任何优化，我们一共需要计算 444 个值，才能计算出两个点值：f0(x12),f1(x12),f0(x22),f1(x22)f_0(x_1^2),f_1(x_1^2),f_0(x_2^2),f_1(x_2^2)f0​(x12​),f1​(x12​),f0​(x22​),f1​(x22​). 然而，如果我们令 x1=−x2x_1=-x_2x1​=−x2​，也就是说 x12=x22x_1^2=x_2^2x12​=x22​，那么我们只需要计算 222 个值，就可以计算出两个点值了：f0(x12),f1(x12)f_0(x_1^2),f_1(x_1^2)f0​(x12​),f1​(x12​). 所以，令 T(n)T(n)T(n) 表示计算 degree 为 nnn 的 f(x)f(x)f(x) 的点值，则 T(n)=2T(n/2)+O(n)T(n)=O(nlog⁡n) T(n)=2T(n/2)+O(n)\\\\ T(n)=O(n\\log n) T(n)=2T(n/2)+O(n)T(n)=O(nlogn) 引入复数 上一节我们已经知道了，考虑 nnn 个值 x0,x1,x2…,xn−1x_0,x_1,x_2\\dots, x_{n-1}x0​,x1​,x2​…,xn−1​ 需要满足 x0=−xn/2x1=−xn/2+1…xn/2−1=−xn−1 x_0=-x_{n/2}\\\\ x_1=-x_{n/2+1}\\\\ \\dots\\\\ x_{n/2-1}=-x_{n-1} x0​=−xn/2​x1​=−xn/2+1​…xn/2−1​=−xn−1​然后进入递归，我们就需要对 n/2n/2n/2 个值 x02,x12,x22…xn/2−12x_0^2,x_1^2,x_2^2\\dots x_{n/2-1}^2x02​,x12​,x22​…xn/2−12​ 对应的点值 f(x02),f(x12),f(x22)…,f(xn/2−12)f(x_0^2),f(x_1^2),f(x_2^2)\\dots ,f(x_{n/2-1}^2)f(x02​),f(x12​),f(x22​)…,f(xn/2−12​) 类似的，为了进一步递归下去，这 n/2n/2n/2 个值也应该满足 x02=−xn/42x12=−xn/4+12…xn/4−12=−xn/2−12 x_0^2=-x_{n/4}^2\\\\ x_1^2=-x_{n/4+1}^2\\\\ \\dots\\\\ x_{n/4-1}^2=-x_{n/2-1}^2\\\\ x02​=−xn/42​x12​=−xn/4+12​…xn/4−12​=−xn/2−12​这里就必须引入复数了（不然只能全部等于 000）。","tags":[null]},{"title":"二项式反演","path":"/notes/acm/polynomials/binomial-inversion.html","content":"二项式反演 二项式反演有两种常见的形式 形式一 令 fif_ifi​ 表示恰好使用 iii 个元素、且满足条件的方案数，gng_ngn​ 表示从 nnn 个元素选出 i(i∈[0,n])i(i\\in[0,n])i(i∈[0,n]) 个元素、且满足条件的总方案数，则根据定义有 另一种理解方法：gng_ngn​ 表示至多 nnn 个元素的方案数，fnf_nfn​ 表示恰好 nnn 个元素的方案数 gn=∑i=0n(ni)fi g_n=\\sum_{i=0}^n \\binom{n}{i} f_i gn​=i=0∑n​(in​)fi​通过二项式反演，我们可以通过 gig_igi​ 求出 fif_ifi​. fn=∑i=0n(ni)(−1)n−igi f_n=\\sum_{i=0}^n\\binom{n}{i}(-1)^{n-i} g_i fn​=i=0∑n​(in​)(−1)n−igi​ 证明 形式二 另一种理解方法：gng_ngn​ 表示至少 nnn 个元素的方案数，fnf_nfn​ 恰好 nnn 个元素的方案数 gk=∑i=kn(ik)fi g_k=\\sum_{i=k}^n \\binom{i}{k} f_i gk​=i=k∑n​(ki​)fi​通过二项式反演，我们可以通过 gng_ngn​ 求出 fnf_nfn​ fk=∑i=kn(−1)i−k(ik)gi f_k=\\sum_{i=k}^n (-1)^{i-k} \\binom{i}{k} g_i fk​=i=k∑n​(−1)i−k(ki​)gi​","tags":[null]},{"title":"点分治、点分树 Centroid Decomposition","path":"/notes/acm/tree/centroid-decomposition.html","content":"点分治 点分树 我们重新考虑点分治的过程，并把上一层的重心和下一层的重心连起来，这样就形成了点分树。","tags":[null]},{"title":"轻重链剖分","path":"/notes/acm/tree/hld.html","content":"轻重链剖分 例题","tags":[null,null]},{"title":"分块思想","path":"/notes/acm/tricks/acm-chunkify.html","content":"序列上的分块 树上分块 时间复杂度优化的 Tricks 1. 复杂度瓶颈在清空标记上 假设我们有 O(m)O(m)O(m) 的标记需要清空（比如说用于计数的桶这种，需要在 testcase 开始之前清空的），但是一共有 O(n)O(n)O(n) 轮。如果每一轮结束都暴力清空，那么时间复杂度会来到 O(nm)O(nm)O(nm) 通常是不可承受的。 这时，我们定义数组 tag[i]\\texttt{tag[\\(i\\)]}tag[i] 记录标记 iii 最后一次使用是第几个 testcase。那么这样，如果当前 testcase 的 id 和 tag[i]\\texttt{tag[\\(i\\)]}tag[i] 记录的不一样，就可以直接清空；否则就可以正常的进行操作了。 例题 序列分块 CF121E - Lucky Array 我们发现要“维护 4,7,44,…4,7,44,\\dots4,7,44,… 等特殊数字的个数”这个任务很难用线段树操作，所以转而尝试分块。假设块长为 BBB，分了 n/Bn/Bn/B 块。 因为 ai≤104a_i\\le 10^4ai​≤104 且这样的 lucky number 的数量只有 303030 多个，所以我们在块上开 10410^4104 个桶记录值域，即 bucket[v]=∣{i:ai=v}∣\\text{bucket}[v]=|\\{ i: a_i=v \\}|bucket[v]=∣{i:ai​=v}∣ 这里还有区间加的操作，我们在块上维护 tag 表示区间累计加了多少，查询 lucky number 个数的时候相应减去即可。时间复杂度 O(nn),B=nO(n\\sqrt n),B=\\sqrt nO(nn​),B=n​ AC Code 带着 tag 查询 lucky number 的时候需要注意 value = tag 这个问题，不然数组会越界。 #include bits/extc++.h#include bits/stdc++.hconstexpr int N = 1e5 + 10;constexpr int V = 1e4 + 10;constexpr int B = 400;std::vectorint P;int ck[V];int n, m;int a[N];int L[B], R[B], belong[N], bsize, bnum;int map[B][V], tag[B];void init_ck() for (int b = 1; b = 4; b++) for (int i = 0; i (1 b); i++) int t = 0; for (int j = 0; j b; j++) if (i (1 j)) t = t * 10 + 4; else t = t * 10 + 7; if (t V) ck[t] = 1; P.push_back(t); int check(int x) return ck[x]; void rebuild(int l, int r, int v) int id = belong[l]; if (tag[id] == 0) if (v == 0) return; for (int i = l; i = r; i++) map[id][a[i]]--; a[i] += v; map[id][a[i]]++; else for (int i = L[id]; i = R[id]; i++) map[id][a[i]]--; a[i] += tag[id]; if (l = i i = r) a[i] += v; map[id][a[i]]++; tag[id] = 0; void add(int l, int r, int v) int s = belong[l], e = belong[r]; if (s == e) rebuild(l, r, v); else rebuild(l, R[s], v); rebuild(L[e], r, v); for (int i = s + 1; i e; i++) tag[i] += v; int query(int l, int r) int s = belong[l], e = belong[r]; int ans = 0; if (s == e) rebuild(l, r, 0); for (int i = l; i = r; i++) ans += ck[a[i]]; else rebuild(l, R[s], 0); rebuild(L[e], r, 0); for (int i = l; i = R[s]; i++) ans += ck[a[i]]; for (int i = L[e]; i = r; i++) ans += ck[a[i]]; for (int i = s + 1; i e; i++) for (int p : P) if (p = tag[i]) ans += map[i][p - tag[i]]; return ans;int main() std::cin.tie(nullptr)-sync_with_stdio(false); std::cout.tie(nullptr); init_ck(); std::cin n m; bsize = std::sqrt(P.size() * n); bnum = (n + bsize - 1) / bsize; for (int i = 1; i = bnum; i++) L[i] = (i - 1) * bsize + 1; R[i] = std::min(i * bsize, n); for (int j = L[i]; j = R[i]; j++) belong[j] = i; for (int i = 1; i = n; i++) std::cin a[i]; map[belong[i]][a[i]]++; std::string op; int l, r, v; while (m--) std::cin op l r; if (op == add) std::cin v; add(l, r, v); else std::cout query(l, r) ; LOJ#6282. 数列分块入门 6 当然可以用平衡树过题，不过我们还是来看一看用分块怎么做. 以下 N,QN,QN,Q 同阶 我们令块长为 B=NB=\\sqrt NB=N​，则共 N/BN/BN/B 块。在每一个块上，维护 std::vector 保存块内的数，这些块的 vector 按顺序拼接起来就是完整的数组。 对于查询操作，我们可以 O(N/B)O(N/B)O(N/B) 暴力找出 pospospos 所在的块，然后直接输出 vector 内的数。时间复杂度 O(N/B)O(N/B)O(N/B) 对于插入操作，我们先 O(N/B)O(N/B)O(N/B) 找出 pospospos 所在的块，然后直接用 vector.insert() 插入。insert() 的复杂度是 O(B)O(B)O(B) 的，所以插入的复杂度是 O(B+N/B)O(B+N/B)O(B+N/B) 但是另外一个问题是，每一次插入操作都会让某一个块的长度越来越长，导致 B0B_0B0​ 越来越大。比如说，如果 QQQ 次操作都是把数插入到最后一个块的第一个位置，那么最后一块的块长将来到 O(Q)=O(N)O(Q)=O(N)O(Q)=O(N)，总时间复杂度将来到 O(NQ)O(NQ)O(NQ) 肯定是不行的。 所以针对最后一块的失衡问题，我们考虑重构操作：收集所有元素，然后重新分配到每一个块中，这样单次重构的复杂度是 O(N)O(N)O(N) 的. 如果在每 O(Q)O(\\sqrt Q)O(Q​) 次插入后都重构一次： 因为最多 QQQ 次操作，所以最多有 O(Q)O(\\sqrt Q)O(Q​) 次重构操作，重构的整体复杂度是 O(NQ)O(N\\sqrt Q)O(NQ​) 的. 而且，因为每 O(Q)O(\\sqrt Q)O(Q​) 次操作后就重构一次，插入操作时，块长最多为 B+QB+\\sqrt QB+Q​，依然是 O(N)O(\\sqrt N)O(N​) 的，保证了插入的复杂度正确性 查询依然还是 O(N/B)O(N/B)O(N/B) 所以综上，重构的总时间复杂度 O(NQ)O(N\\sqrt Q)O(NQ​)，插入查询的单词时间复杂度为 O(N)O(\\sqrt N)O(N​)，于是总时间复杂度为 O(NN)O(N\\sqrt N)O(NN​). AC Code #include bits/stdc++.hconstexpr int N = 1e5 + 10;constexpr int B = 500;struct block int len; std::pairint, int range; std::vectorint elem;;int n, opt, l, r, c;int sqrtn, bcnt, totlen, icnt0;std::arrayint, N a, bel;std::arrayblock, B blk;void rebuild() std::vectorint a; a.push_back(-1); for (int i = 1; i = bcnt; i++) a.insert(a.end(), blk[i].elem.begin(), blk[i].elem.end()); blk[i].elem.clear(); sqrtn = std::sqrt(totlen); bcnt = (totlen + sqrtn - 1) / sqrtn; for (int i = 1; i = bcnt; i++) auto [l, r] = blk[i].range = (i - 1) * sqrtn + 1, std::min(totlen, i * sqrtn); blk[i].len = r - l + 1; for (int j = l; j = r; j++) blk[i].elem.push_back(a[j]); icnt = 0;void insert(int pos, int val) for (int i = 1; i = bcnt; i++) if (pos blk[i].len) pos -= blk[i].len; else blk[i].elem.insert(blk[i].elem.begin() + pos - 1, val); blk[i].len++, totlen++, icnt++; if (icnt = sqrtn) rebuild(); break; void get(int pos) for (int i = 1; i = bcnt; i++) if (pos blk[i].len) pos -= blk[i].len; else std::cout blk[i].elem[pos - 1] ; break; int main() std::cin n; sqrtn = std::sqrt(n); bcnt = (n + sqrtn - 1) / sqrtn; totlen = n; for (int i = 1; i = n; i++) std::cin a[i]; for (int i = 1; i = bcnt; i++) auto [l, r] = blk[i].range = (i - 1) * sqrtn + 1, std::min(i * sqrtn, n); blk[i].len = blk[i].range.second - blk[i].range.first + 1; for (int j = l; j = r; j++) bel[j] = i, blk[i].elem.push_back(a[j]); for (int _ = 1; _ = n; _++) std::cin opt l r c; opt == 0 ? insert(l, r) : get(r); LOJ#6283. 数列分块入门 7 提示：在每个块上维护懒标记 tag = add, mul 表示块内所有元素的真实值为 ai′←ai⋅mul+add a_i\\gets a_i \\cdot \\texttt{mul} + \\texttt{add} ai′​←ai​⋅mul+add时间复杂度 O(nn)O(n\\sqrt n)O(nn​) AC Code #include bits/stdc++.husing ll = long long;constexpr int N = 1e5 + 10;constexpr int B = 505;constexpr int M = 10007;struct tag ll add0, mul1; // x * mul + add bool is_null() return add == 0 mul == 1; void clear() add = 0, mul = 1; void apply_mul(ll v) (mul *= v) %= M, (add *= v) %= M; void apply_add(ll v) (add += v) %= M; void merge(tag T) assert(T.add == 0 or T.mul == 1); if (T.add != 0) apply_add(T.add); else apply_mul(T.mul); ;struct block tag t; int l, r;;int n, opt, l, r, c;int sqrtn, blkcnt;std::arrayint, N bel;std::arrayll, N a;std::arrayblock, B blks;void rebuild(int L, int R, tag v) int b = bel[L]; for (int i = blks[b].l; i = blks[b].r; i++) a[i] = (a[i] * blks[b].t.mul % M + blks[b].t.add) % M; if (L = i i = R) if (v.add != 0) (a[i] += v.add) %= M; else (a[i] *= v.mul) %= M; blks[b].t.clear();void apply(int L, int R, tag T) int s = bel[L], e = bel[R]; if (s == e) rebuild(L, R, T); else rebuild(L, blks[s].r, T); rebuild(blks[e].l, R, T); for (int i = s + 1; i = e - 1; i++) blks[i].t.merge(T); void query(int pos) int b = bel[pos]; std::cout (a[pos] * blks[b].t.mul % M + blks[b].t.add) % M ;int main() std::cin n; for (int i = 1; i = n; i++) std::cin a[i], a[i] %= M; sqrtn = std::sqrt(n), blkcnt = (n + sqrtn - 1) / sqrtn; for (int i = 1; i = blkcnt; i++) l = (i - 1) * sqrtn + 1, r = std::min(i * sqrtn, n); blks[i].l = l, blks[i].r = r; for (int j = l; j = r; j++) bel[j] = i; for (int i = 1; i = n; i++) std::cin opt l r c; if (opt == 0) apply(l, r, c, 1); else if (opt == 1) apply(l, r, 0, c); else query(r);","tags":[null]},{"title":"珂朵莉树","path":"/notes/acm/tricks/chtholly-tree.html","content":"珂朵莉树算法 具体实现 算法时间复杂度分析 为什么一定要学习如何分析珂朵莉树的时间复杂度？ 珂朵莉树时间复杂度的正确性基于针对操作的分析，分析时间复杂度通常需要分析“连续段”数量的变化，才能计算操作的均摊时间复杂度。因此学会分析时间复杂度是正确使用珂朵莉树的关键所在。（否则只能期待玄学出现或者祈祷出题人不会卡了 珂朵莉树的推广 例题 其实也不算例题，毕竟没有哪一道题的标答是奔着珂朵莉树去的，只能说可以用珂朵莉树的思想去做题","tags":[null]},{"title":"Min-Max 容斥","path":"/notes/acm/tricks/min-max-inclusion-exclusion.html","content":"Min-Max 容斥算法 Min-Max 容斥可以在已知一者的情况下求出另外一者，如 max⁡(a,b)=a+b−min⁡(a,b)max⁡(a,b,c)=a+b+c−min⁡(a,b)−min⁡(a,c)−min⁡(b,c)+min⁡(a,b,c) \\max(a,b)=a+b-\\min(a,b)\\\\ \\max(a,b,c)=a+b+c-\\min(a,b)-\\min(a,c)-\\min(b,c)+\\min(a,b,c) max(a,b)=a+b−min(a,b)max(a,b,c)=a+b+c−min(a,b)−min(a,c)−min(b,c)+min(a,b,c)我们把这个式子推广到 nnn 个变量的情况的话，就有（令 S={ a1,a2,…,an }S=\\set{a_1,a_2,\\dots,a_n}S={a1​,a2​,…,an​}） min⁡(S)=∑T⊆S(−1)∣T∣−1max⁡(T)max⁡(S)=∑T⊆S(−1)∣T∣−1min⁡(T) \\min(S)=\\sum_{T\\subseteq S} (-1)^{|T|-1} \\max(T)\\\\ \\max(S)=\\sum_{T\\subseteq S} (-1)^{|T|-1} \\min(T) min(S)=T⊆S∑​(−1)∣T∣−1max(T)max(S)=T⊆S∑​(−1)∣T∣−1min(T) 扩展：kkk-th Min-Max 容斥 mink(S)=∑T⊆S(−1)∣T∣−k(∣T∣−1k−1)max⁡(T)maxk(S)=∑T⊆S(−1)∣T∣−k(∣T∣−1k−1)min⁡(T)\\mathop{\\mathrm{min_k}}(S)=\\sum_{T\\subseteq S}(-1)^{|T|-k}\\binom{|T|-1}{k-1}\\max(T)\\\\\\mathop{\\mathrm{max_k}}(S)=\\sum_{T\\subseteq S}(-1)^{|T|-k}\\binom{|T|-1}{k-1}\\min(T)mink​(S)=T⊆S∑​(−1)∣T∣−k(k−1∣T∣−1​)max(T)maxk​(S)=T⊆S∑​(−1)∣T∣−k(k−1∣T∣−1​)min(T) 证明","tags":[null]},{"title":"ACM Tricks","path":"/notes/acm/tricks/techniques-01.html","content":"距离之间的转化 Manhattan 距离、Chebyshev 距离 Manhattan 距离可以和 Chebyshev 距离互相转化。考虑两个点 P1(x1,y1),P2(x2,y2)P_1(x_1, y_1), P_2(x_2, y_2)P1​(x1​,y1​),P2​(x2​,y2​)，其曼哈顿距离为 ∣x1−x2∣+∣y1−y2∣ |x_1-x_2|+|y_1-y_2| ∣x1​−x2​∣+∣y1​−y2​∣其切比雪夫距离为 max⁡(∣x1−x2∣,∣y1−y2∣) \\max(|x_1-x_2|, |y_1-y_2|) max(∣x1​−x2​∣,∣y1​−y2​∣)两个点的曼哈顿距离可以等价转化为切比雪夫距离。构造两个切比雪夫点 C1(x1+y1,x1−y1),C2(x2+y2,x2−y2)C_1(x_1+y_1, x_1-y_1), C_2(x_2+y_2, x_2-y_2)C1​(x1​+y1​,x1​−y1​),C2​(x2​+y2​,x2​−y2​)，则 P1,P2P_1, P_2P1​,P2​ 的曼哈顿距离为 C1,C2C_1, C_2C1​,C2​ 的切比雪夫距离。 并且这个结论可以拓展到 kkk 维的情形。考虑 kkk 维曼哈顿距离是 d1(x,y)=∑i=1k∣xi−yi∣d_1(\\bold{x},\\bold{y})=\\sum_{i=1}^k |x_i-y_i|d1​(x,y)=i=1∑k​∣xi​−yi​∣而 kkk 维的切比雪夫距离为 d∞(x,y)=max⁡i=1k∣xi−yi∣d_{\\infin}(\\bold{x},\\bold{y})=\\max_{i=1}^k |x_i-y_i|d∞​(x,y)=i=1maxk​∣xi​−yi​∣【例题】 JOISC 2021 道路建设","tags":[null]},{"title":"Vector Space 向量空间","path":"/notes/math/linalg/vector-space.html","content":"Vector Space","tags":[null]},{"title":"优化理论（一）：基础概念","path":"/notes/math/optim/optimization-001.html","content":"Optimization Theory 在研究什么 OT 在研究的内容可以概括为：在给定 requirement 和 constraint 的情况下，minimize/maximize 一个或多个 objective functions graph LR; A(Analysis) --> B(Design) --> C(Optimization) Optimal Design: feasible, best on some measure of effectiveness. Fundamental Concepts 以下涉及到的概念可以应用到大部分 optimization 的问题之中。 General Problem Statement GPS 的形式可以表示为： min⁡f(x)=…wherex=[x1x2⋮xn]s.t.gi(x)≤0,i=1,2,…,mhj(x)=0,j=1,2,…,L \\begin{array}{rllll} \\min f(\\bold{x})=\\dots \\\\ \\text{where} \\bold{x}=\\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix} \\\\ \\text{s.t.} g_i(\\bold{x}) \\le 0, i=1,2,\\dots, m \\\\ % equations part h_j(\\bold{x})=0, j=1,2,\\dots,L \\\\ \\end{array} minwheres.t.​f(x)=…x=​x1​x2​⋮xn​​​gi​(x)≤0,i=1,2,…,mhj​(x)=0,j=1,2,…,L​即在 mmm 个不等式和 LLL 个等式的约束下，最小化有 nnn 个变量的 objective function f(x)f(\\bold{x})f(x) 的值. 我们也可以通过一些方法把其他形式的优化目标/不等式约束转化成 GPS: max⁡f(x) ⟺ min⁡−f(x)\\max f(\\bold{x})\\iff \\min -f(\\bold{x})maxf(x)⟺min−f(x) 把连不等式 (side inequality) 拆分为多个不等式 Constraints, Design Space Design Space 是所有可能解的空间，而在 constraints 的约束下，只有一小部分解空间能够符合所有 constraints，这样的一部分称为 Feasible Region 对于某个特定的解 x∗\\bold{x}^\\astx∗，如果 g(x∗)0g(\\bold{x}^\\ast)\\gt 0g(x∗)0，此时不等式条件不成立，我们称约束为 Violated Constraint. g(x∗)=0g(\\bold{x}^\\ast)=0g(x∗)=0，此时不等式恰好取等，我们称其为 Active Constraint. g(x∗)0g(\\bold{x}^\\ast)\\lt 0g(x∗)0，此时不等式严格满足，称其为 Inactive Constraint. Gradient Vector, Hessian Matrix 梯度向量与黑塞矩阵 梯度向量是列向量 ∇f(x)=[∂f∂x1∂f∂x2⋮∂f∂xn] \\gdef\\pt#1#2{\\frac{\\partial #1}{\\partial #2}} abla f(\\bold{x})= \\begin{bmatrix} \\pt{f}{x_1}\\\\ \\pt{f}{x_2}\\\\ \\vdots\\\\ \\pt{f}{x_n} \\end{bmatrix} ∇f(x)=​∂x1​∂f​∂x2​∂f​⋮∂xn​∂f​​​Hessian Matrix 是一个 n×nn\\times nn×n 的方阵，每一项都是二阶偏导 ∇2f(x)=[∂2f∂x12∂2f∂x1∂x2…∂2f∂x1∂xn∂2f∂x2∂x1∂2f∂x22…∂2f∂x2∂xn⋮⋮⋱⋮∂2f∂xn∂x1∂2f∂xn∂x2…∂2f∂xn2] \\gdef\\pf#1#2{\\frac{\\partial^2 #1}{\\partial #2^2}} \\gdef\\pt#1#2#3{\\frac{\\partial^2 #1}{\\partial #2\\partial #3}} abla^2 f(\\bold{x})= \\begin{bmatrix} \\pf{f}{x_1}\\pt{f}{x_1}{x_2}\\dots \\pt{f}{x_1}{x_n}\\\\ \\pt{f}{x_2}{x_1}\\pf{f}{x_2}\\dots \\pt{f}{x_2}{x_n}\\\\ \\vdots \\vdots \\ddots \\vdots\\\\ \\pt{f}{x_n}{x_1}\\pt{f}{x_n}{x_2}\\dots \\pf{f}{x_n} \\end{bmatrix} ∇2f(x)=​∂x12​∂2f​∂x2​∂x1​∂2f​⋮∂xn​∂x1​∂2f​​∂x1​∂x2​∂2f​∂x22​∂2f​⋮∂xn​∂x2​∂2f​​……⋱…​∂x1​∂xn​∂2f​∂x2​∂xn​∂2f​⋮∂xn2​∂2f​​​计算这些矩阵可以用于 Sensitivity Analysis.","tags":[null]},{"title":"优化理论（三）：优化问题的分类","path":"/notes/math/optim/optimization-categorization.html","content":"本章会以 the nature of design variables 或者 the nature of objective/constraint functions 对优化问题进行分类 Continuous vs Discreate 若 design variable 是离散的，那么优化问题就是离散的（也称组合优化问题）。例如，x∈{ 0,1 },x∈Z,x∈Ax\\in\\set{0,1}, x\\in \\Z, x\\in Ax∈{0,1},x∈Z,x∈A. Linear vs Non-Linear 如果 objective function 和 constraint functions 都是线性函数，那么就是 Linear Optimization Problem，也被称为 Linear Programming Problem. 只要任意一个不是线性的，就是 Non-Linear Optimization Problem. Parameter vs Trajectory Problem Type Explanation Parameter Optimization Problem design variables 之间互相独立 Trajectory Optimization Problem design variables 之间可能存在函数关系 例如，f(b(x),f(x))f(b(x),f(x))f(b(x),f(x)) 就是 Trajectory Optimization Problem. 一种处理办法是用多个独立变量拟合 dependent variables，再当作 Parameter Optimization Problems. Categorization of Methods","tags":[null]},{"title":"优化理论（二）：最优条件","path":"/notes/math/optim/optimization-conditions.html","content":"Conditions of Optimality 考虑对 f(x)f(x)f(x) 在极小值点 x∗x^\\astx∗ 进行泰勒展开，则有 f(x)=f(x∗)+[∇f(x∗)]TΔx∗+12[Δx∗]T[∇2f(x∗)]Δx∗+R \\gdef\\wrap#1{\\Big[#1\\Big]} f(x) = f(x^\\ast) + \\wrap{ abla f(x^\\ast)}^T \\Delta x^\\ast + \\frac12 \\wrap{\\Delta x^\\ast}^T\\wrap{ abla^2 f(x^\\ast)} {\\Delta x^\\ast} + R f(x)=f(x∗)+[∇f(x∗)]TΔx∗+21​[Δx∗]T[∇2f(x∗)]Δx∗+R其中 Δx∗=x−x∗\\Delta x^\\ast=x-x^\\astΔx∗=x−x∗, RRR 是包含高次 Δx∗\\Delta x^\\astΔx∗ 的多项式。 如果梯度向量 ∇f(x)=0 abla f(\\bold{x})=0∇f(x)=0，这就说明 x\\bold{x}x 是一个稳定点 stationary point，例如极值点、鞍点、拐点等等。此时，我们可以通过 Hessian Matrix 进一步判断 x∗\\bold{x}^\\astx∗ 究竟是极小值点、极大值点、或是鞍点。 如果 quadratic form 满足 [Δx∗]T[∇2f(x∗)]Δx∗0,Δx∗≠0 \\gdef\\wrap#1{\\Big[#1\\Big]} \\wrap{\\Delta x^\\ast}^T\\wrap{ abla^2 f(x^\\ast)} {\\Delta x^\\ast}\\gt 0, \\Delta x^\\ast e 0 [Δx∗]T[∇2f(x∗)]Δx∗0,Δx∗=0即 quadratic form 恒大于 000，我们称 Hessian Matrix 为 positive definite. 若不等号为 0\\gt 00，则称之为 positive semidefinite. 类似的，可以定义出 negative definite 和 negative semidefinite. 若既有可能 positive 又有可能 negative，则称为 indefinite. 也可以通过检查 Hessian Matrix 的所有 eigenvalues 来判断。若全都 0\\gt 00 则为 positive definite；若全都 ≥0\\ge 0≥0，则为 positive semidefinite. ……无约束条件的优化问题 First-Order Necessary Condition 如果 x∗\\bold{x}^\\astx∗ 是极小值点，则 f(x)f(\\bold{x})f(x) 在该点处的 Gradient Vector 为 000. x∗ is a local minimum ⟹ ∇f(x∗)=0\\text{\\(\\bold{x}^\\ast\\) is a local minimum}\\implies abla f(\\bold{x}^\\ast)=0x∗ is a local minimum⟹∇f(x∗)=0 Second-Order Necessary Condition 如果 x∗\\bold{x}^\\astx∗ 是函数 f(x)f(\\bold{x})f(x) 的极小值点，则 f(x)f(\\bold{x})f(x) 在此处的二阶导为 positive definite 或者 positive semidefinite. x∗ is a local minimum ⟹ ∇2f(x) is positive (semi)definite\\text{\\(\\bold{x}^\\ast\\) is a local minimum}\\implies \\text{\\( abla^2f(\\bold{x})\\) is \\textbf{positive (semi)definite}}x∗ is a local minimum⟹∇2f(x) is positive (semi)definite Second-Order Sufficient Condition 若 ∇2f(x∗) abla^2 f(\\bold{x}^\\ast)∇2f(x∗) 是 positive definite 的，并且满足了 First Order Necessary Condition，那么，x=x∗\\bold x=\\bold x^\\astx=x∗ 是 f(x)f(\\bold x)f(x) 的极小值点。 如果 x0\\bold x_0x0​ 只满足必要条件，但不满足任何充分条件，我们无法断定 x0\\bold x_0x0​ 是极小值点。 ……有约束条件的优化问题 新的问题：最优解的点可能会出现在边界上 为了解决这个问题，我们对约束条件乘上 Lagrange multiplier λ\\lambdaλ 然后加到 objective function 里面，组成新的复合 objective function Lagrange Function (Lagrangean) LLL. L(x,λ,s)=f(x)+∑j=1mλj[gj(x)+sj2]+∑k=1lλm+k[hk(x)] \\begin{aligned} L(\\bold x, \\lambda, s)\\\\ =f(\\bold x)\\\\ +\\sum_{j=1}^m \\lambda_j \\Big[ g_j(\\bold x) + s_j^2 \\Big] \\\\ + \\sum_{k=1}^l \\lambda_{m+k}\\Big[ h_k(\\bold x) \\Big] \\end{aligned} =++​L(x,λ,s)f(x)j=1∑m​λj​[gj​(x)+sj2​]k=1∑l​λm+k​[hk​(x)]​其中 sss 为 slack variable，λi\\lambda_iλi​ 是 Lagrange Multiplier。Lagrange Function 的设计使得我们可以像处理 unconstrained problem 那样处理 constrained problems，通过 λ\\lambdaλ 和 slack variable 保证约束的满足。 此时也要把 λ,sj\\lambda,s_jλ,sj​ 也看成是变量，对其求导并令其导数为 000. Conditions of Optimality 使用条件 当且仅当 x0\\bold x_0x0​ 是 Regular point 的时候才可以对 constrained problem 使用 unconstrained problem 的判定技巧。 Regular point 是指，等式约束以及 active 的不等式约束在 x0\\bold x_0x0​ 处的一阶导数 ∇hk(x0),∇gj(x0) abla h_k(\\bold x_0), abla g_j(\\bold x_0)∇hk​(x0​),∇gj​(x0​) 是 线性无关 (linearly independent) 的 Karush-Kuhn-Tucker Conditions（有约束情形下的一阶导必要条件） 通过对 Lagrange function 求导，我们既可以得到约束条件，又可以得到 f(x)f(\\bold x)f(x) 的导数： ∇L(x∗,λ,s)=∇f(x∗)+∑j=1mλj∇gj(x∗)+∑k=1lλm+k∇hk(x∗)=0∂L∂λj=gj(x∗)+sj2=0∂L∂λm+k=hk(x∗)=0∂L∂sj=2λjsj=0 abla L(\\bold x^\\ast, \\lambda, s)= abla f(\\bold x^\\ast) + \\sum_{j=1}^m\\lambda_j abla g_j(\\bold x^\\ast) + \\sum_{k=1}^l \\lambda_{m+k} abla h_k(\\bold x^\\ast)=0 \\\\ \\frac{\\partial L}{\\partial \\lambda_j}=g_j(\\bold x^\\ast) +s_j^2=0\\\\ \\frac{\\partial L}{\\partial \\lambda_{m+k}}=h_k(\\bold x^\\ast)=0\\\\ \\frac{\\partial L}{\\partial s_j}=2\\lambda_js_j=0∇L(x∗,λ,s)=∇f(x∗)+j=1∑m​λj​∇gj​(x∗)+k=1∑l​λm+k​∇hk​(x∗)=0∂λj​∂L​=gj​(x∗)+sj2​=0∂λm+k​∂L​=hk​(x∗)=0∂sj​∂L​=2λj​sj​=0 λj\\lambda_jλj​ 需要非负，λm+k\\lambda_{m+k}λm+k​ 则随意 对 sj2s_j^2sj2​ 项求导可知，要么 sj=0s_j=0sj​=0 要么 λj=0\\lambda_j=0λj​=0（或者都是） Second Order Necessary Condition（有约束情形下的二阶导必要条件） 若 x=x∗\\bold x=\\bold x^\\astx=x∗ 是 f(x)f(\\bold x)f(x) 的极小值点，则 Lagrange Function 的 Hessian Matrix 的 Quadratic form 满足： (Δx∗)T ∇2L(x∗,λ∗,s∗) (Δx)≥0(\\Delta \\bold x^\\ast)^T \\, abla^2L(\\bold x^\\ast, \\lambda^\\ast, s^\\ast) \\, (\\Delta \\bold x) \\ge 0(Δx∗)T∇2L(x∗,λ∗,s∗)(Δx)≥0其中 Δx∗≠0\\Delta\\bold x^\\ast e 0Δx∗=0 并且与 Active Constraints 的 Gradient Vector 互相 Orthogonal，即 [∇gj(x∗)]TΔx∗=0,where gj(x∗)≤0 is active[∇hk(x∗)]TΔx∗=0[ abla g_j(\\bold x^\\ast)]^T \\Delta\\bold x^\\ast=0, \\text{where \\(g_j(\\bold x^\\ast)\\le 0\\) is active}\\\\ [ abla h_k(\\bold x^\\ast)]^T \\Delta\\bold x^\\ast=0[∇gj​(x∗)]TΔx∗=0,where gj​(x∗)≤0 is active[∇hk​(x∗)]TΔx∗=0这是为了保证 Δx∗\\Delta \\bold x^\\astΔx∗ 的移动方向是 feasible 的. Second Order Sufficient Condition（有约束情形下的二阶导充分条件） 若 Lagrange Function 的 Hessian Matrix 满足 (Δx∗)T∇2L(x∗,λ∗,s∗)Δx∗0(\\Delta \\bold x^\\ast)^T abla^2L(\\bold x^\\ast,\\lambda^\\ast,s^\\ast) \\Delta\\bold x^\\ast\\gt 0(Δx∗)T∇2L(x∗,λ∗,s∗)Δx∗0并且 Δx∗≠0\\Delta\\bold x^\\ast e 0Δx∗=0 满足 {∇gj(x∗)TΔx∗=0,if λj∗0∇gj(x∗)TΔx∗≤0,if λj∗=0∇hk(x∗)TΔx∗=0,k={ 1,2,…,l }\\begin{cases} abla g_j(\\bold x^\\ast)^T\\Delta\\bold x^\\ast=0,\\text{if \\(\\lambda_j^\\ast\\gt 0\\)}\\\\ abla g_j(\\bold x^\\ast)^T\\Delta\\bold x^\\ast\\le 0,\\text{if \\(\\lambda_j^\\ast= 0\\)}\\\\ abla h_k(\\bold x^\\ast)^T\\Delta\\bold x^\\ast=0,k=\\set{1,2,\\dots,l} \\end{cases}⎩⎨⎧​∇gj​(x∗)TΔx∗=0,∇gj​(x∗)TΔx∗≤0,∇hk​(x∗)TΔx∗=0,​if λj∗​0if λj∗​=0k={1,2,…,l}​","tags":[null]},{"title":"计算几何：圆","path":"/wiki/algo/geometry/circles.html","content":"圆的数据结构定义 struct circle point c; // 圆的中心 real r; // 半径; 注意事项 尽量避免直接用 (long) double 进行操作，考虑到浮点误差，更建议使用向量操作 圆与直线 过某点做某圆的切线 过一个点 PPP 做圆 CCC 的切线 若点 PPP 在圆上，则切线垂直于半径 CPCPCP，直接令直线 s=P, d=perp(CP⃗)\\texttt{s=\\(P\\), d=perp(\\(\\vec{CP}\\))}s=P, d=perp(CP). 否则 PPP 在圆外，否则没有切线。此时有两条切线，且应当关于 CPCPCP 对称。考虑用误差更小的向量组合求出 DDD，然后另一边也能求了。 弦切角定理 令 ∣CP∣=d|CP|=d∣CP∣=d，推理 ΔPCD\\Delta PCDΔPCD 的面积，发现 ∣AD∣=rd2−r2d,∣AP∣=d2−r2d|AD| = \\frac{r\\sqrt{d^2-r^2}}{d}, |AP|=\\frac{d^2-r^2}{d}∣AD∣=drd2−r2​​,∣AP∣=dd2−r2​ 所以 D=P+∣AP∣⋅PC⃗±∣AD∣⋅perp(PC⃗)D = P+|AP|\\cdot \\vec{PC} \\plusmn |AD|\\cdot \\texttt{perp}(\\vec{PC})D=P+∣AP∣⋅PC±∣AD∣⋅perp(PC) AC Code std::vectorline tangent(point x, circle c) real d = distance(c.c, x); std::vectorline res = ; if (d == c.r) res.push_back(linex, perp(c.c - x)); // 点在圆上 else if(d c.r) // 点在圆外 real c1 = (d.sqr() - c.r.sqr()) / d; real c2 = c.r * (sqrt(d.sqr() - c.r.sqr())) / d; res.push_back(linex, (c.c-x).rescale(c1) + (c.c-x).Rrot().rescale(c2)); res.push_back(linex, (c.c-x).rescale(c1) + (c.c-x).Lrot().rescale(c2)); // 否则点在圆内 return res; 圆与三角形 三角形外接圆、三点定圆 给定不共线的三个点 A,B,CA,B,CA,B,C，求圆 PPP 过三个点。 由于是三点共圆，所以 ∣AP∣=∣BP∣=∣CP∣|AP| = |BP| = |CP|∣AP∣=∣BP∣=∣CP∣，于是，我们可以求出 ABABAB 与 BCBCBC 的中垂线，则这两条中垂线的交点必然就是圆心。于是半径也很容易求了。 三角形内切圆 给定三角形 ΔABC\\Delta ABCΔABC，求出其内切圆。 圆与圆 两圆的公切线 我们首先需要就两圆的不同位置情况进行分类讨论，以下，我们假设圆 AAA 的半径 r1r_1r1​ 大于圆 CCC 的半径 r2r_2r2​，且令 ∣AC∣=d|AC| = d∣AC∣=d 两圆内含 由于两圆内含，这时半径与圆心距离应该满足dr1−r2d\\lt r_1-r_2dr1​−r2​ 显然，这个情况下没有公切线。 两圆内切 此时半径与圆心距离应当满足d=r1−r2d=r_1-r_2d=r1​−r2​ 此时，只有一条公切线，如下图所示 我们找出直线 ACACAC 的表达式，并且找出 BBB 的坐标，然后过 BBB 作 ACACAC 垂线。 怎么找出 BBB 点坐标呢？考虑到 A,B,CA,B,CA,B,C 三点共线，于是有 B=A+AC⃗⋅r1dB=A+\\vec{AC}\\cdot \\frac{r_1}{d}B=A+AC⋅dr1​​ 两圆相交 此时满足r1−r2dr1+r2r_1-r_2\\lt d \\lt r_1+r_2r1​−r2​dr1​+r2​ 作圆 同时与两条直线相切，作给定半径的圆 同时与直线 u,vu,vu,v 相切，半径为 rrr 的圆 我们先把两条直线沿垂线平移 ±r\\plusmn r±r 的距离，这样就有 2×22\\times 22×2 个交点可以作为圆心。 得到与给定直线相切、过某点、半径给定的圆 得到与直线 uuu 相切，过点 QQQ，半径为 rrr 的圆 同理，先将直线 uuu 平移 ±r\\plusmn r±r 的距离得到 u1,u2u_1,u_2u1​,u2​，然后任务就变成了，在两条新的直线上，找点 PPP，使得 ∣PQ∣=r|PQ|=r∣PQ∣=r 所以，先求出 QQQ 到 uiu_iui​ 的距离和垂足 HHH，用勾股定理计算 PPP 到垂足 HHH 的距离，然后向量加即可。 过两点，半径给定的两个圆 得到过 a,ba, ba,b 两点，半径为 rrr 的两个圆 先作出 a,ba,ba,b 中垂线，然后在中垂线上取点即可。 圆上定理 托勒密定理 令 ABCDABCDABCD 为圆内接四边形，则必有 AC⋅BD=AB⋅CD+AD⋅BCAC\\cdot BD=AB\\cdot CD+AD\\cdot BCAC⋅BD=AB⋅CD+AD⋅BC 圆幂定理 令割线（弦也可以） AB,CDAB,CDAB,CD 交于 PPP，则 PA⋅PB=PC⋅PDPA\\cdot PB=PC\\cdot PDPA⋅PB=PC⋅PD"},{"title":"计算几何：凸包","path":"/wiki/algo/geometry/convex-hull.html","content":"求解凸包"},{"title":"计算几何：半平面交","path":"/wiki/algo/geometry/half-plane.html","content":"半平面交 增量法求解 例题"},{"title":"旋转卡尺","path":"/wiki/algo/geometry/rotating-caliper.html","content":"Rotating Caliper 旋转卡尺实际上利用的是答案在凸包上的单峰性，即选定一条边后，我们按逆时针（或顺时针）检查所有其他点，并计算其与这条边构成的答案，那么有一个（或两个相邻的）点它的答案是最大的。并且，当我从这条边你时候（或顺时针）移动到下一条边时，这个答案会减小，所以我们可以继续转动点。 其实有点像凸包上的双指针，一个指针指向边，另一个指针指向点。"},{"title":"三角剖分与 Voronoi 图","path":"/wiki/algo/geometry/triangulation.html","content":"Delauney 三角剖分 双向链接边表 DCEL"},{"title":"Haskell 里的数据类型","path":"/notes/cs/coding/haskell/haskell-datatypes.html","content":"Lists Lists are used to store multiple values of the same type. List Operations 这些操作并不修改原列表的值，返回的是其 copy. head :: [a] - a -- returns the first elementlast :: [a] - a -- returns the last elementtail :: [a] - [a] -- returns everything except the first elementinit :: [a] - [a] -- returns everything except the last elementtake :: Int - [a] - [a] -- returns the n first elementsdrop :: Int - [a] - [a] -- returns everything except the n first elements(++) :: [a] - [a] - [a] -- lists are catenated with the ++ operator(!!) :: [a] - Int - a -- lists are indexed with the !! operatorreverse :: [a] - [a] -- reverse a listnull :: [a] - Bool -- is this list empty?length :: [a] - Int -- the length of a list Maybe Type 约等于 Rust 里的 OptionT Maybe Int-- =Nothing, Just 0, Just 114514, ... Either Type Either String Int-- =Left asdf, Right 1, Right 67, Left qweohuff 0, ...","tags":[null]},{"title":"Haskell Catamorphic","path":"/notes/cs/coding/haskell/haskell-catamorphic.html","content":"Functional Programming on Lists map: transform [a] to [f(a)] -- 函数签名map :: (a - b) - [a] - [b] filter filter :: (a - Bool) - [a] - [a] Utilities words 可以把 String 按空格分割开来 tails 可以返回一个字符串的全部后缀 Partial Application 当一个参数传入函数，默认总是占据最左侧没被赋值的参数 Prefix and Infix Notations 每一个中缀形式的运算符都有等价形式的前缀形式，写法是用括号 () 包住运算符然后提前 1 + 2-- 等价于……(+) 1 2 zipWith -- 函数签名zipWith :: (a - b - c) - [a] - [b] - [c]-- 使用例zipWith (+) [0,2,5] [1,4,7]-- Result = [1, 6, 12] Lambda Functions 定义一个匿名函数的方式如下 -- 反斜杠，用空格隔开的参数，-，函数体\\x y z - x + y + z\\x - x 7-- 使用例filter (\\x - x 7) [1, 10, 100] . 运算符与 $ 运算符 点运算符 . 运算符将多个函数结合，类似于数学里面的记号 (.) :: (b - c) - (a - b) - a - c-- 考虑……(f . g) x-- 等价于f (g x)-- 使用例third = head . tail . tail -- fetches the third element of a list 美元运算符 ($) :: (a - b) - a - b-- 即f $ x-- 等价于f x $ 的作用主要是用于消除函数嵌套时的括号： f1 (f2 (f3 ...... (fn x)))-- 用 $ 符号化简f1 $ f2 $ f3 ...... $ fn x-- 或者同时用 . 和 $ 化简f1 . f2 . f3 ...... fn $ x More Functions takeWhile, dropWhile takeWhile, dropWhlie 与 filter 的区别在于，takeWhile, dropWhlie 其实是只考虑列表的前缀，而 filter 是过滤整个列表。 takeWhile :: (a - Bool) - [a] - [a] -- take elements from a list as long as they satisfy a predicatedropWhile :: (a - Bool) - [a] - [a] -- drop elements from a list as long as they satisfy a predicate-- 使用例takeWhile even [2,4,1,2,3] -- == [2,4]dropWhile even [2,4,1,2,3] -- == [1,2,3] elem 检查元素是否在列表里面。 const 总是返回两个参数里的第一个参数 const :: a - b - a Lists and Recursion 在列表头添加元素 (:) :: a - [a] - [a]-- 使用例1:[2,3] Pattern Matching for Lists [x:_] -- 提取列表第一个元素f (_:x:_) = x -- 提取第二个元素[x:y:_] -- 前两个元素[x:y:z:_] -- 前三个元素 Tail Recursion 类似的，列表上的递归函数也可以应用尾递归 List Comprehension 和 Python 一样，Haskell 也有列表操作 [f x | x - lis] -- = map f lis[x | x - lis, pred x] -- = filter pred lis[f x | x - lis, p x]-- 等价于map f (filter p lis) 同样支持其他操作 -- 多个列表[ first ++ ++ last | first - [John, Mary], last - [Smith,Cooper] ] == [John Smith,John Cooper,Mary Smith,Mary Cooper]-- Local Definition[ reversed | word - [this,is,a,string], let reversed = reverse word ] == [siht,si,a,gnirts]-- Pattern MatchingfirstLetters string = [ char | (char:_) - words string ]firstLetters Hello World! == HW 自定义（中缀）运算符 Haskell 里的运算符定义只需要任意符号即可 -- For example(+) :: [Int] - [Int] - [Int]xs + ys = zipWith (+) xs ys Typed Holes 类似于 C++ 的 auto，Haskell 可以自动推导 _name 的类型，但是会通过报错的形式告诉你 定义语法是：下划线 _ + 任意名字 keepElements :: [a] - [Bool] - [a]keepElements xs bs = _doIt (zip xs bs)-- interactive: error:-- • Found hole: _doIt :: [(a, Bool)] - [a]","tags":[null]},{"title":"Haskell 基础语法","path":"/notes/cs/coding/haskell/haskell-grammars.html","content":"Local Definition 可以定义临时变量 where 语法 circleArea :: Double - DoublecircleArea r = pi * rsquare where pi = 3.1415926 rsquare = r * r let ... in 语法 circleArea r = let pi = 3.1415926 square x = x * x in pi * square r 流程控制 if-then-else login user password = if user == unicorn73 then if password == f4bulous! then unicorn73 logged in else wrong password else unknown user Pattern Matching PM in Function Definition 使用 _ 下划线作为 default\\tt{default}default 通配 greet :: String - String - Stringgreet Finland name = Hei, ++ namegreet Italy name = Ciao, ++ namegreet England name = How do you do, ++ namegreet _ name = Hello, ++ name Conditional Definition (Guards) 可以使用更加复杂的条件进行函数定义 f x y z | condition1 = something | condition2 = other | otherwise = somethingother 甚至可以和 Pattern Matching 搭配使用 Example guessAge :: String - Int - StringguessAge Griselda age | age 47 = Too low! | age 47 = Too high! | otherwise = Correct!guessAge Hansel age | age 12 = Too low! | age 12 = Too high! | otherwise = Correct!guessAge name age = Wrong name! case-of 语法 更加方便的针对某个变量进行 routing case value of pattern - expression pattern - expression","tags":[null]},{"title":"Real Classy","path":"/notes/cs/coding/haskell/haskell-real-classy.html","content":"Tuples 和 Python 的 Tuple 很类似，Haskell 在 Tuple 上定义了一些有用的函数 -- std::pair.firstfst :: (a, b) - a-- std::pair.secondsnd :: (a, b) - bzip :: [a] - [b] - [(a, b)] -- two lists to list of pairsunzip :: [(a, b)] - ([a], [b]) -- list of pairs to pair of listspartition :: (a - Bool) - [a] - ([a], [a]) -- elements that satisfy and dont satisfy a predicate tuple 上也可以进行 pattern matching swap :: (a,b) - (b,a)swap (x,y) = (y,x) Folding 什么是 folding? 给定一个列表 LLL、函数 fff 和初始值 aaa，将一整个列表不断应用 a←f(Li,a)a\\gets f(L_i, a)a←f(Li​,a) 计算出一个值。 -- foldr, 类似于求数组的和foldr :: (a - b - b) - b - [a] - bfoldr f y [] = yfoldr f y (x:xs) = f x (foldr f y xs) Type Classes (==) :: (Eq a) = a - a - Bool For all types a that belong to the class Eq, this is a function of type a - a - Bool. That is, if the type a is a member of the class Eq, you can give two values of type a to == and get a Bool result."},{"title":"CodeForces Round 1026 (Div. 2)","path":"/notes/acm/competitions/codeforces/cf2110.html","content":"这次写了四道题，E 题有思路但是不会写，再接再厉吧 A. Fashionable Array 观察到 nnn 非常小，只有 505050. 因此我们可以直接对 A[]A[]A[] 排序后，用 O(n2)O(n^2)O(n2) 暴力枚举 min⁡,max⁡\\min,\\maxmin,max 即可。唯一值得注意的是，min⁡,max⁡\\min,\\maxmin,max 可以是同一个数。 Code #include algorithm#include headers/io/io.hpp#include vectorIO::IO io;void run() int n = io.scanint(); std::vectorint mp(n); io.read(mp); std::ranges::sort(mp); int ans = 1e9; for (int i = 0; i n; i++) for (int j = i; j n; j++) if ((mp[i] + mp[j]) % 2 != 0) continue; ans = std::min(ans, n - (j - i + 1)); io.println(ans);int main() int T = io.scanint(); while (T--) run(); B. Down with Brackets 因为需要且必须删除一个左括号、一个右括号，考虑两种情况： 删的右括号在左括号前面 我们可以证明这个情况是不行的。由于原串已经是合法的括号序了，令左括号为 ci=1c_i=1ci​=1、右括号为 ci=−1c_i=-1ci​=−1，则对于每一个位置都有 ∀i,f(i)=∑1≤j≤icj≥0 \\forall i, f(i)=\\sum_{1\\le j\\le i} c_j \\ge 0 ∀i,f(i)=1≤j≤i∑​cj​≥0那么，如果先删右括号，这个和式必然不会减小。比如说删除了 iii 位置的 ')' 和 jjj 位置的 '(' 且有 iji\\lt jij，那么 f(k),kif(k),k\\lt if(k),ki 都不变，f(k),ikjf(k),i\\lt k\\lt jf(k),ikj 会变大，f(k),kjf(k),k\\gt jf(k),kj 不变。所以这个串必然还是合法的括号序 删的左括号在右括号前面 对于这种情况的话，我们发现，如果某个位置上 f(i)=0f(i)=0f(i)=0，那么删掉它（或者它之前的一个 '('）就会让括号序不合法 Code #include headers/io/io.hpp#include stringIO::IO io;void run() std::string s = io.scanstd::string(); int n = s.length(); std::vectorint pf(n + 1); pf[0] = 0; for (int i = 1; i = n; i++) if (s[i - 1] == () pf[i] = pf[i - 1] + 1; else pf[i] = pf[i - 1] - 1; if(in pf[i] == 0) io.println(YES); return; io.println(NO);int main() int T = io.scanint(); while (T--) run(); C. Racing 合法性比较容易想到，就是用 [li,ri][l_i,r_i][li​,ri​] 维护走完 did_idi​ 后可能的高度，判断和规定的区间是否有交集。没有则不合法，否则一定合法。 对于构造答案，我们从任意一个结束时的合法值出发，然后倒着看 did_idi​。如果符合要求的区间高度，则下降；否则不下降。 Code #include headers/io/io.hpp#include utilityusing pii = std::pairint, int;IO::IO io;void run() int n = io.scanint(); std::vectorint d(n); std::vectorpii a(n); io.read(d, a); std::vectorpii inter(n + 1, 0, 0); for (int i = 1; i = n; i++) if (d[i - 1] != -1) inter[i].first = inter[i - 1].first + d[i - 1]; inter[i].second = inter[i - 1].second + d[i - 1]; else inter[i].first = inter[i - 1].first; inter[i].second = inter[i - 1].second + 1; inter[i].first = std::max(inter[i].first, a[i - 1].first); inter[i].second = std::min(inter[i].second, a[i - 1].second); if (inter[i].first inter[i].second) io.println(-1); return; // possible int H = inter[n].first; for (int i = n; i 0; i--) if (d[i - 1] != -1) H -= d[i - 1]; else if (H - 1 = inter[i - 1].first H - 1 = inter[i - 1].second) H -= 1, d[i - 1] = 1; else d[i - 1] = 0; for (auto i : d) io.print(i, ); io.println();int main() int T = io.scanint(); while (T--) run(); D. Fewer Battery 考虑任意一条路径，我们发现如果走整条路径的话，终点时手上的电池数应该等于路径上的最大边权（当然前提要保证有足够多的电池可以走过这些边） 所以我们可以使用二分答案，过滤 w≥midw\\ge \\texttt{mid}w≥mid 的边，然后检查剩下的边是否可以成功到达终点。 Code #include headers/io/iov2.hpp#include algorithm#include utility#include vectorusing i64 = long long;using pii = std::pairint, i64;constexpr i64 V = -1e10;IO::IO io; void run() auto [n, m] = io.scanint, int(); std::vectori64 b(n); io.read(b); std::vectorstd::vectorpii adj(n); std::vectori64 ws; for (int i = 0; i m; i++) auto [u, v, w] = io.scanint, int, i64(); u--, v--; adj[u].emplace_back(v, w); ws.push_back(w); std::vectori64 dp(n, V); std::ranges::sort(ws); ws.erase(std::unique(ws.begin(), ws.end()), ws.end()); i64 l = -1, r = ws.size(); auto ck = [](i64 mid) - bool std::ranges::fill(dp, V); dp[0] = b[0]; for (int u = 0; u n; u++) if (dp[u] == V) continue; for (auto [v, w] : adj[u]) if (w mid) continue; if (dp[u] w) continue; dp[v] = std::max(dp[v], dp[u] + b[v]); return dp[n - 1] = 0; ; while (l + 1 r) i64 mid = l + ((r - l) 1); if (ck(ws[mid])) r = mid; else l = mid; io.println(r == ws.size() ? -1 : ws[r]); int main() int T = io.scanint(); while (T--) run(); E. Melody 考虑左侧一列点位为 volume，右侧一列点为 pitch，条件相当于要求任意相邻两项是 v 不同、p 不同这样交错下去。 所以按上面这样建图的话，这个问题就是二分图上找欧拉路径 Code #include algorithm#include headers/io/iov2.hpp#include map#include ranges#include utility#include vector#include rangesusing i64 = long long;using pii = std::pairint, int;IO::IO io;void run() int n = io.scanint(); auto vp = io.scanstd::vectorpii(n); std::vectorint vs, ps; for (auto [v, p] : vp) vs.push_back(v); ps.push_back(p); std::ranges::sort(vs), std::ranges::sort(ps); vs.erase(std::unique(vs.begin(), vs.end()), vs.end()); ps.erase(std::unique(ps.begin(), ps.end()), ps.end()); int numv = vs.size(), nump = ps.size(); int num = numv + nump; std::vectorstd::vectorpii G(num); std::vectorint deg(num, 0); std::mappii, int index; for (int i = 0; i n; i++) auto [v, p] = vp[i]; v = std::lower_bound(vs.begin(), vs.end(), v) - vs.begin(); p = std::lower_bound(ps.begin(), ps.end(), p) - ps.begin() + numv; deg[v]++, deg[p]++; G[v].push_back(p, i); G[p].push_back(v, i); index[v, p] = i; index[p, v] = i; std::vectorint used(n, 0); std::vectorint ans; std::vectorint cur(num, 0); auto dfs = [](auto F, int u) - void while(cur.at(u) G.at(u).size()) auto [v, id] = G[u][cur[u]]; cur[u]++; if (used[id]) continue; used[id] = 1; F(F, v); ans.push_back(u); ; int root = 0, cnt = 0; for (int i = 0; i num; i++) if (deg[i] % 2 == 1) cnt++, root = i; dfs(dfs, root); if (ans.size() != n + 1 || cnt 2) io.println(NO); return; auto pt = ans | std::views::adjacent_transform2([](int a, int b) return index[a, b] + 1; ); io.println(YES); std::ranges::for_each(pt, [](int x) io.print(x), io.print( ); ); io.println();int main() int T = io.scanint(); while (T--) run(); F. Faculty 最没有注意力的一集…… 我们需要观察到以下几点： min⁡(x,y)≤f(x,y)≤max⁡(x,y),x≠y\\min(x,y)\\le f(x,y)\\le \\max(x,y), x e ymin(x,y)≤f(x,y)≤max(x,y),x=y 这个结论的证明比较简单。不妨设 xyx\\lt yxy，则令 y=kx+r,k≥1∧0≤rxy=kx+r,k\\ge 1 \\land 0\\le r\\lt xy=kx+r,k≥1∧0≤rx. 那么 x≤f(x,y)=x+r≤kx+r=y x\\le f(x,y)=x+r\\le kx+r=y x≤f(x,y)=x+r≤kx+r=y故得证 考虑 xyzx\\lt y\\lt zxyz，则 f(x,y)≤f(y,z)f(x,y)\\le f(y,z)f(x,y)≤f(y,z) 由 (1) 可证，f(x,y)≤max⁡(x,y)=y=min⁡(y,z)≤f(y,z)f(x,y)\\le \\max(x,y)=y=\\min(y,z)\\le f(y,z)f(x,y)≤max(x,y)=y=min(y,z)≤f(y,z) 这个结论也告诉我们，对于任意一个数组 a[1…n]a[1\\dots n]a[1…n]，实际上只有 nnn 个值需要检查：即 f(a1,amax),f(a2,amax)…,f(an,amax)f(a_1,a_{max}),f(a_2,a_{max})\\dots, f(a_n,a_{max})f(a1​,amax​),f(a2​,amax​)…,f(an​,amax​) 若 xy2xx\\lt y\\lt 2xxy2x，则 f(x,y)=yf(x,y)=yf(x,y)=y 也可以从 (1) 证明。此时 y=kx+r ⟹ k=1y=kx+r\\implies k=1y=kx+r⟹k=1，故 f(x,y)=x+r=yf(x,y)=x+r=yf(x,y)=x+r=y. 有了这几点后我们就可以利用势能分析法了. 设新加入的数为 aia_iai​，a1:i−1a_{1:i-1}a1:i−1​ 的最大值为 amaxa_{max}amax​ 如果 ai≤amaxa_i\\le a_{max}ai​≤amax​，那么根据 (2)，我们只需要考虑 f(ai,amax)f(a_i,a_{max})f(ai​,amax​) 会不会成为答案即可。 如果 amaxai2amaxa_{max}\\lt a_i\\lt 2a_{max}amax​ai​2amax​，那么根据 (3)，我们只需要考虑 f(ai,amax)=aif(a_i,a_{max})=a_if(ai​,amax​)=ai​ 会不会成为答案即可。 否则若 ai≥2amaxa_i\\ge 2a_{max}ai​≥2amax​，那么我们就遍历数组暴力求解 1/2 两步都是 O(1)O(1)O(1) 的，对于 (3)，由于每一次 amaxa_{max}amax​ 都至少翻倍，因此最多执行 O(log⁡V)O(\\log V)O(logV) 次第三步，因此这样的时间复杂度是 O(nlog⁡V)O(n\\log V)O(nlogV) Code #include headers/io/iov2.hppIO::IO io;void run() int n = io.scanint(); std::vectorint a = io.scanstd::vectorint(n); auto F = [](int x, int y) return x % y + y % x; ; int max = a[0]; int ans = 0; for (int i = 0; i n; i++) ans = std::max(ans, F(max, a[i])); if (max a[i] a[i] 2 * max) max = a[i]; ans = std::max(ans, F(max, a[i])); else if (a[i] = 2 * max) max = a[i]; for (int j = 0; j = i; j++) ans = std::max(ans, F(a[i], a[j])); io.print(ans); io.println();int main() int T = io.scanint(); while (T--) run(); 18comic.vip","tags":[null]},{"title":"Codeforces Round 1040 (Div. 1 + 2)","path":"/notes/acm/competitions/codeforces/cf2129.html","content":"Div2 C/Div1 A - Double Perspective 为了让 f(S)−g(S)f(S)-g(S)f(S)−g(S) 最大化，我们总是贪心地选取线段：如果这条线段没有与之前选的线段构成环，那么就直接加入；否则，若构成了环，说明之前选择的线段可以恰好覆盖这条线段，这条线段的加入也不可能使得 f(S)f(S)f(S) 更大，反而使 g(S)g(S)g(S) 更大了。 判断线段是否构成环可以用并查集维护。时间复杂度 O(n⋅α(n))O(n\\cdot \\alpha(n))O(n⋅α(n)) 其中 α(n)\\alpha(n)α(n) 是并查集的复杂度。 AC Code #include bits/stdc++.husing pii = std::pairint, int;template class Tusing vec = std::vectorT;struct dsu vecint fa, siz; dsu(int n = 0) init(n); void init(int n) fa.resize(n + 1, 0); siz.resize(n + 1, 1); std::iota(fa.begin(), fa.end(), 0); int root(int x) return x == fa[x] ? x : fa[x] = root(fa[x]); bool connected(int u, int v) return root(u) == root(v); bool merge(int u, int v) int fu = root(u), fv = root(v); if (fu == fv) return false; if (siz[fu] siz[fv]) std::swap(fu, fv); fa[fu] = fv; siz[fv] += siz[fu]; return true; ;void run() int n; std::cin n; vecpii a(n); for (auto x : a) std::cin x.first x.second; dsu d(n * 2); vecint ans; for (int i = 0; i n; i++) if (d.connected(a[i].first, a[i].second)) continue; ans.push_back(i + 1); d.merge(a[i].first, a[i].second); std::cout ans.size() ; for (auto i : ans) std::cout i ; std::cout ;int main() #ifdef ONLINE_JUDGE std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0);#endif int t; std::cin t; while (t--) run(); Div2 D/Div1 B - Stay or Mirror 考虑将 aia_iai​ 换成 2n−ai2n-a_i2n−ai​. 对于 jij\\lt iji 的 aja_jaj​ 而言，若初始时 ajaia_j\\lt a_iaj​ai​，那么就算替换了也不会产生新的逆序对；对于 jij\\gt iji 若初始时 ajaia_j\\lt a_iaj​ai​，那么就算替换了，(i,j)(i,j)(i,j) 仍然还是逆序对。 所以，对于 aia_iai​ 只关心其前后 ai\\gt a_iai​ 的元素数量，由于 2n−ai≥n≥ai2n-a_i\\ge n\\ge a_i2n−ai​≥n≥ai​，所以，一旦替换，则必然减少 (x,i)(x,i)(x,i) 的逆序对数量，增加 (i,y)(i,y)(i,y) 的逆序对数量。如果减少的比增加的多，那么就可以替换。 逆序对可以用树状数组统计，时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn). AC Code #include bits/stdc++.hstruct fenwick std::vectorint b; int n; fenwick(int n = 0) init(n); void init(int n) this-n = n; b.assign(n + 1, 0); void add(int p, int v) for (; p = n; p += p -p) b[p] += v; int sum(int p) int s = 0; for (; p 0; p -= p -p) s += b[p]; return s; int sum(int l, int r) return sum(r) - sum(l - 1); ;void run() int n; std::cin n; std::vectorint a(n); for (int i = 0; i n; i++) std::cin a[i]; fenwick F(n * 2), B(n * 2); for (int i = 0; i n; i++) F.add(a[i], 1); for (int i = n - 1; i = 0; i--) F.add(a[i], -1); int f = F.sum(a[i] + 1, n * 2); int b = B.sum(a[i] + 1, n * 2 - a[i]); if (f = b) a[i] = n * 2 - a[i]; B.add(a[i], 1); fenwick Q(n * 2); int ans = 0; for (int i = 0; i n; i++) ans += Q.sum(a[i] + 1, n * 2); Q.add(a[i], 1); std::cout ans ;int main() #ifdef ONLINE_JUDGE std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0);#endif int t; std::cin t; while (t--) run();","tags":[null]},{"title":"Codeforces Round 1048 (Div. 2)","path":"/notes/acm/competitions/codeforces/cf2139.html","content":"A - Maple and Multiplication 签到，如果相等就不用操作；如果 aaa 是 bbb 的倍数，则只需要对 bbb 进行一次操作；否则，需要进行两次操作把 a,ba,ba,b 都变到 LCM。 AC Code #include bits/stdc++.h void Run() int a, b; std::cin a b; if (a b) std::swap(a, b); if (a == b) std::cout 0; else if (b % a == 0) std::cout 1; else std::cout 2; std::cout ; int main() int t; std::cin t; while (t--) Run(); B - Cake Collection 注意到对于烤箱 iii 来说，我们最后从这个烤箱取出的蛋糕总数量只取决于我们最后时刻是什么时候从这个烤箱取的。例如在 t={ 3,7,9 }t=\\set{3,7,9}t={3,7,9} 取三次蛋糕，分别取出 3ai+(7−3)ai+(9−7)ai=9ai3a_i+(7-3)a_i+(9-7)a_i=9a_i3ai​+(7−3)ai​+(9−7)ai​=9ai​ 个蛋糕。 所以排序完之后从大往小从 min⁡(m,n)\\min(m,n)min(m,n) 个烤箱里取即可 AC Code #include bits/stdc++.husing ll = long long;constexpr int N = 2e5 + 10;ll n, m;ll a[N];void Run() std::cin n m; ll sum = 0; for (int i = 1; i = n; i++) std::cin a[i], sum += a[i]; std::sort(a + 1, a + 1 + n); ll ans = 0; ll r = m % n; for (int i = n; i = 1 m 0; i--) ans += a[i] * m; m--; std::cout ans ;int main() int t; std::cin t; while (t--) Run(); C - Cake Assignment 两个操作的本质是（令 T=2k+1T=2^{k+1}T=2k+1，即总数）： C←C2C\\gets \\frac{C}{2}C←2C​ C←C+T2C\\gets \\frac{C+T}{2}C←2C+T​ 所以，假设经过 ppp 次操作，最后 Chocola 手里的蛋糕数量一定可以表示为 x=C+bT2p,b∈N x=\\frac{C+bT}{2^p}, b\\in\\N x=2pC+bT​,b∈N考虑二进制下的表示：xxx 末尾有 aaa 个 000，分子上 CCC 末尾有 kkk 个 000，bTbTbT 末尾至少有 k+1k+1k+1 个 000。于是分子的末尾至少有 kkk 个 000。于是 d⋅2a=e⋅2k2p d\\cdot 2^a=\\frac{e\\cdot 2^k}{2^p} d⋅2a=2pe⋅2k​所以 min⁡p=k−a\\min p=k-aminp=k−a，然后 bbb 也就可以求出来了，于是我们可以从 bbb 的二进制表示推理出操作次序。时间复杂度 O(log⁡n)O(\\log n)O(logn). AC Code #include bits/stdc++.husing ll = long long;using ull = unsigned long long;void Run() ll k, n; std::cin k n; if (n == (1ll k)) return void(std::cout 0 ); int f = __builtin_ctzll((ull)n); int need = k - f; ull od = n f; ull S = (od - 1) 1; std::cout need ; for (int i = 0; i need; i++) std::cout ((S i) 1 ? 2 : 1) ; std::cout ;int main() int t; std::cin t; while (t--) Run(); D - 神秘题，赛时说是题意没讲清楚导致 unrated 首先考虑长度为 333 的时候，发现 [3,2,1][3,2,1][3,2,1] 这种情况就不是 perfect 的了，所以思考这个性质能不能推广。 发现是可以的。考虑例子 5 2 4 1 3，如果我先用 op1 交换 5 2，那么就变成 2 5 4 1 3，我可以对中间的 5 4 1 使用 op2。所以我们可以使用 op2 减少操作次数的方式为：先用 op1 交换出长度为 333 的递减子序列，然后用 op2 进行交换。 所以结论是：区间里不能存在长度为 333 的子序列，否则就不是 perfect 的。 后面想到了但是不知道怎么写（ E1/E2 - 首先是一个贪心的想法：我们让深度相同的点染上相同的颜色，这样所有叶子的公共前缀就肯定能尽可能长了。 令深度为 iii 的点有 cic_ici​ 个，我们考虑第 iii 层能否染成 000。这个就相当于能否选出若干层全部染色成 000，满足染色为 0,10,10,1 的点的数量分别不超过 k,n−kk,n-kk,n−k. E1 我们令 dp[k] 表示可以选出若干层染成 000 且点数为 kkk。解决存在性 dp 的一个方法是 bitset dp，即可行性的转移为 dp |= dp c[i]. 那么我们怎么确定能否满足点的数量分别不超过 k,n−kk,n-kk,n−k 呢？设前 iii 层共有 sss 个点，假设可以找到一种选层的方式选出 fff 个点染成 000，那么也就是说，剩下的 s−fs-fs−f 个点都染成了 111，所以我们可以列出不等式 f≤kf≤ss−f≤n−kf≥0 f\\le k\\\\ f\\le s\\\\ s-f\\le n-k\\\\ f\\ge 0 f≤kf≤ss−f≤n−kf≥0故我们要检查的 dp[f] 的范围是 max⁡(0,s−n+k)≤f≤min⁡(s,k)\\max(0,s-n+k)\\le f\\le \\min(s,k)max(0,s−n+k)≤f≤min(s,k)，所以可以直接从小到大枚举前 iii 层，转移 dp 并暴力检查 dp 取最大的 iii. 时间复杂度的话，瓶颈在 dp 上：最多 O(n)O(n)O(n) 层，每次 dp 转移是 O(nw)O(\\frac{n}{w})O(wn​) 的，检查 dp[f] 也是 O(n)O(n)O(n) 的，所以时间复杂度是 O(n2)O(n^2)O(n2). AC Code for E1 #include bits/stdc++.h// using namespace std;void Run() int n, k; std::cin n k; std::vectorstd::vectorint g(n + 1); for (int i = 2; i = n; ++i) int p; std::cin p; g[p].push_back(i); std::vectorint depth(n + 1, 0); std::queueint q; q.push(1); depth[1] = 1; std::vectorint cnt(n + 2, 0); while (!q.empty()) int u = q.front(); q.pop(); cnt[depth[u]]++; for (int v : g[u]) depth[v] = depth[u] + 1; q.push(v); int Dmin = INT_MAX; for (int u = 1; u = n; ++u) if (g[u].empty()) Dmin = std::min(Dmin, depth[u]); std::vectorint levels; for (int d = 1; d = Dmin; ++d) levels.push_back(cnt[d]); sort(levels.begin(), levels.end()); int ans = 0; std::bitset1005 dp; dp.reset(); dp[0] = 1; int su = 0; for (int L = 1; L = (int)levels.size(); ++L) int x = levels[L - 1]; su += x; dp |= (dp x); int f = n - su; int low = std::max(0, k - f); int high = std::min(k, su); bool ok = false; if (low = high) for (int s = low; s = high; ++s) if (dp[s]) ok = true; break; if (ok) ans = L; std::cout ans ;int main() std::ios::sync_with_stdio(false); std::cin.tie(nullptr); int T; std::cin T; while (T--) Run(); return 0; E2 考虑进一步优化：BFS 以及收集 levels 肯定是无法优化了的，所以得从 dp 下手。 一个发现：levels 里可能有很多重复的 aia_iai​，这样的 aia_iai​ 至多有 O(n)O(\\sqrt n)O(n​) 个，我们考虑能不能把相同的 aia_iai​ 放到一起计算。 设 aia_iai​ 出现 mim_imi​ 次。E1 的做法是做 mim_imi​ 次 dp 转移，我们可以使用倍增，将 mim_imi​ 次拆分为 1,2,4,…1,2,4,\\dots1,2,4,…，这样既可以拼出任意的 x∈[0,mi]x\\in [0,m_i]x∈[0,mi​]，时间复杂度也能降低至 O(log⁡mi)O(\\log m_i)O(logmi​). 此外，我们也可以二分 iii，表示前 iii 层可以选出这样的 fff。单调性是显然的（点数量都不够当然随便选）。 总时间复杂度：O(log⁡n)⋅O(n)⋅O(nw)=O(nnlog⁡nw)O(\\log n)\\cdot O(\\sqrt n)\\cdot O(\\frac{n}{w})=O(\\frac{n\\sqrt n\\log n}{w})O(logn)⋅O(n​)⋅O(wn​)=O(wnn​logn​) AC Code #include bits/stdc++.h#define sz(x) int(x.size())constexpr int N = 2e5 + 10;using ll = long long;using vi = std::vectorint;int n, k, depth[N], cnt[N], m;vi g[N], lv, vals, tot, pf;void Run() std::cin n k; for (int i = 2, fa; i = n; i++) std::cin fa, g[fa].push_back(i); [] depth[1] = 1; std::queueint q; q.push(1); while (!q.empty()) int u = q.front(); q.pop(); cnt[depth[u]]++; for (auto v : g[u]) depth[v] = depth[u] + 1; q.push(v); (); int dmin = INT_MAX; for (int i = 1; i = n; i++) if (g[i].empty()) dmin = std::min(dmin, depth[i]); for (int d = 1; d = dmin; d++) lv.push_back(cnt[d]); std::sort(lv.begin(), lv.end()); for (int i = 0; i sz(lv);) int j = i; while (j sz(lv) lv[i] == lv[j]) j++; vals.push_back(lv[i]); tot.push_back(j - i); i = j; m = sz(vals); pf.push_back(0); for (int i = 0; i m; i++) pf.push_back(pf[i] + tot[i]); auto check = [](int M) if (M == 0) return true; vi used(m, 0); int need = M; ll sum = 0; for (int i = 0; i m; i++) if (need = 0) break; int t = std::min(need, tot[i]); used[i] = t; need -= t; sum += 1ll * vals[i] * t; ll rest = n - sum; int l = std::max(0ll, 1ll * k - rest); int r = std::min(1ll * k, sum); if (l r) return false; std::bitsetN dp; dp.reset(); dp.set(0); for (int i = 0; i m; i++) int x = vals[i], u = used[i], p = 1; while (u 0) int t = std::min(p, u); int w = x * t; dp |= dp w; u -= t; p = 1; for (int i = l; i = r; i++) if (dp.test(i)) return true; return false; ; int l = -1, r = dmin + 1; while (l + 1 r) int mid = l + ((r - l) 1); if (check(mid)) l = mid; else r = mid; std::cout l ;void Clean() for (int i = 1; i = n; i++) cnt[i] = 0; g[i].clear(); depth[i] = 0; lv.clear(), tot.clear(), vals.clear(), pf.clear();int main() std::ios::sync_with_stdio(false); std::cin.tie(nullptr); int T; std::cin T; while (T--) Run(), Clean();","tags":[null]},{"title":"Codeforces Round 1049 (Div. 2)","path":"/notes/acm/competitions/codeforces/cf2140.html","content":"A - Shift Sort 令有 ccc 个 000，则最小操作次数为 s[1,c]s[1, c]s[1,c] 中 111 的数量。 考虑 s[c+1,n]s[c+1, n]s[c+1,n] 中 000 的数量，这个数与 s[1,c]s[1,c]s[1,c] 中 111 的数量相同，设为 MMM. 我们总是可以选取一个 misplaced 111、一个 misplaced 000 和另外一个 correctly placed digit 进行操作。 同时，如果三个选择的数都是 misplaced，这个其实是没用的。 AC Code #include bits/stdc++.husing vi = std::vectorint;char s[105];void Run() int n, c = 0; std::cin n; for (int i = 1; i = n; i++) std::cin s[i], c += s[i] == 0; vi v; for (int i = 1; i = c; i++) if (s[i] == 1) v.push_back(i); std::cout v.size() ;int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0), std::cerr.tie(0); int t = 1; std::cin t; while (t--) Run(); B - Another Divisibility Problem 添加一个 y=2xy=2xy=2x 即可，这样 xyˉ=x×100…002\\bar{xy}=x\\times 100\\dots 002xyˉ​=x×100…002，而 x+y=3xx+y=3xx+y=3x，显然 100…002=0(mod3)100\\dots 002=0\\pmod 3100…002=0(mod3). C - Ultimate Value 考虑 Bob 会怎么操作，由于 alice 必定会让 f(a)f(a)f(a) 变最大，于是 Bob 最优的操作最多就是撤销 alice 的操作（而且还会增加 cost\\tt{cost}cost），故 Bob 的最优解是直接结束。 所以游戏就变成了 alice 能否在一步操作内让 f(a)f(a)f(a) 最大化，或者直接结束。令 S=∑i=1n(−1)i−1aiS=\\sum_{i=1}^n (-1)^{i-1}a_iS=∑i=1n​(−1)i−1ai​。 若 alice 直接结束，则 f(a)=Sf(a)=Sf(a)=S； 若 alice 交换奇偶相同的下标，那么 SSS 不变，cost\\tt{cost}cost 最多增加 n−1n-1n−1（nnn 为奇数）或 n−2n-2n−2（nnn 为偶数） 若 alice 交换奇偶不同的下标，那么假设交换 i,ji,ji,j，且 iii 为奇数、jjj 为偶数，则 f(a)←f(a)+2(aj−ai)+∣i−j∣f(a)\\gets f(a)+2(a_j-a_i)+|i-j|f(a)←f(a)+2(aj​−ai​)+∣i−j∣所以，我们按 iji\\lt jij 和 iji\\gt jij 分成两个部分 若 iji\\lt jij，我们 maximize 的目标为 2aj+j−(2ai+i)2a_j+j-(2a_i+i)2aj​+j−(2ai​+i) 若 iji\\gt jij，我们 maximize 的目标为 2aj−j−(2ai−i)2a_j-j-(2a_i-i)2aj​−j−(2ai​−i) 两次线性枚举分别维护前缀 min 和后缀 min 即可。时间复杂度 O(n)O(n)O(n) AC Code #include bits/stdc++.h#define mid(l, r) (l + ((r - l) 1))#define sz(x) (int(x.size()))#define repeat(i, a, b) for (int i = (a); i = (b); i++)#define until(i, a, b) for (int i = (a); i = (b); i--)#define array_of(T) std::unique_ptrT[]using ll = long long;using vi = std::vectorint;using pii = std::pairint, int;constexpr int N = 2e5 + 10;int n;ll sum;ll a[N];ll s1[N];void Run() std::cin n; sum = 0; repeat(i, 1, n) std::cin a[i]; sum += a[i] * (i % 2 == 1 ? 1 : -1); ll p = n % 2 == 0 ? n - 2 : n - 1; repeat(i, 1, n) s1[i] = a[i] * 2 + i; ll pmin = 1e18; ll m = 0; repeat(i, 1, n) if (i % 2 == 1) pmin = std::min(pmin, s1[i]); else m = std::max(m, s1[i] - pmin); repeat(i, 1, n) s1[i] = a[i] * 2 - i; ll smin = 1e18; until(i, n, 1) if (i % 2 == 1) smin = std::min(smin, s1[i]); else m = std::max(m, s1[i] - smin); std::cout sum + std::max(0ll, m, p) ;int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0), std::cerr.tie(0); int t = 1; std::cin t; while (t--) Run(); D - A Cruel Segment’s Thesis 赛时没想到 max⁡\\maxmax 的转化导致没做出来 首先我们肯定选上所有线段，令 S=∑i=1nri−liS=\\sum_{i=1}^n r_i-l_iS=∑i=1n​ri​−li​。然后，我们想要选出 ⌊n2⌋\\lfloor \\frac{n}{2}\\rfloor⌊2n​⌋ 对线段对 (a,b)(a,b)(a,b) 使得 maximize ∑(a,b)max⁡{ rb−la,ra−lb } \\text{maximize }\\sum_{(a,b)} \\max\\set{r_b-l_a,r_a-l_b} maximize (a,b)∑​max{rb​−la​,ra​−lb​}考虑后面的 max⁡\\maxmax，我们对他应用一个转化 max⁡{ a,b }=a+b+∣a−b∣2\\max\\set{a,b}=\\frac{a+b+|a-b|}{2}max{a,b}=2a+b+∣a−b∣​，所以有 ∑(a,b)rb−la+ra−lb+∣(rb−la)−(ra−lb)∣2 ⟹ ∑(a,b)(rb−lb)+(ra−la)+∣(rb+lb)−(ra+la)∣2 \\sum_{(a,b)} \\frac{r_b-l_a+r_a-l_b+|(r_b-l_a)-(r_a-l_b)|}{2} \\\\ \\implies \\sum_{(a,b)} \\frac{(r_b-l_b)+(r_a-l_a)+|(r_b+l_b)-(r_a+l_a)|}{2} (a,b)∑​2rb​−la​+ra​−lb​+∣(rb​−la​)−(ra​−lb​)∣​⟹(a,b)∑​2(rb​−lb​)+(ra​−la​)+∣(rb​+lb​)−(ra​+la​)∣​当 nnn 为偶数时，所有线段都会用得上，我们把线段按 l+rl+rl+r 排序就可以求出绝对值里的东西之和了。 当 nnn 为奇数的时候稍微麻烦一些，我们需要枚举是哪一条线段失配，并且还要根据这个判断第 (n+1)/2(n+1)/2(n+1)/2 条线段应该被算在较小的 n/2n/2n/2 条线段里，还是较大的 n/2n/2n/2 条线段里。 总时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn). AC Code #include bits/stdc++.h#define mid(l, r) (l + ((r - l) 1))#define sz(x) (int(x.size()))#define repeat(i, a, b) for (int i = (a); i = (b); i++)#define until(i, a, b) for (int i = (a); i = (b); i--)#define array_of(T) std::unique_ptrT[]#define all(x) x.begin(), x.end()#define range(x, a, b) x + a, x + b + 1using ll = long long;using vi = std::vectorint;using pii = std::pairint, int;using pll = std::pairll, ll;constexpr int N = 2e5 + 10;int n;pll a[N];void Run() std::cin n; ll ans = 0; repeat(i, 1, n) std::cin a[i].first a[i].second, ans += a[i].second - a[i].first; std::sort(range(a, 1, n), [](pll x, pll y) return x.first + x.second y.first + y.second; ); if (n % 2 == 0) for (int i = 1, j = n; i j; i++, j--) auto [l1, r1] = a[i]; auto [l2, r2] = a[j]; ans += (r1 - l1 + r2 - l2 + r2 + l2 - r1 - l1) / 2; else ll tp = 0, add = 0, sub = 0; int t = (n + 1) / 2; repeat(i, 1, n) tp += a[i].second - a[i].first; if (i t) sub += a[i].first + a[i].second; else if (i t) add += a[i].first + a[i].second; ll tans = 0; repeat(i, 1, n) tp -= a[i].second - a[i].first; ll v = a[i].first + a[i].second; if (i t) sub += a[t].first + a[t].second - v; else if (i t) add += a[t].first + a[t].second - v; tans = std::max(tans, tp + add - sub); tp += a[i].second - a[i].first; if (i t) sub -= a[t].first + a[t].second - v; else if (i t) add -= a[t].first + a[t].second - v; ans += tans / 2; std::cout ans ;int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0), std::cerr.tie(0); int t = 1; std::cin t; while (t--) Run(); E1 - Prime Gaming (Easy Version) 看到 n≤20n\\le 20n≤20 可以思考会不会是 bitmask 暴力 dp。我们令 bitmask 中 bi=0b_i=0bi​=0 表示第 iii 堆石子数量为 111，bi=1b_i=1bi​=1 表示第 iii 堆石子数量为 222。 然后我们发现一个状态是可以复用的。考虑令 dp[rem][mask][A/B]=1\\texttt{dp[rem][mask][A/B]}=1dp[rem][mask][A/B]=1 表示：剩余 rem\\tt remrem 个石子堆，并且这 rem\\tt remrem 个石子堆的 1,21,21,2 分布由 mask\\tt{mask}mask 表示，当前为 A/B\\tt A/BA/B 行动，dp=1\\texttt {dp}=1dp=1 表示最后 alice 可以取到数量为 222 的石子堆。 这里的“可以复用”是说，比如说某个状态 mask=100111011\\text{mask}=100111011mask=100111011，它可以是从 mask′=100‾0111011\\text{mask}=10\\underbar{0}0111011mask′=100​0111011 移除下划线 000 得到，也可以从 mask′=100111‾1011\\text{mask}=10011\\underbar{1}1011mask′=100111​1011 中移除 111 得到，而后续的变化是相同的，可以复用。 所以，如果当前是 Alice 行动，让 Alice 在 mask\\tt maskmask 石子堆里移除某一位然后交给 Bob 行动。只要存在某一个后继状态其 dp 值为 111，就说明 alice 可以移除某一位到达这个后继状态，从而使得自己可以走到 111。 如果当前是 Bob 行动，让 Bob 在 mask\\tt maskmask 移除某一位。只要存在某一个后继状态的 dp 值为 000，就说明 Bob 可以通过操作阻止 Alice 最后走到 111. 我们最多会走过 O(2n)O(2^n)O(2n) 个 mask\\tt maskmask，每个 mask\\tt maskmask 里，我们需要 O(n)O(n)O(n) 枚举移除的位置。所以我们可以直接写一个记忆化搜索，时间复杂度 O(n⋅2n)O(n\\cdot 2^n)O(n⋅2n) AC Code #include bits/stdc++.h#define mid(l, r) (l + ((r - l) 1))#define sz(x) (int(x.size()))#define repeat(i, a, b) for (int i = (a); i = (b); i++)#define until(i, a, b) for (int i = (a); i = (b); i--)#define array_of(T) std::unique_ptrT[]#define all(x) x.begin(), x.end()#define range(x, a, b) x + a, x + b + 1using ll = long long;using vi = std::vectorint;using pii = std::pairint, int;constexpr int T = 20;constexpr int N = (1 20) + 5;int n, m, k;int g[30];int dp[T + 5][N][2];void Run() std::cin n m k; repeat(i, 1, k) std::cin g[i]; if (m == 1) std::cout 1 ; return; for (int i = 1; i = n; i++) for (int j = 0; j (1 i); j++) dp[i][j][0] = dp[i][j][1] = -1; auto remove = [](int mask, int p) int low = mask ((1 p) - 1); int high = (mask (p + 1)) p; return high | low; ; auto dfs = [](auto F, int remain, int config, int turn) - int auto val = dp[remain][config][turn]; if (val != -1) return val; if (remain == 1) return val = config; if (turn == 0) repeat(i, 1, k) if (g[i] = remain) // remove g[i]-1 th bit int next = remove(config, g[i] - 1); if (F(F, remain - 1, next, 1) == 1) return val = 1; return val = 0; else repeat(i, 1, k) if (g[i] = remain) int next = remove(config, g[i] - 1); if (F(F, remain - 1, next, 0) == 0) return val = 0; return val = 1; ; for (int mask = 0; mask (1 n); mask++) dfs(dfs, n, mask, 0); int ans = 0; for (int mask = 0; mask (1 n); mask++) ans += 1 + dp[n][mask][0]; std::cout ans ;int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0), std::cerr.tie(0); int t = 1; std::cin t; while (t--) Run(); E2 - Prime Gaming (Hard Version) 我们尝试把 0,10,10,1 的意义拓展到 m2m\\gt 2m2 的范围，如果令 1:ai≥k1:a_i\\ge k1:ai​≥k 0:aik0:a_i\\lt k0:ai​k 根据 E1，我们钦定初始 010101 分布的 mask 的时候，可以计算出最后剩下的石子堆是否是 0/10/10/1，记为 f:mask={ 0,1 }n↦{ 0,1 }f:\\text{mask}=\\set{0,1}^n\\mapsto \\set{0,1}f:mask={0,1}n↦{0,1}. 所以，假设最后剩下的石子堆数量为 aaa，那么它会被一些 mask 统计为 111，这样的 mask 满足 f(mask)=1f(\\text{mask})=1f(mask)=1，也就是说，它会被 ≥1,≥2,≥3,…,≥a\\ge 1, \\ge 2, \\ge 3,\\dots,\\ge a≥1,≥2,≥3,…,≥a 统计总共 aaa 次。 但是 mask\\text{mask}mask 只是 { 0,1 }\\set{0,1}{0,1} 分布的，怎么才能计算出 mask\\text{mask}mask 对应的序列数量呢？假设 popcount(mask)=r\\text{popcount}(\\text{mask})=rpopcount(mask)=r，说明 mask 中有 rrr 个 111、n−rn-rn−r 个 000，所以","tags":[null]},{"title":"计算几何：圆","path":"/notes/acm/geometry/circle/circles-n-lines.html","content":"圆的数据结构定义 struct circle point c; // 圆的中心 real r; // 半径; 注意事项 尽量避免直接用 (long) double 进行操作，考虑到浮点误差，更建议使用向量操作 圆与直线 过某点做某圆的切线 过一个点 PPP 做圆 CCC 的切线 若点 PPP 在圆上，则切线垂直于半径 CPCPCP，直接令直线 s=P, d=perp(CP⃗)\\texttt{s=\\(P\\), d=perp(\\(\\vec{CP}\\))}s=P, d=perp(CP). 否则 PPP 在圆外，否则没有切线。此时有两条切线，且应当关于 CPCPCP 对称。考虑用误差更小的向量组合求出 DDD，然后另一边也能求了。 弦切角定理 令 ∣CP∣=d|CP|=d∣CP∣=d，推理 ΔPCD\\Delta PCDΔPCD 的面积，发现 ∣AD∣=rd2−r2d,∣AP∣=d2−r2d|AD| = \\frac{r\\sqrt{d^2-r^2}}{d}, |AP|=\\frac{d^2-r^2}{d}∣AD∣=drd2−r2​​,∣AP∣=dd2−r2​ 所以 D=P+∣AP∣⋅PC⃗±∣AD∣⋅perp(PC⃗)D = P+|AP|\\cdot \\vec{PC} \\plusmn |AD|\\cdot \\texttt{perp}(\\vec{PC})D=P+∣AP∣⋅PC±∣AD∣⋅perp(PC) Reference Code std::vectorline tangent(point x, circle c) real d = distance(c.c, x); std::vectorline res = ; if (d == c.r) res.push_back(linex, perp(c.c - x)); // 点在圆上 else if(d c.r) // 点在圆外 real c1 = (d.sqr() - c.r.sqr()) / d; real c2 = c.r * (sqrt(d.sqr() - c.r.sqr())) / d; res.push_back(linex, (c.c-x).rescale(c1) + (c.c-x).Rrot().rescale(c2)); res.push_back(linex, (c.c-x).rescale(c1) + (c.c-x).Lrot().rescale(c2)); // 否则点在圆内 return res;","tags":[null,null]},{"title":"圆与三角形","path":"/notes/acm/geometry/circle/circles-n-triangles.html","content":"圆与三角形 三角形外接圆、三点定圆 给定不共线的三个点 A,B,CA,B,CA,B,C，求圆 PPP 过三个点。 由于是三点共圆，所以 ∣AP∣=∣BP∣=∣CP∣|AP| = |BP| = |CP|∣AP∣=∣BP∣=∣CP∣，于是，我们可以求出 ABABAB 与 BCBCBC 的中垂线，则这两条中垂线的交点必然就是圆心。于是半径也很容易求了。 三角形内切圆 给定三角形 ΔABC\\Delta ABCΔABC，求出其内切圆。 考虑内切圆的性质，圆心到三条边的距离相等，所以内切圆圆心实际上是三个内角角平分线的交点. 于是我们只需要求出两条角平分线就可以求出圆心了，求出圆心后，半径也很容易算了。 圆与三角形相交的面积","tags":[null,null]},{"title":"线段、射线、直线","path":"/notes/acm/geometry/lines/lines.html","content":"直线数据结构 struct line point s; // 直线上的某点 vec d; // 方向向量; 对于线段而言，我们令其两个端点为 sss, s+ds+ds+d 对于射线而言，令射线的源点为 sss，射线方向为 sss 注意事项 尽量使用向量操作（叉积点积），而非直接操作角度等等 直线与直线 求两直线的交点 我们已经两条直线上的点 p1=A,p2=Cp_1=A,p_2=Cp1​=A,p2​=C 和各自的方向向量 d1,d2d_1,d_2d1​,d2​，求解其交点 BBB. 对于这个问题，我们考虑对 BBB 列出等式： p1+t1d1=p2+t2d2=B p_1+t_1d_1=p_2+t_2d_2=B p1​+t1​d1​=p2​+t2​d2​=B由于是向量，所以可以看成两个方程，一个关于 xxx，另一个关于 yyy {t1⋅xd1+t2⋅(−xd2)=−(xp1−xp2)t1⋅yd1+t2⋅(−yd2)=−(yp1−yp2) \\begin{cases} t_1\\cdot x_{d_1}+t_2\\cdot (-x_{d_2})=-(x_{p_1}-x_{p_2})\\\\ t_1\\cdot y_{d_1}+t_2\\cdot (-y_{d_2})=-(y_{p_1}-y_{p_2}) \\end{cases} {t1​⋅xd1​​+t2​⋅(−xd2​​)=−(xp1​​−xp2​​)t1​⋅yd1​​+t2​⋅(−yd2​​)=−(yp1​​−yp2​​)​解得 t1=(xp1−xp2)yd2−(yp1−yp2)xd2xd2yd1−yd2xd1=(p1−p2)×d2d2×d1 t_1=\\frac{(x_{p_1}-x_{p_2})y_{d_2} - (y_{p_1}-y_{p_2})x_{d_2}}{x_{d_2}y_{d_1}-y_{d_2}x_{d_1}}=\\frac{(p_1-p_2)\\times d_2}{d_2\\times d_1} t1​=xd2​​yd1​​−yd2​​xd1​​(xp1​​−xp2​​)yd2​​−(yp1​​−yp2​​)xd2​​​=d2​×d1​(p1​−p2​)×d2​​这里把点看作向量进行叉积了。 所以，我们就可以愉快地通过 B=p1+t1d1B=p_1+t_1d_1B=p1​+t1​d1​ 计算出交点了。 例题","tags":[null]},{"title":"凸包","path":"/notes/acm/geometry/polygon/convex-hull.html","content":"凸包 动态凸包（上下凸壳法） 我们动态地维护凸包的上凸壳 UUU 和下凸壳 LLL，同时为了避免多余的讨论，我们让凸包最左侧的点 p1p_1p1​ 和最右侧的点 pmp_mpm​ 同时出现在 U,LU,LU,L 里. 我们考虑怎么维护下凸壳（上凸壳同理）。假设点在下凸壳的下方（即凸包外部），如图所示，我们在 A∼HA\\sim HA∼H 下凸壳上加入点 III 我们可以看到，为了维护凸包的性质，需要删除一些点。 以 III 为起点向右侧看去，I,E,F,G,HI,E,F,G,HI,E,F,G,H 这五个点也应该组成凸壳。所以，考察 I,E,FI,E,FI,E,F，则 EEE 不在凸包上，删去；然后考察 I,F,GI,F,GI,F,G 发现他们满足凸包的性质，于是停止，因为 F,G,HF,G,HF,G,H 在旧凸包上，故一定满足凸包性质。 同理，以 III 为起点向左侧看去，I,A,B,C,DI,A,B,C,DI,A,B,C,D 这五个点也应该组成凸壳。所以考察 I,C,DI,C,DI,C,D 后发现 DDD 应当被删去；然后考察 I,C,BI,C,BI,C,B 他们满足凸包的性质，于是停止。 所以，III 的加入需要删除 D,ED,ED,E 两点。总结一下这个流程，为了维护凸包的性质，我们首先找出 D,ED,ED,E 两点使得 D,ED,ED,E 把 III 夹在中间，然后，分别向左、向右不断删除不在新凸包上的点，直到满足凸包的性质。 那我们该怎么判断 III 如果凸包内部呢？实际上，如果 III 在凸包内部，那么从 III 出发向左或者向右都会满足凸包的性质，也就是说，不会删除任何点。我们可以利用这一个结论简化我们的代码。 此外，如果 III 是新点集里最左或者最右的点，那么无论如何都一定会在新凸包上。 动态凸包 例题代码，可以算是模板题：Codeforces 70D 由于这里边界条件判断用的是 != .end(), != .begin()，所以这份代码并不需要预先读入三个点进行凸包的初始化，可以直接插入。 #include bits/stdc++.husing ll = long long;struct point ll x, y; bool operator(point rhs) const return x != rhs.x ? x rhs.x : y rhs.y; ll cross(point rhs) const return x * rhs.y - y * rhs.x; point operator-(point rhs) const return x - rhs.x, y - rhs.y; point operator+(point rhs) const return x + rhs.x, y + rhs.y; bool operator==(point rhs) const return x == rhs.x y == rhs.y; ;class DynConvex std::setpoint lower, upper; // 上下凸壳 void _insert_lower(point p) auto position = lower.lower_bound(p); std::vectorpoint trash; if (position != lower.end()) auto it = position; if (it != lower.end()) auto itt = std::next(it); while (it != lower.end() itt != lower.end() (*it - p).cross(*itt - p) = 0) trash.push_back(*it); it++, itt++; if (position != lower.begin()) auto it = std::prev(position); if (it != lower.begin()) auto itt = std::prev(it); while ((*it - *itt).cross(p - *itt) = 0) trash.push_back(*it); if (itt == lower.begin()) break; it--, itt--; if (position == lower.begin() || position == lower.end()) lower.insert(p); else auto pv = std::prev(position), nx = position; if ((p - *pv).cross(p - *nx) = 0) lower.insert(p); for (auto x : trash) lower.erase(x); void _insert_upper(point p) auto position = upper.lower_bound(p); std::vectorpoint trash; if (position != upper.end()) auto it = position; if (it != upper.end()) auto itt = std::next(it); while (true) if (it == upper.end() || itt == upper.end()) break; if ((*it - *itt).cross(p - *itt) 0) break; trash.push_back(*it); it++, itt++; if (position != upper.begin()) auto it = std::prev(position); if (it != upper.begin()) auto itt = std::prev(it); while ((*it - p).cross(*itt - p) = 0) trash.push_back(*it); if (itt == upper.begin()) break; it--, itt--; if (position == upper.begin() || position == upper.end()) upper.insert(p); else auto pv = std::prev(position), nx = position; if ((p - *pv).cross(p - *nx) = 0) upper.insert(p); for (auto x : trash) upper.erase(x); public: void add(point p) _insert_lower(p); _insert_upper(p); bool consult(point p) auto lp = lower.lower_bound(p); if (*lp == p) return true; else if (lp == lower.end() || lp == lower.begin()) return false; auto up = upper.lower_bound(p); if (*up == p) return true; else if (up == upper.end() || up == upper.begin()) return false; auto plp = std::prev(lp), pup = std::prev(up); if ((p - *plp).cross(p - *lp) = 0 (p - *pup).cross(p - *up) = 0) return true; else return false; hull;int q, op;ll x, y;int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0); std::cin q; while (q--) std::cin op x y; if (op == 1) hull.add(x, y); else std::cout (hull.consult(x, y) ? YES : NO) ; 凸包上的经典算法 点是否在凸包内部 方法一：上下凸壳判定法（适用 Andrew 算法） 如果我们已知上凸壳和下凸壳，那么点在凸包内部的充要条件是点既在下凸壳的上方、又在上凸壳的下方. 这里，我们假定上下凸壳的起点、终点的两个点是相同的。 我们如何判断点 PPP 是否在下凸壳的上方呢？我们按横坐标对下凸壳排序，找出两个点 A,BA,BA,B 使得 AAA 在 PPP 的左侧， BBB 在其右侧。这一步可以通过二分找到。 如果 AAA 或者 BBB 不存在，那么就说明 PPP 一定在凸包的外部。否则，判断叉积 PA⃗×PB⃗\\vec{PA}\\times \\vec{PB}PA×PB，若 ≥0\\ge 0≥0 说明在凸壳上或者在凸壳上方，若 0\\lt 00 则说明在凸壳下方。 上凸壳也是同理，先二分再判断叉积。时间复杂度 O(log⁡n)O(\\log n)O(logn). 上下凸壳判定法 这里，*lp == p 和 *up == p 还额外判定了在凸壳角点的情况。这是必要的，因为当点在凸包外部左侧，或者恰好为凸包上最左侧的点时，lower_bound() 都会找到 lower.begin()\\texttt{lower.begin()}lower.begin()，从而被 else if\\texttt{else if}else if 过滤掉。因此，我们需要手动排除恰好为凸包最左侧点时的情况. bool consult(point p) auto lp = lower.lower_bound(p); if (*lp == p) return true; else if (lp == lower.end() || lp == lower.begin()) return false; auto up = upper.lower_bound(p); if (*up == p) return true; else if (up == upper.end() || up == upper.begin()) return false; auto plp = std::prev(lp), pup = std::prev(up); if ((p - *plp).cross(p - *lp) = 0 (p - *pup).cross(p - *up) = 0) return true; else return false; 方法二：二分三角形法（适用 Graham 算法） 假设我们已知凸包上的所有点（并且按顺时针/逆时针排列好了），我们可以使用二分法判断点是否在凸包的内部。我们固定凸包上的一个点向其他顶点发射线，那么可以切分出 n−2n-2n−2 个三角形。 假设我们现在判断点 PPP 是否在凸包内部，这里我们选择 CCC 为基点。我们先使用二分找出 CP⃗\\vec{CP}CP 被哪两条射线夹住，然后再判断点 PPP 是否在这个三角形内部。 二分的话，可以判断叉积是否 ≥0\\ge 0≥0，判断是否在三角形的内部可以判断 PCi⃗×PCi+1⃗\\vec{PC_i}\\times \\vec{PC_{i+1}}PCi​​×PCi+1​​ 的符号搞定。","tags":[null]},{"title":"区块链在数字资产与去中心化金融上的应用","path":"/notes/fintech/blockchain/applications/digital-assets-and-decentralized-finance.html","content":"Digital Assets 一般来说，我们可以把数字资产分成几类： Security Tokens Real-World Assets Stablecoins Security Tokens 证券型代币 Security Token 实际上就是 traditional security 如 stocks, bonds, funds 在区块链上的 digital representation. 通常，ST 可以代表 ownership rights (对应 stocks 股权), dividends (对应 bonds 债券), 或者其他 financial entitlements. Benefits Quick Settlement Fractional Ownership Cost Reduction 传统的管理费可以免去. Administrative costs of traditional shareholder management Unlock Capital 企业（包括不上市的企业）都可以通过这种方式进行融资 24/7 Captial Market 比较容易理解，因为是在区块链上交易，各种检查也会在区块链上由程序（智能合约）进行，所以无需人力介入，实现全天候的交易。 Unified Platform 这个很容易理解，既然都变成了 token 的形式，那么大家就都可以在同一个区块链上进行交易，避免了不同证券需要在不同平台交易的问题 Real World Asstes (RWAs) Physical assets that are represented as digital tokens on a blockchain. For liquidity and access Stable Coins A stablecoin is a cryptocurrency that is pegged (挂钩) to another asset. Low volatility meet the requirements of sound money (健全的货币), i.e., it must be … Medium of Exchange Store of Value Unit of Account Fungible, easily divisible, and easily moved Crypto-Collaterized 加密货币背书 通常 backed by other Crypto-currencies (如 ETH) 使用 smart contract 用于锁定其他加密货币，从而发行 stable coin. 如果抵押品的价值下跌，系统会自动触发清算 超额抵押 (Over-collateralization): 因为加密资产波动剧烈，系统通常要求抵押价值 稳定币价值（比如抵押价值要至少 150%） 清算 (Liquidation): 如果抵押资产价格下跌，导致抵押率低于安全阈值，智能合约会自动卖出抵押品来回购或销毁稳定币，保证稳定币仍然有足够资产支持。 Algorithmic Stable Coins 无抵押物 Smart contracts adjust supply based on demand to maintain peg Fiat-Collaterized 法定货币背书 Backed by real-world assets Controlled and owned by a central entity To guarantee the issuance and redeemability of the stablecoin into the underlying asset Regulation are required enforcing liquidity, redemption rights, and risk management standards Regulations Stable Coin 现状 目前只有 Fiat-backed stable coins 可以流通，其余两种因为 (1) 缺乏消费者保护 (2) 脱钩风险 (3) 系统风险 等等因素还不被允许。 除了 stable coins 以外，还有一些其他选择：Bank Deposit Tokens, Central Bank Digital Currencies Bank Deposit Tokens Issuer: Licensed depository institutions (banks) Backing: Fully backed by fiat currency deposits at the issuing bank Regulatory oversight: Subject to existing banking regulations Technology: Issued and transacted on blockchain platforms or other distributed ledger technology (DLT) Functionality: Programmable, enabling smart contract integration Central Bank Digital Currencies direct liabilities of the central bank 批发型 CBDC (Wholesale) 主要用户：商业银行、金融机构。 功能：用于银行间清算、证券结算、大额支付。 与 RTGS 的关系： RTGS 是现有的央行大额实时结算系统。 批发型 CBDC 可以作为替代方案（基于分布式账本），也可以和 RTGS 并行互补。 好处：提高效率，尤其是在 跨境支付和清算 场景，能缩短时间、降低成本。 零售型 CBDC (Retail) 主要用户：普通大众、企业。 功能：日常支付、转账、领取政府补贴。 使用方式：一般通过 央行或指定机构发行的数字钱包 (eWallet)。 意义：相当于“数字化现金”，让所有人都能直接使用央行货币。 批发型 → 金融机构用，替代/补充 RTGS，优化 批发市场和跨境支付。 零售型 → 老百姓用，替代部分纸钞和存款，强化 日常支付与普惠金融。 Decentralized Finance (DeFi) Defi Traditional Finance Smart Contracts and Decentralized,Networks Intermediaries: Banks, Brokers,Regulators","tags":[null]},{"title":"Smart Contract 智能合约","path":"/notes/fintech/blockchain/applications/smart-contracts.html","content":"Smart Contract 是如何运作的？","tags":[null]},{"title":"complete-numerical-optimization","path":"/notes/math/optim/nonlinear/complete-numerical-optimization.html","content":"Procedure","tags":[null]},{"title":"多变量有约束非线性优化问题","path":"/notes/math/optim/nonlinear/multi-var-constrained-optimization.html","content":"如何解决 Multi-Variable Constrained Optimization? 一种方法是把一个 Constrained Porblems 转化成 a sequence of Unconstrained Problems →\\to→ Sequential Unconstrained Minimization Techniques (SUMT). 另一种方法是在迭代过程中直接将约束纳入考量，于是迭代结束时约束满足且为最小值点 →\\to→ Primal Methods Converting Methods: Exterior Penalty Function Method 我们在原 objective function fff 上加上 penalty function PPP，构造出新目标函数 ϕ\\phiϕ ϕ(x,rp)=f(x)+rpP(x) \\phi(\\bold{x}, r_p)=f(\\bold{x})+r_p P(\\bold{x}) ϕ(x,rp​)=f(x)+rp​P(x)其中 rpr_prp​ 是 penalty 的系数用以控制 penalty 的强度。我们令 PPP 采取下面这样的形式 P(x)=∑j=1m[max⁡(0,gj(x))]2+∑k=1lhk2(x) P(\\bold{x})=\\sum_{j=1}^m \\Bigg[ \\max\\bigg(0, g_j(\\bold{x})\\bigg) \\Bigg]^2+\\sum_{k=1}^l h_k^2(\\bold{x}) P(x)=j=1∑m​[max(0,gj​(x))]2+k=1∑l​hk2​(x) 为什么 hk2(x)h_k^2(\\bold{x})hk2​(x) 是平方项？ 因为我们想让 hk(x)h_k(\\bold{x})hk​(x) 保持严格为 000，小于 000 和大于 000 在平方之后都会变成 0\\gt 00 的值，这样就可以优化了。 为什么 [max⁡(0,gj(x))]2[\\max(0,g_j(\\bold{x}))]^2[max(0,gj​(x))]2 要取 max⁡\\maxmax？为什么要取平方？ 取 max⁡\\maxmax 是因为 gj(x)≤0g_j(\\bold{x})\\le 0gj​(x)≤0 是不等式约束，只要在 gj(x)0g_j(\\bold{x})\\gt 0gj​(x)0 时才会产生 penalty，所以需要和 000 取 max⁡\\maxmax. 取平方是因为考虑 x=x0\\bold{x}=\\bold{x}_0x=x0​ 使得 gj(x0)=0g_j(\\bold{x}_0)=0gj​(x0​)=0，我们发现在这一个点 max⁡(0,gj(x))\\max(0, g_j(\\bold{x}))max(0,gj​(x)) 这个函数是不可导的，所以加上平方之后在该点处就可导了。 迭代策略 rpr_prp​ 的选取 如果 rpr_prp​ 较小，很有可能 ϕ()\\phi()ϕ() 取到最小值的解不满足约束条件；但是如果 rpr_prp​ 太大，也会造成数值计算困难等问题 这个 Method 先从较小的 rp(0)r_p^{(0)}rp(0)​ 初值开始，优化函数 ϕ(x,rp(0))\\phi(\\bold{x}, r_p^{(0)})ϕ(x,rp(0)​) 在下一步迭代开始时，rpr_prp​ 会乘上一个倍率 rp(t+1)←γrp(t) r_p^{(t+1)}\\gets \\gamma r_p^{(t)} rp(t+1)​←γrp(t)​然后去优化函数 ϕ(x,rp(t+1))\\phi(\\bold{x}, r_p^{(t+1)})ϕ(x,rp(t+1)​). 重复迭代直到相邻两次迭代得到的最小值小于误差，并且每一个约束是否被满足（需要小于给定误差） ∣f(x(t+1))−f(x(t))∣ϵfgj(x)≤ϵg∣hk(x)∣≤ϵh \\Big| f(\\bold{x}^{(t+1)})-f(\\bold{x}^{(t)}) \\Big|\\lt \\epsilon_f \\\\ g_j(\\bold{x}) \\le \\epsilon_g \\\\ |h_k(\\bold{x})| \\le \\epsilon_h ​f(x(t+1))−f(x(t))​ϵf​gj​(x)≤ϵg​∣hk​(x)∣≤ϵh​ Generalized Reduced Gradient (GRG) Method 使用 slack variable 将不等式约束转化为等式约束，于是约束可以表示成如下的形式 gj(x)+sj=0,j∈[1,m]hk(x)=0,k∈[1,l]xil≤xi≤xiu,i∈[1,n]sj≥0,j∈[1,m] \\begin{array}{ll} g_j(\\bold{x})+s_j=0, j\\in [1, m]\\\\ h_k(\\bold{x})=0, k\\in [1,l]\\\\ x_i^l\\le x_i\\le x_i^u, i\\in[1, n]\\\\ s_j\\ge 0, j\\in[1, m] \\end{array} gj​(x)+sj​=0,hk​(x)=0,xil​≤xi​≤xiu​,sj​≥0,​j∈[1,m]k∈[1,l]i∈[1,n]j∈[1,m]​ 和 Lagrange 方法不同，我们添加的不是 sj2s_j^2sj2​ 项而是 sjs_jsj​ 项，而且额外添加 sj≥0s_j\\ge 0sj​≥0 约束 更进一步地，我们把 sjs_jsj​ 也看作是 x\\bold xx 的一部分，sj≥0s_j\\ge 0sj​≥0 视作 0≤sj≤∞0\\le s_j\\le \\infin0≤sj​≤∞，那么 gj(x)+sj=0g_j(\\bold x)+s_j=0gj​(x)+sj​=0 也可以看作是 h(x′)=0h(\\bold{x})=0h(x′)=0，于是上面的约束可以写成 hk(x)=0,k∈[1,m+l]xil≤xi≤xiu,i∈[1,n+m] \\begin{array}{ll} h_k(\\bold {x})=0, k\\in[1, m+l]\\\\ x_i^l\\le x_i\\le x_i^u, i\\in[1, n+m] \\end{array} hk​(x)=0,xil​≤xi​≤xiu​,​k∈[1,m+l]i∈[1,n+m]​m+lm+lm+l 个等式可以消除 m+lm+lm+l 个变量，于是还剩下 (n+m)−(m+l)=n−l(n+m)-(m+l)=n-l(n+m)−(m+l)=n−l 个独立变量 (non-basic variables)，我们令 non-basic 的为 y\\bold yy，basic 的为 z\\bold zz，故 x={yz} \\bold{x}=\\begin{Bmatrix} \\bold y\\\\ \\bold z \\end{Bmatrix} x={yz​}所以这个问题对于 y\\bold yy 来说就变成了 unconstrained problem（只有定义域约束，可以在优化过程中直接检查解的合法性）。我们从 initial point 开始，保证每一步移动都在 feasible region 内即可。 梯度推导 每一步的移动既要满足在定义域内，又要满足保持 hk(x)=0h_k(\\bold{x})=0hk​(x)=0. 而在曲线上移动一小段距离即为 Δhk(x)=∇hk(x)TΔx \\Delta h_k(\\bold{x})= abla h_k(\\bold{x})^T \\Delta \\bold{x} Δhk​(x)=∇hk​(x)TΔx代入 x=[yz]T\\bold{x}=\\begin{bmatrix}\\bold y \\bold z\\end{bmatrix}^Tx=[y​z​]T 并对两部分分别求偏导 ⟹ ∇yhk(x)TΔy+∇zhk(x)TΔz=0 \\implies abla_y h_k(\\bold{x})^T\\Delta \\bold y+ abla_z h_k(\\bold{x})^T\\Delta \\bold z=0 ⟹∇y​hk​(x)TΔy+∇z​hk​(x)TΔz=0所以一共有 m+lm+lm+l 个这样的矩阵等式，可以合并为一个等式 AΔy+BΔz=0 ⟹ Δz=−B−1AΔy \\begin{array}{rrl} \\bold{A}\\Delta\\bold{y} + \\bold{B}\\Delta\\bold{z}=0\\\\ \\implies\\Delta \\bold{z}=-\\bold{B}^{-1}\\bold{A}\\Delta\\bold{y} \\end{array} ⟹​AΔy+BΔzΔz​=0=−B−1AΔy​所以 f(x)f(\\bold{x})f(x) 的变化量为 f(x)=∇f(x)Δx=∇yf(x)TΔy+∇zf(x)TΔz=∇yf(x)TΔy−∇zf(x)TB−1AΔy=(∇yf(x)T−∇zf(x)TB−1A)Δy=(∇yf(x)−(B−1A)T∇zf(x))TΔy=(df(x)dy)TΔy \\begin{aligned} f(\\bold{x})= abla f(\\bold{x}) \\Delta \\bold{x}\\\\ = abla_y f(\\bold{x})^T\\Delta \\bold y + abla_z f(\\bold{x})^T\\Delta \\bold z\\\\ = abla_y f(\\bold{x})^T\\Delta \\bold y - abla_z f(\\bold{x})^T \\bold{B}^{-1}\\bold{A}\\Delta\\bold{y}\\\\ =\\Bigg( abla_y f(\\bold{x})^T - abla_z f(\\bold{x})^T \\bold{B}^{-1}\\bold{A}\\Bigg) \\Delta\\bold{y}\\\\ =\\Bigg( abla_y f(\\bold{x}) - \\bigg(\\bold{B}^{-1}\\bold{A}\\bigg)^T abla_z f(\\bold{x})\\Bigg)^T \\Delta\\bold{y}\\\\ =\\Big( \\frac{d f(\\bold{x})}{d \\bold{y}} \\Big)^T\\Delta\\bold{y} \\end{aligned} f(x)​=∇f(x)Δx=∇y​f(x)TΔy+∇z​f(x)TΔz=∇y​f(x)TΔy−∇z​f(x)TB−1AΔy=(∇y​f(x)T−∇z​f(x)TB−1A)Δy=(∇y​f(x)−(B−1A)T∇z​f(x))TΔy=(dydf(x)​)TΔy​其中 df(x)dy\\frac{d f(\\bold{x})}{d \\bold{y}}dydf(x)​ 是一个记号，称为 Generalized reduced gradient.","tags":[null,null,null]},{"title":"多变量无约束非线性优化问题","path":"/notes/math/optim/nonlinear/multi-var-unconstrained-optimization.html","content":"回顾单变量形式中的问题 Formulation，其中的 search direction s(q)\\bold s^{(q)}s(q) 通常由当前的函数值 f(x)f(\\bold x)f(x) 和当前的 Gradient Vector ∇f(x) abla f(\\bold x)∇f(x) 有的方法使用到了 f(x)f(\\bold x)f(x) 的一阶导数，被称为 First-order methods; 有的只使用函数值，不用导数，称为 Zeroth-order methods. Steepest Descent Method 易于理解，每一次迭代的搜索向量就是当前梯度向量的负数 s(q)=−∇f(x(q)) \\bold s^{(q)}=- abla f\\Big(\\bold x^{(q)}\\Big) s(q)=−∇f(x(q))那么我们什么时候停止呢？我们可以检查连续两次迭代函数值的差值，只要小于一定误差即认为收敛。 ∣f(x(q+1))−f(x(q))∣≤ϵf \\Big| f(\\bold x^{(q+1)}) - f(\\bold x^{(q)}) \\Big| \\le \\epsilon_f ​f(x(q+1))−f(x(q))​≤ϵf​然而通常不能取得很好的效果，一是因为 steepest descent 在 local context 上表现良好，但不一定在 global context 上表现良好；二是因为这个方法有个性质，下一步迭代的方向与上一次正交，这导致搜索路径变成了 zig-zag，而且通常在前几步迭代下降的快，后续迭代中速度变慢，进而引发 (1) 迭代次数增多 (2) 收敛慢。 为什么会正交？从 x(q−1)\\bold x^{(q-1)}x(q−1) 到 x(q)\\bold x^{(q)}x(q) 的过程中，总有变量的梯度在 x(q)\\bold x^{(q)}x(q) 处变成 000，∇f(x(q)) abla f(\\bold x^{(q)})∇f(x(q)) 对这些变量的导数为 000，转而通过其他变量的梯度降低函数值，导致了连续两次迭代的 search vector 正交。 Conjugate Gradient Method 可以看成是 Improved Steepest Descent Method，区别在于用 Exponential Moving Average 的方式改进了下一步迭代的 search vector. s(0)=−∇f(x(0))s(q+1)=−∇f(x(q+1))+β(q+1)⋅s(q) \\begin{array}{rll} \\bold s^{(0)} = - abla f(\\bold x^{(0)})\\\\ \\bold s^{(q+1)} = - abla f(\\bold x^{(q+1)})+\\beta^{(q+1)}\\cdot \\bold s^{(q)}\\\\ \\end{array} s(0)s(q+1)​==​−∇f(x(0))−∇f(x(q+1))+β(q+1)⋅s(q)​其中 β(q+1)\\beta^{(q+1)}β(q+1) 表示前后两次迭代梯度 magnitude 的比值，即 β(q+1)=∣−∇f(x(q+1))∣2∣−∇f(x(q))∣2 \\beta^{(q+1)}=\\frac{\\Big|- abla f(\\bold x^{(q+1)})\\Big|^2}{\\Big|- abla f(\\bold x^{(q)})\\Big|^2} β(q+1)=​−∇f(x(q))​2​−∇f(x(q+1))​2​ Hooke and Jeeves Method Hooke-Jeeves 法不使用导数，只使用函数值进行迭代。在上面两种方法里，每一步迭代的步长由梯度向量决定，而在 Hooke-Jeeves 法里，每一步迭代的步长是 pre-determined on prescribed value 的. 算法流程 我们需要先选择初始点 base point x(0)\\bold x^{(0)}x(0) 和对应的函数值 f(x(0))f(\\bold x^{(0)})f(x(0)) 接着，我们需要对每一个变量定义初始步长 Δi\\Delta_iΔi​ 和失败衰减 did_idi​. 然后交替进行 Exploratory Search 和 Pattern Move. 我们先进行 Exploratory Search. 对于每个 variable xix_ixi​ 都进行两次尝试，我们先检查 xi+Δix_i+\\Delta_ixi​+Δi​（其他变量不变）能否减小 f()f()f()，不能话尝试 xi−Δix_i-\\Delta_ixi​−Δi​（其他变量不变）。 如果能够减小，那么我们固定下 xi±Δix_i\\plusmn \\Delta_ixi​±Δi​ 然后继续检查下一个 xi+1x_{i+1}xi+1​ 如果不能，则先跳过。 如果在所有方向上都不能减小 f()f()f()，则缩小步长，即令 Δi←Δi×di\\Delta_i\\gets \\Delta_i \\times d_iΔi​←Δi​×di​. 如果对于所有变量，缩小前的步长大于给定误差，那么就继续；否则直接结束。 在尝试完所有的 xix_ixi​ 后，我们得到一个新的 base point x(q)\\bold x^{(q)}x(q)，然后尝试 Pattern Move. Pattern Move 可以理解为：尝试复制 x(q−1)→x(q)\\bold x^{(q-1)}\\to \\bold x^{(q)}x(q−1)→x(q) 的走法. xpattern(q)=x(q)+(x(q)−x(q−1)) \\bold x^{(q)}_{\\text{pattern}} = \\bold x^{(q)}+\\Big( \\bold x^{(q)} - \\bold x^{(q-1)} \\Big) xpattern(q)​=x(q)+(x(q)−x(q−1))xpattern(q)\\bold x^{(q)}_{\\text{pattern}}xpattern(q)​ 称为 Pattern Point， 如果这个点更好，那么我们以这个点为新的起点（然而并不算作 base point），在其基础上重新开始 Exploratory Search. 如果这个点并不优，我们直接忽略，在上一个 base point 的基础上进行 Exploratory Search. 终止条件是，某次迭代中，所有变量的步长都小于等于某个给定的误差，即 Δxi≤ϵΔi\\Delta x_i\\le \\epsilon_{\\Delta i}Δxi​≤ϵΔi​ 最终的结果就是最后一个 base point. 我们可以把 Exploratory Search 看作是确定 search vector s(q)\\bold s^{(q)}s(q)，Pattern Move 可以看作是 x(q+1)←x(q)+s(q)\\bold x^{(q+1)}\\gets \\bold x^{(q)} + \\bold s^{(q)}x(q+1)←x(q)+s(q) 的转移。","tags":[null,null,null]},{"title":"单变量无约束非线性优化问题","path":"/notes/math/optim/nonlinear/single-var-unconstrained-optimization.html","content":"Non-Linear Programming: Problem Formulation 和 Gradient Descent 的过程感觉非常相似……令 x(q)\\bold x^{(q)}x(q) 是当前点，s(q)\\bold s^{(q)}s(q) 是搜索方向，qqq 是 Iteration 次数，我们就是希望找一个步长 α\\alphaα x(q+1)=x(q)+Δx(q)=x(q)+αs(q),α≥0 \\begin{aligned} \\bold x^{(q+1)}=\\bold x^{(q)} + \\Delta \\bold x^{(q)}\\\\ =\\bold x^{(q)}+\\alpha \\bold s^{(q)}, \\alpha\\ge 0 \\end{aligned} x(q+1)​=x(q)+Δx(q)=x(q)+αs(q),α≥0​并且我们把 g(α)=f(x+αs)g(\\alpha)=f(\\bold x+\\alpha\\bold s)g(α)=f(x+αs) 看作是关于 α\\alphaα 的函数，我们希望找到的步长 α∗\\alpha^\\astα∗ 可以最小化这个 g(α)g(\\alpha)g(α) 假设 g(α)g(\\alpha)g(α) 的最小值点存在，并且在 α\\alphaα 的定义域内是 唯一 (unimodal) 的 不过，很有可能 g(α)g(\\alpha)g(α) 存在其他极小值点。在数值优化方法里，通常分为两个步骤： 确定最小值点所在的大概范围 [αl,αr][\\alpha_l, \\alpha_r][αl​,αr​] 通过数值迭代对 α∗\\alpha^\\astα∗ 进行近似 Initial Bracketing of Minimum Point 给定单变量函数 f(x)f(x)f(x)，我们可以希望 bracket the interval containing the minimum point，一种方式是通过搜索 Equal Interval Search 我们钦定 δ0\\delta\\gt 0δ0，然后对 x=iδ,i∈Nx=i\\delta, i\\in\\Nx=iδ,i∈N 分别计算 f(x)f(x)f(x). 然后，我们可以通过 f(x)f(x)f(x) 的变化情况判断出 x∗x^\\astx∗ 所在的范围。即找到 s∈Ns\\in Ns∈N 使得 f((s−1)δ)≥f(sδ) and f(sδ)≤f((s+1)δ) f((s-1)\\delta) \\ge f(s\\delta) \\texttt{ and } f(s\\delta) \\le f((s+1)\\delta) f((s−1)δ)≥f(sδ) and f(sδ)≤f((s+1)δ)于是我们可以判断出 x∗x^\\astx∗ 的范围： (s−1)δ≤x∗≤(s+1)δ (s-1)\\delta \\le x^\\ast \\le (s+1)\\delta (s−1)δ≤x∗≤(s+1)δ Variable Interval Search 和 Equal Interval Search 类似，我们固定下 δ\\deltaδ 后，我们让后续的 interval 长度以 aaa 的比例增长，即 x(s+1)=x(s)+asδ,s∈N=δ⋅∑i=0sai \\begin{aligned} x^{(s+1)}=x^{(s)}+a^s\\delta, s\\in\\N\\\\ =\\delta\\cdot \\sum_{i=0}^s a^i \\end{aligned} x(s+1)​=x(s)+asδ,s∈N=δ⋅i=0∑s​ai​最小值所在区间的确定和 Equal Interval Search 也是一样的，即找到 sss 使得 f(x(s−1))≥f(x(s)) and f(x(s))≤f(x(s+1)) ⟹ x(s−1)≤x∗≤x(s+1) f(x^{(s-1)})\\ge f(x^{(s)})\\texttt{ and }f(x^{(s)})\\le f(x^{(s+1)})\\\\ \\implies x^{(s-1)}\\le x^\\ast \\le x^{(s+1)} f(x(s−1))≥f(x(s)) and f(x(s))≤f(x(s+1))⟹x(s−1)≤x∗≤x(s+1)那么这个 aaa 怎么取呢？一种是黄金分割搜索，即 a=1+52a=\\frac{1+\\sqrt{5}}{2}a=21+5​​. Approximation of Minimum Point 这一块的任务是在单谷函数里找最小值（和算法竞赛里的三分很像） Golden Section Method 我们不断在 [xl,xr][x_l, x_r][xl​,xr​] 的区间里选取两个点 x1=φ⋅xl+(1−φ)⋅xrx2=(1−φ)⋅xl+φ⋅xr x_1=\\varphi\\cdot x_l + (1-\\varphi)\\cdot x_r\\\\ x_2=(1-\\varphi)\\cdot x_l + \\varphi\\cdot x_r x1​=φ⋅xl​+(1−φ)⋅xr​x2​=(1−φ)⋅xl​+φ⋅xr​其中 φ=5−12\\varphi=\\frac{\\sqrt{5}-1}{2}φ=25​−1​. 所以我们接着比较 f(xl),f(x1),f(x2),f(xr)f(x_l), f(x_1), f(x_2), f(x_r)f(xl​),f(x1​),f(x2​),f(xr​)，并且不断收紧 [xl,xr][x_l, x_r][xl​,xr​]. 每次收紧，可以舍弃 φ\\varphiφ 的部分，时间复杂度 O(log⁡V)O(\\log V)O(logV). 我们之所以使用 φ\\varphiφ 作为分割比例，是因为下一次的迭代可以复用计算出来的函数值。例如，第一次迭代中 [xl,xr][x_l, x_r][xl​,xr​] 收窄为了 [xl,x2][x_l, x_2][xl​,x2​]，在第二次迭代里，第一次迭代中的 x1x_1x1​ 就是下一次迭代里的 x2′x_2x2′​. 我们考虑三分搜索的终止条件，我们可以设立一个 error tolerance ϵlr\\epsilon_{lr}ϵlr​，只要上下界的绝对误差或者相对误差小于 ϵlr\\epsilon_{lr}ϵlr​ 即可停止 xr−xlxr(0)−xl(0)≤ϵlr or xr−xl≤ϵlr \\frac{x_r-x_l}{x_r^{(0)}-x_l^{(0)}}\\le \\epsilon_{lr}\\texttt{ or } x_r-x_l\\le \\epsilon_{lr} xr(0)​−xl(0)​xr​−xl​​≤ϵlr​ or xr​−xl​≤ϵlr​此时我们令最小值点为其中点 x∗=xl+xr2 x^\\ast=\\frac{x_l+x_r}{2} x∗=2xl​+xr​​ Polynomial Approximation Polynomial Approximation 的核心思想是用一个多项式去拟合原函数 f(x)f(x)f(x)，当拟合程度比较高的时候，我们可以认为我们选择的这个多项式的最小值点比较 approximate f(x)f(x)f(x) 的最小值点了。 Quadric Approximation 我们先来考察多项式为二次的时候。对于二次多项式 g(x)=a0+a1x+a2x2g(x)=a_0+a_1x+a_2x^2g(x)=a0​+a1​x+a2​x2，我们需要三个点来计算其系数，我们取 xl,xmid,xrx_l, x_{\\text{mid}}, x_rxl​,xmid​,xr​. 其中 xmid=xl+xr2x_{\\text{mid}}=\\frac{x_l+x_r}{2}xmid​=2xl​+xr​​ 于是我们可以计算出多项式的最低点 x∗=−a12a2,a20 x^\\ast=-\\frac{a_1}{2a_2}, a_2\\gt 0 x∗=−2a2​a1​​,a2​0所以我们可以计算出四个点的取值 xl,x∗,xmid,xrx_l, x^\\ast, x_{\\text{mid}}, x_rxl​,x∗,xmid​,xr​，然后和 Golden Section Method 类似，我们可以不断收紧区间 [xl,xr][x_l, x_r][xl​,xr​] 当连续两次的 x∗x^\\astx∗ 足够接近的时候，我们就可以停止了，即 ∣x∗(r+1)−x∗(r)∣≤ϵx |x^{\\ast(r+1)}-x^{\\ast(r)}|\\le \\epsilon_x ∣x∗(r+1)−x∗(r)∣≤ϵx​其中 ϵx\\epsilon_xϵx​ 是事先约定好的误差，r∈Nr\\in\\Nr∈N 是迭代次数。","tags":[null,null,null]},{"title":"OCaml Debugging","path":"/notes/cs/coding/ocaml/basics/ocaml-debugging.html","content":"OCaml Debugging","tags":[null]},{"title":"Ocaml 里的数据结构：Lists","path":"/notes/cs/coding/ocaml/data and types/ocaml-lists.html","content":"列表 OCaml 里的列表的底层实现是单向链表，并且是类型相同的不可变元素。","tags":[null]},{"title":"OCaml 格式化输出","path":"/notes/cs/coding/ocaml/basics/ocaml-documentation-printing.html","content":"Printf Module","tags":[null]},{"title":"Dune Build System","path":"/notes/cs/coding/ocaml/basics/ocaml-dune.html","content":"OCaml 使用 Dune 作为自己的构建系统（像 CMake 之于 C++ 一般）","tags":[null]},{"title":"Expression","path":"/notes/cs/coding/ocaml/basics/ocaml-expression.html","content":"类型 和其他语言一样，有 int, float, char, string, bool 等等. Operations of Type ^ 字符串拼接，*. 浮点数相乘，xxxxx.[0] 从字符串中取字符 在 OCaml 里判断相等用 = 与 ，而不用 == 与 !=. 类型转换 target_of_source 类型转换 Assertions let () = assert (f input1 = output1) 流程控制 条件语法 if e1 then e2 else e3 类型检查：若 e1:bool\\texttt{e1}: \\texttt{bool}e1:bool 且 e2,e3:t\\texttt{e2,e3}: \\texttt{t}e2,e3:t，则整个表达式为 t\\tt tt Let 语法 let x = e1 in e2","tags":[null]},{"title":"Functions","path":"/notes/cs/coding/ocaml/basics/ocaml-functions.html","content":"函数 (* 非递归函数定义 *)let func x = ...(* 递归函数定义：多了一个 rec *)let rec func x = ... Mutually Recursive Function 很类似于数学里的螺旋归纳法。用 and 关键词定义 let rec f x1 ... xn = e1 and g y1 ... yn = e2 匿名函数 let inc x = x + 1(* 等价形式 *)let inc = fun x - x + 1 函数：多态 其实更类似于模板自动推导？ let first x y = x 参数的名字 (* 参数名字，[name] 是用于给别人看的，[arg] 是在函数内部使用的 *)let function ~name1:arg1 ~name2:arg2 = ...(* 参数带类型 *)let function ~name1:(default1: int) ~name2:(default2: int) = ...(* 可选参数带默认值 *)let function ?name:(arg1 = default_value_1) = Essence of Function Function 每一个 function 其实都只接受一个参数 let f x1 x2 ... xn = e(* 等价于 *)let f = fun x1 - (fun x2 - (... (fun xn - e)...)) 从 type 上也可看出端倪 t1 - t2 - t3 - t4t1 - (t2 - (t3 - t4)) 所以 function type 是 right-associative 的，但是 function application 是 left associative 的 f x1 x2 ... xn(((f x1) x2 ) ...) xn 尾递归优化 一般函数的递归调用会占用栈空间 O(d)O(d)O(d)，所以递归深度有限制。通过尾递归优化，可以把 O(d)O(d)O(d) 优化到 O(1)O(1)O(1). 尾递归优化的写法：在 function return 的时候，把要 return 的值放在参数里，而不是当成返回值，答案即是递归结束时的参数值。 The “remaining computation” —the addition of 1— now happens before the recursive call not after. let rec count n = if n = 0 then 0 else 1 + count (n-1)(* 尾递归优化 *)let rec count_aux n accumulate = if n = 0 then acc else count_aux (n - 1) (accumulate + 1)let count n = count_aux n 0 (* 包装，将 accumulate 参数隐藏 *)","tags":[null]},{"title":"The 2025 ICPC Asia East Continent Online Contest (I)","path":"/notes/acm/competitions/regionals/asia-ec/2025-icpc-asia-ec-online-i.html","content":"A. Who Can Win 模拟题。把 submission 按时间排序后，对每个队伍保存 solved 数、罚时、reject 数、unknown 数。接着按 (solved,penalty)\\tt{(solved, penalty)}(solved,penalty) 排序，然后检查队伍 iii 是否可以夺冠，就令他们所有的题目都通过，并且所有其他队伍都没通过，然后和目前的冠军比较一下即可。 实现的时候需要注意计入 penalty 的条件. 时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn) AC Code #include bits/stdc++.h#define mid(l, r) (l + ((r - l) 1))#define sz(x) (int(x.size()))#define repeat(i, a, b) for (int i = (a); i = (b); i++)#define until(i, a, b) for (int i = (a); i = (b); i--)#define array_of(T) std::unique_ptrT[]#define all(x) x.begin(), x.end()#define range(x, a, b) x + a, x + b + 1using ll = long long;using vi = std::vectorint;using pii = std::pairint, int;constexpr int P = 30, N = 1e5 + 10;using ll = long long;struct team std::string name; std::arrayint, P ac, un, rej; ll penalty, solved; team() ac.fill(-1), un.fill(-1), rej.fill(0); penalty = 0, solved = 0; ;struct record std::string name, verdict; int time; int problem; r[N];std::mapstd::string, team contest;int n;void Run() std::cin n; contest.clear(); repeat(i, 1, n) char f; std::cin r[i].name f r[i].time r[i].verdict; r[i].problem = f - A; team t = team(); t.name = r[i].name; contest.insert(r[i].name, t); std::sort(range(r, 1, n), [](record a, record b) return a.time b.time; ); repeat(i, 1, n) auto [name, verdict, time, prob] = r[i]; if (verdict == Accepted) if (contest[name].ac[prob] == -1) contest[name].ac[prob] = time; contest[name].penalty += time; contest[name].penalty += 20 * contest[name].rej[prob]; contest[name].solved++; else if (verdict == Rejected) if (contest[name].ac[prob] == -1) contest[name].rej[prob]++; else if (contest[name].ac[prob] == -1 contest[name].un[prob] == -1) contest[name].un[prob] = time; std::vectorteam v; for (auto [x, y] : contest) v.push_back(y); std::sort(all(v), [](team a, team b) return a.solved == b.solved ? a.penalty b.penalty : a.solved b.solved; ); ll championT = v[0].penalty, championP = v[0].solved; std::vectorstd::string ans; for (auto t : v) repeat(i, 0, 25) if (t.un[i] != -1) t.solved++, t.penalty += t.un[i] + 20 * t.rej[i]; if (t.solved championP) ans.push_back(t.name); else if (t.solved == championP t.penalty = championT) ans.push_back(t.name); std::sort(all(ans)); for (auto s : ans) std::cout s ; std::cout ;int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0), std::cerr.tie(0); int t = 1; std::cin t; while (t--) Run(); B. Creating Chaos 结论是只要取 1,2,…,k1,2,\\dots, k1,2,…,k 就一定是最优的。证明待补（ AC Code #include bits/stdc++.h#define mid(l, r) (l + ((r - l) 1))#define sz(x) (int(x.size()))#define repeat(i, a, b) for (int i = (a); i = (b); i++)#define until(i, a, b) for (int i = (a); i = (b); i--)#define array_of(T) std::unique_ptrT[]#define all(x) x.begin(), x.end()#define range(x, a, b) x + a, x + b + 1using ll = long long;using vi = std::vectorint;using pii = std::pairint, int;void Run() int n, k; std::cin n k; repeat(i, n - k + 1, n) std::cout i [i == n];int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0), std::cerr.tie(0); int t = 1; // std::cin t; while (t--) Run(); G. Sorting 考虑 ai=i+1,ai+1=ia_i=i+1,a_{i+1}=iai​=i+1,ai+1​=i 的情况，此时，若 i,i+1i,i+1i,i+1 之间没有连边，那么永远无法通过 k≠i,k≠i+1k e i, k e i+1k=i,k=i+1 将这两个数交换过来。 所以充要条件是对 i∈[1,n−1]i\\in [1,n-1]i∈[1,n−1]，(i,i+1)(i,i+1)(i,i+1) 有连边。时间复杂度 O(n)O(n)O(n) AC Code #include bits/stdc++.h#define mid(l, r) (l + ((r - l) 1))#define sz(x) (int(x.size()))#define repeat(i, a, b) for (int i = (a); i = (b); i++)#define until(i, a, b) for (int i = (a); i = (b); i--)#define array_of(T) std::unique_ptrT[]#define all(x) x.begin(), x.end()#define range(x, a, b) x + a, x + b + 1using ll = long long;using vi = std::vectorint;using pii = std::pairint, int;constexpr int N = 2e5 + 10;int n, m;int a[N], b[N];int mark[N];void Run() std::cin n m; repeat(i, 1, n) mark[i] = 0; repeat(i, 1, m) std::cin a[i] b[i]; if (b[i] == a[i] + 1) mark[a[i]] = 1; int t = 0; repeat(i, 1, n - 1) t += mark[i]; std::cout (t == n - 1 ? Yes : No) ;int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0), std::cerr.tie(0); int t = 1; // std::cin t; while (t--) Run(); I. Knapsack Problem 一个很关键的点：给定序列 { ai }\\set{a_i}{ai​} 用大小为 VVV 的背包顺序或逆序的把所有数放进去，则无论顺序还是逆序，使用的背包数量是相同的. 所以这道题就变成了 Dijkstra 的变体：我们在每个节点上维护 使用的背包数量（越少越好） 当前背包的剩余容量（越多越好） 然后把这个东西当作 dist\\tt{dist}dist 跑 Dijkstra 即可。时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn) AC Code #include bits/stdc++.h#define mid(l, r) (l + ((r - l) 1))#define sz(x) (int(x.size()))#define repeat(i, a, b) for (int i = (a); i = (b); i++)#define until(i, a, b) for (int i = (a); i = (b); i--)#define array_of(T) std::unique_ptrT[]#define all(x) x.begin(), x.end()#define range(x, a, b) x + a, x + b + 1using ll = long long;using vi = std::vectorint;using pii = std::pairint, int;constexpr int N = 1e5 + 10;constexpr int inf = 1e9;struct edge int u, v, w; E[N];int n, m, V, T;vi G[N];pii dist[N];int fa[N], vis[N];int endpoint(edge e, int u) return e.u + e.v - u; void Run() std::cin n m V T; repeat(i, 1, m) std::cin E[i].u E[i].v E[i].w; G[E[i].u].push_back(i); G[E[i].v].push_back(i); repeat(i, 1, n) dist[i] = inf, 0; [] dist[T] = 1, V; using node = std::pairpii, int; auto f = [](node a, node b) - bool return !(a.first.first == b.first.first ? a.first.second b.first.second : a.first.first b.first.first); ; std::priority_queuenode, std::vectornode, decltype(f) pq(f); pq.push(1, V, T); while (!pq.empty()) auto [D, u] = pq.top(); pq.pop(); if (vis[u]) continue; vis[u] = 1; for (auto eid : G[u]) int v = endpoint(E[eid], u), w = E[eid].w; pii nd = dist[u]; if (nd.second w) nd.first++, nd.second = V - w; else nd.second -= w; if (f(nodedist[v], v, nodend, u)) dist[v] = nd; pq.push(nd, v); (); repeat(i, 1, n) std::cout (dist[i].first == inf ? -1 : dist[i].first) [i == n];int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0), std::cerr.tie(0); int t = 1; // std::cin t; while (t--) Run(); M. Teleporter 首先可以看出来是个分层图。但是我们不能直接把全图建出来跑 dijkstra，这样的话一共会有 O(n2+mn)O(n^2+mn)O(n2+mn) 条边，跑 Dijkstra 很容易爆。 所以我们手动在层与层之间更新 dist\\tt{dist}dist： 第 000 层我们随便跑一个 DFS； 从第 kkk 层推到第 k+1k+1k+1 层，我们先处理 teleport. 然后对于那些“使用 teleport 可以减少 dist”的点，我们把他们加入 dijkstra 的堆中 在当前层跑一次 Dijkstra 第 000 层的 DFS 是 O(n)O(n)O(n) 的（虽然我写成了 O(n2)O(n^2)O(n2) 但是无伤大雅）；从上一层推到下一层首先需要 O(m)O(m)O(m) 处理 teleport，然后会在树上跑 Dijkstra，有 O(n)O(n)O(n)条边，所以每一层的转移是 O(m+nlog⁡n)O(m+n\\log n)O(m+nlogn) 的。由于一共要处理 nnn 层，所以总的时间复杂度是 O(nm+n2log⁡n)O(nm+n^2\\log n)O(nm+n2logn) AC Code #include bits/stdc++.h#define mid(l, r) (l + ((r - l) 1))#define sz(x) (int(x.size()))#define repeat(i, a, b) for (int i = (a); i = (b); i++)#define until(i, a, b) for (int i = (a); i = (b); i--)#define array_of(T) std::unique_ptrT[]#define all(x) x.begin(), x.end()#define range(x, a, b) x + a, x + b + 1using ll = long long;using vi = std::vectorint;using pii = std::pairint, int;using pli = std::pairll, int;constexpr int N = 5005;constexpr ll inf = 1e18;int n, m;std::vectorpli G[N], H[N];ll map[N][N], dp[N][N];void Run() std::cin n m; repeat(i, 1, n - 1) int u, v, w; std::cin u v w; G[u].push_back(w, v); G[v].push_back(w, u); repeat(i, 1, m) int u, v; std::cin u v; H[u].push_back(0, v); H[v].push_back(0, u); auto source = [](int s) repeat(i, 1, n) map[s][i] = inf; map[s][s] = 0; auto dfs = [](auto F, int u, int fa) - void for (auto [w, v] : G[u]) if (v == fa) continue; map[s][v] = map[s][u] + w; F(F, v, u); ; dfs(dfs, s, s); ; repeat(i, 1, n) source(i); ll sum = 0; repeat(i, 1, n) dp[0][i] = map[1][i]; sum += dp[0][i]; std::cout sum ; repeat(depth, 1, n) ll sum = 0; repeat(i, 1, n) dp[depth][i] = dp[depth - 1][i]; using node = std::pairll, int; std::priority_queuenode, std::vectornode, std::greaternode q; repeat(u, 1, n) for (auto [w, v] : H[u]) if (dp[depth][v] dp[depth - 1][u]) dp[depth][v] = dp[depth - 1][u]; q.push(dp[depth][v], v); while (!q.empty()) auto [d, u] = q.top(); q.pop(); if (dp[depth][u] d) continue; for (auto [w, v] : G[u]) if (dp[depth][v] dp[depth][u] + w) dp[depth][v] = dp[depth][u] + w; q.push(dp[depth][v], v); repeat(i, 1, n) sum += dp[depth][i]; std::cout sum ; int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0), std::cerr.tie(0); int t = 1; // std::cin t; while (t--) Run();","tags":[null]},{"title":"2025 NTU Training 3","path":"/notes/acm/competitions/regionals/apac/2025-ntu-training3.html","content":"C - Constellations 我们的大方向肯定是用某种数据结构维护 distance 的，所以，我们考虑能否用 std::priority_queue\\tt std::priority\\_queuestd::priority_queue 进行维护。 很快我们发现一个问题是，假设当前 (u,v,du,v)(u,v,d_{u,v})(u,v,du,v​) 是最小的，我把他们两个合并之后，应该如何计算受到影响的 distance？也就是，当我合并了 u,vu,vu,v 之后（假设合并的新星系是 www），我该怎么更新 di,w,iwd_{i,w},i\\lt wdi,w​,iw（这里 iwi\\lt wiw 是因为可能其他星系的合并结果保存在 [n+1,w−1][n+1, w-1][n+1,w−1] 里）？ 所以这时候我们得将式子拆开为两部分，一部分是分母的 size\\tt sizesize，另一部分是分子的 f(A,B)=∑a∈A∑b∈B∥a−b∥2f(A,B)=\\sum_{a\\in A}\\sum_{b\\in B} \\|a-b\\|^2f(A,B)=∑a∈A​∑b∈B​∥a−b∥2. 我们发现分子具有可加性，也就是如果我们想更新 f(i,A+B)f(i,A+B)f(i,A+B)，只需要 f(i,A+B)=f(i,A)+f(i,B)f(i,A+B)=f(i,A)+f(i,B)f(i,A+B)=f(i,A)+f(i,B) 即可，而 size\\tt sizesize 也具有可加性。 所以这就提示我们需要分别维护 f(i,j)f(i,j)f(i,j) 和 size\\tt sizesize. 每次合并完成后（w←u+vw\\gets u+vw←u+v），需要 O(n)O(n)O(n) 遍历 iwi\\lt wiw 以更新 f(i,w)=f(i,u)+f(i,v)f(i,w)=f(i,u)+f(i,v)f(i,w)=f(i,u)+f(i,v)，并将他们全部插入 std::priority_queue\\tt std::priority\\_queuestd::priority_queue. 时间复杂度 O(n2log⁡n)O(n^2\\log n)O(n2logn). 此外，题目中的“按时间合并”的条件可以转化为：将合并后的新星系编号成 [n+1,2n−1][n+1, 2n-1][n+1,2n−1]，这样时间的条件就变成了三元数组的偏序关系 (dis[i,j],i,j)\\tt (dis[i,j], i, j)(dis[i,j],i,j) 其中 iji\\lt jij. AC Code #include bits/stdc++.husing ll = long long;using ld = long double;using pts = std::complexll;constexpr int N = 2005;struct node ll up, div; int u, v;;auto comp = [](node a, node b) ll L = a.up * b.div, R = b.up * a.div; if (L != R) return L R; if (a.u != b.u) return a.u b.u; else return a.v b.v;;int n;ll d[N * 2][N * 2];int mark[N * 2], size[N * 2];pts p[N];std::priority_queuenode, std::vectornode, decltype(comp) q(comp);int main() std::cin n; for (int i = 1, u, v; i = n; i++) std::cin u v; p[i] = u, v; for (int i = 1; i = n; i++) size[i] = 1; for (int j = i + 1; j = n; j++) d[i][j] = d[j][i] = std::norm(p[i] - p[j]); q.push(d[i][j], 1, i, j); for (int w = 1 + n; w = 2 * n - 1; w++) while (!q.empty()) auto [a, b, u, v] = q.top(); q.pop(); if (mark[u] || mark[v]) continue; mark[u] = mark[v] = 1; size[w] = size[u] + size[v]; std::cout size[w] ; for (int i = 1; i w; i++) if (mark[i]) continue; d[i][w] = d[w][i] = d[u][i] + d[v][i]; q.push(d[i][w], size[i] * size[w], i, w); break; D - Deforestation 赛时理解错题意了，以为可以从树上切下相邻连续的树枝……比如说 W=5W=5W=5，一个点延伸出 3,2,4,1,5\\tt 3,2,4,1,53,2,4,1,5 五根树枝，实际上并不能切成 333 段（(3,2),(4,1),(5)(3,2),(4,1),(5)(3,2),(4,1),(5)），只能切成 444 段（(3),(4),(5),(2,1)(3),(4),(5),(2,1)(3),(4),(5),(2,1)） 我们考虑从叶子往根的过程，在叶子处，我们首先把树枝切成 li/wl_i/wli​/w 段，剩下 limod wl_i \\mod wli​modw 的剩余 rir_iri​。然后来到叶子的父亲节点，我们要做的是让一部分叶子的 rir_iri​ 和父亲的树枝 lil_ili​ 放一起被切下来，让令一部分叶子的 rir_iri​ 单独被切下来，并且使单独被切下来的 rir_iri​ 尽可能少。 所以可以得出一个贪心的结论：在节点处，把子节点的 rir_iri​ 从大往小排序，不断放入 rir_iri​，如果超过 www 则扔掉背包里最大的 rir_iri​ 让他被单独切掉。最后还留在背包里的 ∑jrj\\sum_j r_j∑j​rj​ 和父亲节点的 lil_ili​ 放在一起考虑，最后在父亲节点处还会剩下 (li+∑jrj)mod w(l_i+\\sum_j r_j)\\mod w(li​+∑j​rj​)modw. 注意，如果 rrootr_{root}rroot​ 不为 000，这个部分需要单独装一个背包（即答案 +1+1+1）。时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn) AC Code #include bits/stdc++.husing ll = long long;using pil = std::pairint, ll;constexpr int N = 1e5 + 10;int n1;ll w, a[N], rest[N];std::vectorint G[N];void input(int id) int c; std::cin a[id] c; for (int i = 1; i = c; i++) n++; G[id].push_back(n); input(n); int main() std::cin w; input(1); ll ans = 0; auto dfs = [](auto F, int u) - void rest[u] = a[u]; std::vectorll ws; for (auto v : G[u]) F(F, v); ws.push_back(rest[v]); std::sort(ws.begin(), ws.end(), std::greater()); ll sum = 0; int d = 0; for (int i = 0; i ws.size(); i++) sum += ws[i]; if (sum w) sum -= ws[d++], ans++; sum += a[u]; ans += sum / w; rest[u] = sum % w; ; dfs(dfs, 1); std::cout ans + (rest[1] ? 1 : 0) ; E - Denormalization 蚌埠住了，居然真的是暴力 我们直接枚举 min⁡xi\\min x_iminxi​ 对应的是哪个整数 ttt，然后 O(n)O(n)O(n) 计算 std::round\\tt std::roundstd::round 之后的累计浮点误差 s=∑iΔs=\\sum_i\\Deltas=∑i​Δ。那么如果 ttt 是答案对应的整数，那么 sss 应该很小。 所以我们 O(V)O(V)O(V) 枚举之后，O(n)O(n)O(n) 计算累计误差，排序后选择最小的那个即可。时间复杂度 O(nV)O(nV)O(nV) AC Code #include bits/stdc++.husing ld = long double;constexpr int N = 1e4 + 5;constexpr int V = 1e4;constexpr ld eps = 1e-4;struct number ld v; int id; bool operator(number x) const return v x.v; a[N];int n;int ans[N], t[N];ld tmp[N];void construct(int v) t[1] = v, tmp[1] = v; for (int i = 1; i = n; i++) tmp[i] = tmp[1] * a[i].v / a[1].v; t[i] = std::round(tmp[i]); int g = 0; for (int i = 1; i = n; i++) g = std::gcd(g, t[i]); for (int i = 1; i = n; i++) t[i] /= g, ans[a[i].id] = t[i];int main() std::cin n; for (int i = 1; i = n; i++) std::cin a[i].v, a[i].id = i; std::sort(a + 1, a + 1 + n); bool found = false; std::vectorstd::pairld, int c; for (int v = 1; v = V; v++) ld diff = 0; tmp[1] = v, t[1] = v; for (int i = 2; i = n; i++) tmp[i] = tmp[1] * a[i].v / a[1].v; t[i] = std::round(tmp[i]); diff += std::abs(tmp[i] - t[i]); c.push_back(diff, v); std::sort(c.begin(), c.end()); construct(c[0].second); for (int i = 1; i = n; i++) std::cout ans[i] [i == n]; F - Differences 挺好的题，没想到哈希还可以这么用。 考虑每一位 iii 和字符 ccc，令 f(i,c)={ x:xi=c }f(i,c)=\\set{x:x_i=c}f(i,c)={x:xi​=c}，那么我们就想找 x′xx′ 使得 ∀i,∣∑c∈{ A,B,C,D }f(i,c)−f(i,xi′)∣=k \\forall i,|\\sum_{c\\in\\set{\\texttt{A,B,C,D}}}f(i,c)-f(i,x_i)|=k ∀i,∣c∈{A,B,C,D}∑​f(i,c)−f(i,xi′​)∣=k那么我们怎么快速检查是否 =k=k=k 呢？考虑哈希，令 g(i,c)=∑j∈f(i,c)pj(modM) g(i,c)=\\sum_{j\\in f(i,c)} p^j \\pmod M g(i,c)=j∈f(i,c)∑​pj(modM)则 =k=k=k 的条件可以通过对 ∑i\\sum_i∑i​ 求和得到 ∑i∣∑c∈{ A,B,C,D }g(i,c)−g(i,xi′)∣=k⋅∑ipi−k⋅px′(modM) \\sum_i \\Big|\\sum_{c\\in\\set{\\texttt{A,B,C,D}}}g(i,c)-g(i,x_i)\\Big|=k\\cdot\\sum_i p^i - k\\cdot p^{x} \\pmod M i∑​​c∈{A,B,C,D}∑​g(i,c)−g(i,xi′​)​=k⋅i∑​pi−k⋅px′(modM)时间复杂度 O(nm)O(nm)O(nm) AC Code #include bits/stdc++.htemplate int MOD = 998244353class Modulo private: static int Pow(int a, int b) int res = 1; while (b) if (b 1) res = (1ll * res * a) % MOD; b = 1, a = (1ll * a * a) % MOD; return res; static int Inv(int x) return Pow(x, MOD - 2); public: int num; /*====================*/ Modulo(long long temp = 0) num = temp % MOD; Modulo(const Modulo temp) num = temp.num; /*====================*/ constexpr friend Modulo operator+(const Modulo a, const Modulo b) return Modulo((a.num + b.num) = MOD ? a.num + b.num - MOD : a.num + b.num); constexpr friend Modulo operator-(const Modulo a, const Modulo b) return Modulo((a.num - b.num + MOD) = MOD ? a.num - b.num : a.num - b.num + MOD); constexpr friend Modulo operator*(const Modulo a, const Modulo b) return Modulo(1ll * a.num * b.num % MOD); constexpr friend Modulo operator/(const Modulo a, const Modulo b) return Modulo(1ll * a.num * Inv(b.num) % MOD); constexpr friend Modulo operator%(const Modulo a, const Modulo b) return a; /*====================*/ constexpr friend bool operator(const Modulo a, const Modulo b) return a.num b.num; constexpr friend bool operator==(const Modulo a, const Modulo b) return a.num == b.num; constexpr friend bool operator(const Modulo a, const Modulo b) return a.num b.num; constexpr friend bool operator=(const Modulo a, const Modulo b) return a.num = b.num; constexpr friend bool operator!=(const Modulo a, const Modulo b) return a.num != b.num; constexpr friend bool operator=(const Modulo a, const Modulo b) return a.num = b.num; /*====================*/ constexpr void operator+=(const Modulo x) num = num + x.num; if (num = MOD) num -= MOD; constexpr void operator-=(const Modulo x) num = num - x.num + MOD; if (num = MOD) num -= MOD; constexpr void operator*=(const Modulo x) num = 1ll * num * x.num % MOD; constexpr void operator/=(const Modulo x) num = 1ll * num * Inv(x.num) % MOD; constexpr void operator%=(const Modulo x) num = num % x.num; /*====================*/ friend std::ostream operator(std::ostream output, Modulo integer) output integer.num; return output; friend std::istream operator(std::istream input, Modulo integer) int temp; input temp; integer = (temp % MOD + MOD) % MOD; return input; ;using Z = Modulo998244353;using Zn = Moduloint(1e9 + 7);constexpr int P = 34631;constexpr int N = 1e5 + 10;int n, m, k;std::string a[N];Zn p[N];Zn v[N][5], sv[N];// std::bitsetN ok;int main() std::cin n m k; p[0] = 1; for (int i = 1; i N; i++) p[i] = p[i - 1] * P; for (int i = 1; i = n; i++) std::cin a[i]; // ok.set(i); for (int j = 1; j = m; j++) v[j][a[i][j - 1] - A] += p[i]; for (int i = 1; i = m; i++) for (int j = 0; j 4; j++) sv[i] += v[i][j]; Zn tmp = 0; for (int i = 1; i = n; i++) tmp += p[i]; for (int x = 1; x = n; x++) std::vectorZn u(m + 1, 0); for (int j = 1; j = m; j++) u[j] = sv[j] - v[j][a[x][j - 1] - A]; Zn sum = 0; for (int j = 1; j = m; j++) sum += u[j]; if (sum == tmp * k - p[x] * k) std::cout x ; return 0; L - Circuit Board Design 赛时把角度的上下界搞错了，假设以 aaa 为根的子树分配到的角度范围是 [l,r][l,r][l,r]，那么 dfs 的时候，aaa 的儿子应该按子树大小等比例划分 [l,r][l,r][l,r]，赛时把角度范围写成了 [angle−diff,angle+diff]\\tt [angle - diff, angle + diff][angle−diff,angle+diff]，但实际应该是 [l,r]\\tt [l,r][l,r]，不然的话，在角度的分界处可能会存在两个点很靠近。 如上图所示，灰色射线表示分配给 CCC 子树的角度范围，如果 CCC 的子树划分红色射线的角度范围，那么子树节点很可能跨过当初划定的灰色范围，导致错误。所以必须是绿色范围，而令绿色范围尽可能大，那么就是平行于两条灰色射线，即 [l,r]\\tt [l,r][l,r] 假设以 rrr 为根的树分配到的角度范围是 [L,R][L,R][L,R]，那么考虑 rrr 的儿子 sis_isi​，我们令子树大小更大的 sis_isi​ 分配更大的角度范围，然后递归即可。时间复杂度 O(n)O(n)O(n). AC Code 实现使用了 C++ 的 std::complex\\tt std::complexstd::complex 作为点的表示，可以用 std::polar\\tt std::polarstd::polar 更加快捷地实现旋转。 #include bits/stdc++.husing ld = long double;using point = std::complexld;using vi = std::vectorint;constexpr int N = 1005;const ld pi = std::acos((ld)-1.0);int n, width[N];point p[N];vi G[N];int main() std::cin n; for (int i = 1, u, v; i n; i++) std::cin u v; G[u].push_back(v), G[v].push_back(u); auto pre = [](auto F, int u, int f) - void width[u] = 1; for (auto v : G[u]) if (v == f) continue; F(F, v, u); width[u] += width[v]; ; pre(pre, 1, 0); auto dfs = [](auto F, int u, int f, ld l, ld r) - void ld slice = (r - l) / (width[u] - 1), al = l; for (auto v : G[u]) if (v == f) continue; ld t = al + slice * width[v]; p[v] = p[u] + std::polar(1.0L, (t + al) / 2); F(F, v, u, al, t); al = t; ; p[1] = 0, 0; dfs(dfs, 1, 0, 0, pi * 2); for (int i = 1; i = n; i++) std::cout std::setprecision(20) std::fixed p[i].real() p[i].imag() ; M - The Game 没啥好说的，纯模拟。需要注意： 当牌山打空时，还需要不断操作手牌直到无法操作； 每一堆牌山开头是有数字的（1,1,100,1001,1,100,1001,1,100,100）； 每次要连续打两张牌才能摸牌。 AC Code #include bits/stdc++.hstd::dequeint pile;std::vectorint hand;std::vectorint row[4];void draw() if (pile.empty()) return; hand.push_back(pile.front()); pile.pop_front();bool backward(int r, int val) if (r 2) return val == row[r].back() - 10; else return val == row[r].back() + 10;bool operate() bool placed = false; std::vectorstd::pairint, int cand; for (int i = 0; i hand.size(); i++) for (int r = 0; r 4; r++) if (backward(r, hand[i])) cand.push_back(i, r); placed = true; break; if (cand.size() != 0) break; if (cand.size() != 0) break; for (auto [id, r] : cand) // std::cerr hand[id] is backwarded. ; row[r].push_back(hand[id]); hand.erase(hand.begin() + id); if (cand.size() != 0) return true; std::vectorstd::tupleint, int, int f; for (int i = 0; i hand.size(); i++) for (int r = 0; r 4; r++) if (r 2) if (hand[i] row[r].back()) f.push_back(std::abs(hand[i] - row[r].back()), i, r), placed = true; else if (hand[i] row[r].back()) f.push_back(std::abs(hand[i] - row[r].back()), i, r), placed = true; if (f.empty()) return false; std::sort(f.begin(), f.end()); auto [_, i, r] = *f.begin(); row[r].push_back(hand[i]); hand.erase(hand.begin() + i); return true;int main() for (int i = 1, a; i = 98; i++) std::cin a, pile.push_back(a); row[0].push_back(1), row[1].push_back(1); row[2].push_back(100), row[3].push_back(100); while (true) while (hand.size() 8) draw(); if (hand.empty() || pile.empty()) break; bool res = operate(); // if (!res) break; res = operate(); if (!res) break; int p = hand.size(); while (true) if (!operate()) break; // if (hand.size() == p) break; for (int r = 0; r 4; r++) for (auto i : row[r]) std::cout i ; std::cout ; for (auto i : hand) std::cout i ; std::cout ; for (auto i : pile) std::cout i ; std::cout ;","tags":[null]},{"title":"区块链上的数据结构","path":"/notes/fintech/blockchain/principles/bitcoin/blockchain-data-structures.html","content":"Data Structures for Blockchain Hash Pointer Hash Pointer 本身包含一个数据指针和一个哈希指针：数据指针指向存储数据的地址，而哈希指针与其说是指针，其实只保存了指向的数据的哈希值 // 只是随便举的一个 Data Structure 的例子struct HashPointer // 指向数据的指针, ptr = data data_t *ptr; // 数据的哈希值, hash = H(data) hash_t hash;; Node: Hash Structure 一个 Hash Structure 除了包含 Hash Pointer 之外，还可能包含其他数据，例如 Main Data, Meta Data 等等。那么这时候的 Hash Pointer 的指针与哈希指针分别指向什么呢？答案是：指针指向上一个（整个） Hash Structure 的地址，而哈希指针则计算的是整个 Hash Structure 的哈希。 struct HashStructure data_t metadata; data_t maindata; // hs[i].hp.ptr = hs[i-1] // hs[i].hp.hash = H(hs[i-1]), // including hs[i-1].metadata, h[i-1].maindata, h[i-1].hp HashPointer hp;; 所以这些 Hash Structure 可以形成类似链表的数据结构。链表头被称为“创世区块”。 二叉树：Merkle Tree Merkle Tree 可以看作是 Leafy Tree，是二叉树，且只有叶子节点真正保存 Document 的哈希值，非叶子节点的 Hash Pointer 保存左右两个 Hash Pointer 拼接之后计算出来的哈希。 Hash Combiner 既然是二叉树，而且只有叶子节点有值，那么非叶子节点其实就保存了两个东西：左儿子的 Hash Pointer 和右儿子的 Hash Pointer。这两个数据经过 Hash Combiner 拼接之后，计算出哈希，拼成一个新的 Hash Pointer. struct HashCombiner // left.hash = H([Left HashCombiner]), // i.e. H(left.left, left.right) HashPointer left; HashPointer right;; Overview graph TB A[Hash Pointer] -->|Form| B[Merkle Tree] -->|We only need| C[Merkle Tree Root] -->|Encapsulate into| D[Hash Block] -->|Form Linked List| E[Hash Chain] Merkle Tree 提取为 Hash Structure Hash Structure 构建 Blockchain","tags":[null]},{"title":"2025 NTU Training 5","path":"/notes/acm/competitions/regionals/apac/2025-ntu-training5.html","content":"C - Brave Seekers of Unicorns 令 dpℓ,xdp_{\\ell, x}dpℓ,x​ 表示以 xxx 为结尾、长度为 ℓ\\ellℓ 的序列个数，同时，令 fx=∑ℓ=1ndpℓ,xf_x=\\sum_{\\ell=1}^n dp_{\\ell, x}fx​=∑ℓ=1n​dpℓ,x​，即对不同长度的 dpxdp_xdpx​ 进行求和。 对于 dpℓ,xdp_{\\ell,x}dpℓ,x​，我们考虑怎么构造出结尾为 xxx 的序列，那肯定是在长度为 ℓ−1\\ell-1ℓ−1、结尾为 yyy 的序列末尾添加一个元素 xxx. 但是题目的限制又要求序列的倒数第三项不能是 y⊕xy\\oplus xy⊕x. 我们把这个思路写成 dpdpdp 的递推式： dpℓ,x=∑yxdpℓ−1,y−∑z:zz⊕xxdpℓ−2,z dp_{\\ell,x}=\\sum_{y\\lt x} dp_{\\ell-1, y} - \\sum_{z:z\\lt z\\oplus x\\lt x} dp_{\\ell-2, z} dpℓ,x​=yx∑​dpℓ−1,y​−z:zz⊕xx∑​dpℓ−2,z​两边对 ℓ\\ellℓ 求和，并把 fx=∑ℓ=1ndpℓ,xf_x=\\sum_{\\ell=1}^n dp_{\\ell,x}fx​=∑ℓ=1n​dpℓ,x​ 代入上式： fx=∑yxfy−∑z:zz⊕xxfz f_x=\\sum_{y\\lt x} f_y - \\sum_{z:z\\lt z\\oplus x\\lt x} f_z fx​=yx∑​fy​−z:zz⊕xx∑​fz​所以，我们可以从小到大枚举 xxx 同时维护 ∑ifi\\sum_i f_i∑i​fi​ 的前缀和。考虑怎么减去第二部分的和式，由于涉及到异或，我们从二进制拆位的角度考虑。 因为 zxz\\lt xzx，考虑最高位 p0p_0p0​ 使得 xp0=1,zp0=0x_{p_0}=1, z_{p_0}=0xp0​​=1,zp0​​=0，故 zz⊕xz\\lt z\\oplus xzz⊕x 一定是成立的。从高位向低位枚举，假设 xxx 的最高位是 www. 我们接下来只需要判断 z⊕xxz\\oplus x\\lt xz⊕xx 对于 pwp\\gt wpw，如果 xp=0x_p=0xp​=0，那么 zpz_pzp​ 也必须 =0=0=0，不然若 =1=1=1，则会出现 z⊕xxz\\oplus x\\gt xz⊕xx 的情况. 若 p=wp=wp=w，此时 zpz_pzp​ 也必须 =0=0=0. 当 pwp\\lt wpw 时，考虑每一位 xp=1x_p=1xp​=1，因为此时已经有 xw=1,zw=0,(z⊕x)w=1x_w=1, z_w=0, (z\\oplus x)_w=1xw​=1,zw​=0,(z⊕x)w​=1， 假设 zp=1,(z⊕x)p=0z_p=1,(z\\oplus x)_p=0zp​=1,(z⊕x)p​=0，此时无论 zi,ipz_i,i\\lt pzi​,ip 取什么，z⊕xxz\\oplus x\\lt xz⊕xx 这个不等式一定是成立的，所以可以直接减去一段区间的贡献：z∈[2p,2p+1)z\\in[2^p, 2^{p+1})z∈[2p,2p+1) 但若 zp=0,(z⊕x)p=1z_p=0, (z\\oplus x)_p=1zp​=0,(z⊕x)p​=1，此时 ziz_izi​ 并不能任意取 0/10/10/1，不然异或的结果可能会 ≥x\\ge x≥x，需要继续判断后面的 bit. 所以总体时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn). AC Code #include bits/stdc++.hconstexpr int N = 1e6 + 10;constexpr int B = 25; template int MOD = 998244353class Modulo private: static int Pow(int a, int b) int res = 1; while (b) if (b 1) res = (1ll * res * a) % MOD; b = 1, a = (1ll * a * a) % MOD; return res; static int Inv(int x) return Pow(x, MOD - 2); public: int num; /*====================*/ Modulo(long long temp = 0) num = temp % MOD; Modulo(const Modulo temp) num = temp.num; /*====================*/ constexpr friend Modulo operator+(const Modulo a, const Modulo b) return Modulo((a.num + b.num) = MOD ? a.num + b.num - MOD : a.num + b.num); constexpr friend Modulo operator-(const Modulo a, const Modulo b) return Modulo((a.num - b.num + MOD) = MOD ? a.num - b.num : a.num - b.num + MOD); constexpr friend Modulo operator*(const Modulo a, const Modulo b) return Modulo(1ll * a.num * b.num % MOD); constexpr friend Modulo operator/(const Modulo a, const Modulo b) return Modulo(1ll * a.num * Inv(b.num) % MOD); constexpr friend Modulo operator%(const Modulo a, const Modulo b) return a; /*====================*/ constexpr friend bool operator(const Modulo a, const Modulo b) return a.num b.num; constexpr friend bool operator==(const Modulo a, const Modulo b) return a.num == b.num; constexpr friend bool operator(const Modulo a, const Modulo b) return a.num b.num; constexpr friend bool operator=(const Modulo a, const Modulo b) return a.num = b.num; constexpr friend bool operator!=(const Modulo a, const Modulo b) return a.num != b.num; constexpr friend bool operator=(const Modulo a, const Modulo b) return a.num = b.num; /*====================*/ constexpr void operator+=(const Modulo x) num = num + x.num; if (num = MOD) num -= MOD; constexpr void operator-=(const Modulo x) num = num - x.num + MOD; if (num = MOD) num -= MOD; constexpr void operator*=(const Modulo x) num = 1ll * num * x.num % MOD; constexpr void operator/=(const Modulo x) num = 1ll * num * Inv(x.num) % MOD; constexpr void operator%=(const Modulo x) num = num % x.num; /*====================*/ friend std::ostream operator(std::ostream output, Modulo integer) output integer.num; return output; friend std::istream operator(std::istream input, Modulo integer) int temp; input temp; integer = (temp % MOD + MOD) % MOD; return input; ; using Z = Modulo998244353; int n;Z dp[N], prefix[N], ans; Z sum(int l, int r) if (l r) return 0; else return prefix[r] - prefix[l - 1]; int main() std::cin n; dp[0] = prefix[0] = 1; for (int i = 1; i = n; i++) Z tmp = 0; // highest int h = B; int start = 0, end = 0; while (not(i h 1)) h--; h--; int step = 1 h; while (h = 0) while (h = 0 not(i h 1)) h--, step = 1; if (h 0) break; int s = start, e = end; s = 1 h, e = (1 (h + 1)) - 1; tmp += sum(s, std::min(e, i - 1)); h--; dp[i] = prefix[i - 1]; dp[i] -= tmp; prefix[i] = prefix[i - 1] + dp[i]; ans = 0; for (int i = 1; i = n; i++) ans += dp[i]; std::cout ans ; D - Bank Security Unification 令 dpidp_idpi​ 表示序列结尾取 fif_ifi​ 时的最大 security，那么可以得出一个 O(n2)O(n^2)O(n2) 的递推式：枚举上一个序列的结尾 dpi=max⁡ji(dpj+BitAnd(fj,fi)) dp_i=\\max_{j\\lt i} \\Big( dp_j + \\texttt{BitAnd}(f_j, f_i) \\Big) dpi​=jimax​(dpj​+BitAnd(fj​,fi​))看到 BitAnd\\tt BitAndBitAnd 当时就在猜是不是其实只需要维护 O(log⁡V)O(\\log V)O(logV) 个 dpdpdp 值就够了，想了一下不太严谨的证明。考虑最高位 ppp 使得 fi[p]=1,fj[p]=1f_i[p]=1, f_j[p]=1fi​[p]=1,fj​[p]=1，那么我们什么时候不应该直接在 fjf_jfj​ 后面直接添加 fif_ifi​ 呢？答案是，如果 ∃j′,fj′[p]=1\\exist j, f_{j}[p]=1∃j′,fj′​[p]=1，这样的话，最优解应该是 fj,fj′,fif_j, f_{j}, f_ifj​,fj′​,fi​，这样可以多赚 2p2^p2p 的 security. 所以反过来说，我们对每一个 ppp 维护 rightmost index iii 使得 fi[p]=1f_i[p]=1fi​[p]=1，然后 dpdpdp 的转移只需要从这 O(log⁡V)O(\\log V)O(logV) 个 candidate 里面转移即可。 时间复杂度 O(nlog⁡V)O(n\\log V)O(nlogV) AC Code #include bits/stdc++.husing ll = long long;constexpr int N = 1e6 + 10;constexpr int B = 60;int n;ll f[N], dp[N];int R[B + 1];int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0); std::cin n; for (int i = 1; i = n; i++) std::cin f[i]; for (int i = 1; i = n; i++) for (int b = B; b = 0; b--) int id = R[b]; dp[i] = std::max(dp[id] + (f[id] f[i]), dp[i]); for (int b = B; b = 0; b--) if (f[i] b 1) R[b] = i; std::cout dp[n] ; G - Biological Software Utilities 计数，唉 首先当有奇数个点的时候必定为 000. 接着讨论 nnn 为偶数的时候： 我们把过程分解成两部分，一部分将 nnn 个点划分成 n2\\frac{n}{2}2n​ 条边，另一部分是将这 n2\\frac{n}{2}2n​ 条边连接成一棵树。 第一部分：选一个最大数，在剩下 n−1n-1n−1 个数里选一个和它配对；再选剩下里最大的数，在剩下 n−3n-3n−3 个数里选一个和它配对；……这样配对可以不重不漏。个数是 (n−1)!!=(n−1)(n−3)…⋅3⋅1(n-1)!!=(n-1)(n-3)\\ldots \\cdot 3\\cdot 1(n−1)!!=(n−1)(n−3)…⋅3⋅1 第二部分：考虑一条边连接到另一条边上的时候，我们一共有 444 种方法，一共连接 n2−1\\frac{n}{2}-12n​−1 次边，故 4(n2−1)4^{(\\frac{n}{2}-1)}4(2n​−1)；然后有多少种这样的树呢？我们把这 n2\\frac{n}{2}2n​ 条边看作点，因为我们不关心边与边之间的连接关系，只关注连接，所以套用 Prufer 的结论，一共有 (n2)n2−2(\\frac{n}{2})^{\\frac{n}{2}-2}(2n​)2n​−2 种连接方式。 综上，方案数为 (n−1)!!×(n2)n2−2×4(n2−1) (n-1)!!\\times (\\frac{n}{2})^{\\frac{n}{2}-2} \\times 4^{(\\frac{n}{2}-1)} (n−1)!!×(2n​)2n​−2×4(2n​−1) AC Code #include bits/stdc++.husing namespace std;template int MOD = 998244353class Modulo private: static int Pow(int a, int b) int res = 1; while (b) if (b 1) res = (1ll * res * a) % MOD; b = 1, a = (1ll * a * a) % MOD; return res; static int Inv(int x) return Pow(x, MOD - 2); public: int num; /*====================*/ Modulo(long long temp = 0) num = temp % MOD; Modulo(const Modulo temp) num = temp.num; /*====================*/ constexpr friend Modulo operator+(const Modulo a, const Modulo b) return Modulo((a.num + b.num) = MOD ? a.num + b.num - MOD : a.num + b.num); constexpr friend Modulo operator-(const Modulo a, const Modulo b) return Modulo((a.num - b.num + MOD) = MOD ? a.num - b.num : a.num - b.num + MOD); constexpr friend Modulo operator*(const Modulo a, const Modulo b) return Modulo(1ll * a.num * b.num % MOD); constexpr friend Modulo operator/(const Modulo a, const Modulo b) return Modulo(1ll * a.num * Inv(b.num) % MOD); constexpr friend Modulo operator%(const Modulo a, const Modulo b) return a; /*====================*/ constexpr friend bool operator(const Modulo a, const Modulo b) return a.num b.num; constexpr friend bool operator==(const Modulo a, const Modulo b) return a.num == b.num; constexpr friend bool operator(const Modulo a, const Modulo b) return a.num b.num; constexpr friend bool operator=(const Modulo a, const Modulo b) return a.num = b.num; constexpr friend bool operator!=(const Modulo a, const Modulo b) return a.num != b.num; constexpr friend bool operator=(const Modulo a, const Modulo b) return a.num = b.num; /*====================*/ constexpr void operator+=(const Modulo x) num = num + x.num; if (num = MOD) num -= MOD; constexpr void operator-=(const Modulo x) num = num - x.num + MOD; if (num = MOD) num -= MOD; constexpr void operator*=(const Modulo x) num = 1ll * num * x.num % MOD; constexpr void operator/=(const Modulo x) num = 1ll * num * Inv(x.num) % MOD; constexpr void operator%=(const Modulo x) num = num % x.num; /*====================*/ friend std::ostream operator(std::ostream output, Modulo integer) output integer.num; return output; friend std::istream operator(std::istream input, Modulo integer) int temp; input temp; integer = (temp % MOD + MOD) % MOD; return input; ;using Z = Modulo998244353;int n;int main() std::cin n; if (n % 2 == 1) std::cout 0 ; return 0; Z ans = 1, half = n / 2; for (int i = 1; i = n; i += 2) ans *= i; for (int i = 1; i = n / 2 - 2; i++) ans *= half; for (int i = 1; i = n / 2 - 1; i++) ans *= 4; std::cout ans ; I - Binary Supersonic Utahraptors 把题目的式子化简一下发现和两人怎么走的无关，所以直接计算即可。 AC Code #include bits/stdc++.husing namespace std;int main() ios_base::sync_with_stdio(false); cin.tie(0); int n,m,k; cin n m k; int r = 0; for (int i=1; i=n+m; i++) int c; cin c; r += c; cout abs(n - r); J - Burnished Security Updates 读题目可知原图必定是二分图，二分图的 vertex cover 可以直接贪心做。时间复杂度 O(n)O(n)O(n). AC Code #include bits/stdc++.hconstexpr int N = 3e5 + 10;int n, m;int color[N], vis[N], size[N];std::vectorint G[N];int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0); std::cin n m; for (int i = 1, u, v; i = m; i++) std::cin u v; G[u].push_back(v), G[v].push_back(u); auto dfs = [](auto F, int u, int fa, int c) - void vis[u] = true; color[u] = c; for (auto v : G[u]) if (vis[v]) continue; F(F, v, u, 1 - c); ; for (int i = 1; i = n; i++) if (vis[i]) continue; dfs(dfs, i, i, 1); bool ok = true; for (int u = 1; u = n; u++) vis[u] = 0; for (auto v : G[u]) if (color[u] == color[v]) ok = false; if (!ok) std::cout -1 ; return 0; auto count = [](auto F, int u, int c) - int int cnt = color[u]; vis[u] = 1; c++; for (auto v : G[u]) if (vis[v]) continue; cnt += F(F, v, c); return cnt; ; int ans = 0; for (int u = 1; u = n; u++) if (vis[u]) continue; int tot = 0; int cnt = count(count, u, tot); if (cnt * 2 tot) ans += tot - cnt; else ans += cnt; std::cout ans ; M - Brilliant Sequence of Umbrellas 我们考虑先构造 { gi=gcd⁡(ai,ai+1) }\\set{g_i=\\gcd(a_i, a_{i+1})}{gi​=gcd(ai​,ai+1​)}，然后令 ai=gi−1×gia_i=g_{i-1}\\times g_iai​=gi−1​×gi​ 即可. 也就是要求 gcd⁡(gi,gi−2)=1\\gcd(g_i, g_{i-2})=1gcd(gi​,gi−2​)=1，并且 gig_igi​ 越小越好。所以可以 O(n)O(n)O(n) 构造 { gi }\\set{g_i}{gi​} 再 O(n)O(n)O(n) 构造 aia_iai​. AC Code #include bits/stdc++.h#include randomusing ll = long long;constexpr ll maxv = 1e12;constexpr int maxn = 1e6;ll n, cnt;std::vectorll g, ans;void run() std::cin n; g.clear(), ans.clear(); g.push_back(1), g.push_back(2); ll p = 3, f = 0; while (g.size() maxn) while (std::gcd(p, g[f]) != 1) p++; g.push_back(p), f++, p++; ans.push_back(1); for (int i = 1; i g.size(); i++) if (g[i] * g[i - 1] = n) ans.push_back(g[i] * g[i - 1]); else break; std::cout ans.size() ; for (auto i : ans) std::cout i ; std::cout ;int main() int t = 1; while (t--) run(); N - Best Solution Unknown 考虑一个数 aia_iai​ 能不能吃完所有其他数，考察数组里的最大值 am=Ma_m=Mam​=M，不妨设 imi\\lt mim. 要是 aia_iai​ 可以吃掉整个数组，那么如果不吃掉 ama_mam​ 那么就吃不完整个数组，反过来说，只要能吃掉 ama_mam​ 就一定可以吃完整个数组。 也就是说我们只需要判断 aia_iai​ 是否可以吃掉 ama_mam​. 由于 aj,jma_j,j\\lt maj​,jm 都有 ajama_j\\lt a_maj​am​，所以 aia_iai​ 的最优决策一定是把 a1…am−1a_1\\dots a_{m-1}a1​…am−1​ 全部吃完再来尝试吃掉 ama_mam​. 如果能全部吃掉，那么应该有 ai+m−2≥am a_i+m-2\\ge a_m ai​+m−2≥am​现在问题被归约到一个子问题上，在 [1,m−1][1,m-1][1,m−1] 这个区间里，aia_iai​ 可以吃完这个子区间内的所有数吗？同理，我们依然考虑子区间内的最大值 am′=M′a_{m}=Mam′​=M′，不妨设 im′i\\lt mim′ 则 ai+m′−2≥am′ a_i+m-2\\ge a_{m} ai​+m′−2≥am′​于是可以进一步归约……所以我们可以考虑分治做法：每次考虑区间的最大值 am=Ma_m=Mam​=M，在其左右两侧的某个 aia_iai​ 想要吃掉整个数组，就必须先完全吃掉左侧/右侧，再跨过 ama_mam​ 这关。于是这就对左右两侧的 aia_iai​ 的下界有了一定限制。 每次向下递归时，必定抛弃一个元素 ama_mam​，因此至多 O(n)O(n)O(n) 层递归，每层递归都需要 O(log⁡n)O(\\log n)O(logn) 的时间查询最大值所在的下标和值。时间复杂度 O(nlog⁡n)O(n\\log n)O(nlogn). AC Code #include bits/stdc++.hconstexpr int N = 1e6 + 10;int n, a[N];std::arrayint, N * 4 T;std::vectorint ans;void pushup(int p) int ls = p 1, rs = p 1 | 1; T[p] = a[T[ls]] a[T[rs]] ? T[ls] : T[rs];void build(int p = 1, int l = 1, int r = n) if (l == r) T[p] = l; return; int mid = l + ((r - l) 1); build(p 1, l, mid), build(p 1 | 1, mid + 1, r); pushup(p);std::pairint, int query(int ql, int qr, int p = 1, int l = 1, int r = n) if (ql r || qr l) return -1, -1; if (ql = l r = qr) return a[T[p]], T[p]; int mid = l + ((r - l) 1); return std::max(query(ql, qr, p 1, l, mid), query(ql, qr, p 1 | 1, mid + 1, r));int main() std::cin.tie(0)-sync_with_stdio(0); std::cout.tie(0), std::cerr.tie(0); std::cin n; for (int i = 1; i = n; i++) std::cin a[i]; build(); auto dfs = [](auto F, int l, int r, int C) - void if (l r) return; auto [V, index] = query(l, r); if (V = C) ans.push_back(index); F(F, l, index - 1, std::max(C, V - index + l + 1)); F(F, index + 1, r, std::max(C, V + index - r + 1)); ; dfs(dfs, 1, n, 0); std::cout ans.size() ; std::sort(ans.begin(), ans.end()); for (auto x : ans) std::cout x ; std::cout ;","tags":[null]},{"title":"区块链：数字签名","path":"/notes/fintech/blockchain/principles/bitcoin/blockchain-digital-sign.html","content":"Digital Signatures 数字签名通常可以理解为 H(Content+Private Key)H(\\text{Content}+\\text{Private Key})H(Content+Private Key) 数字签名需要满足的性质： Entity Authentication: Receiver should be able to verify Source of message Non-Repudiation: Sender of the message with a signature can’t Deny later Message Integrity: Message should not be Tampered during transmission In Blockchain, we care about digital signatures in terms of Ownership. Sign: Private key Verify: Public Key 这个更加清晰易懂 【场景】sender 希望给 receiver 发送一些数据；receiver 希望自己接受到的数据没有被调包，并且的确是由 sender 发送的。 对于 sender 来说，sender 把要发送的数据打包成 data，对 data 进行哈希，然后使用自己的 private key 对哈希值进行签名，得到 digital signature。 然后，sender 将原始 data、自己的 public key、digital signature 发送给 receiver. 对于 receiver 来说，他希望验证自己收到的数据是正确的，并且是由 sender 发送的。于是，他 对原始 data 进行哈希，得到一个哈希值 用 sender 发送过来的数字签名、step 1 计算的哈希值、sender’s public key 运行签名验证算法。如果通过，则 OK. 数学原理探秘：椭圆曲线 椭圆曲线指的是这样一类曲线，其定义式 (Weierstrass Normal Form) 可以描述为 y2=x3+ax+b,4a3+27b2≠0 y^2=x^3+ax+b, 4a^3+27b^2 e 0 y2=x3+ax+b,4a3+27b2=0 椭圆曲线上两个点的加法运算 假设 P,QP,QP,Q 为曲线上的两个点，过 PQPQPQ 做一条直线（若 P=QP=QP=Q 则取曲线在 PPP 处的切线），交曲线于 RRR. 再过 RRR 作 yyy 轴平行线交曲线与 R′RR′. 定义 R′RR′ 就是 P,QP,QP,Q 相加的结果 R′=P+Q R=P+Q R′=P+Q 密码学与椭圆曲线 在密码学里，我们主要考察 Fp\\mathbb F_pFp​ 其中 ppp 是素数，即主要考察 mod p\\mod pmodp 的域。此时椭圆曲线是离散的，定义为 { (x,y)∈{ Fp }2:y2≡x3+ax+b(modp)⋀4a3+27b2≢0(modp) } \\set{(x,y)\\in \\set{\\mathbb{F}_p}^2: y^2\\equiv x^3+ax+b\\pmod{p} \\bigwedge 4a^3+27b^2 ot\\equiv 0\\pmod p} {(x,y)∈{Fp​}2:y2≡x3+ax+b(modp)⋀4a3+27b2≡0(modp)} 我们以 secp256k1 椭圆曲线为例进行研究，其表达式为 secp256k1:y2=x3+7 \\text{secp256k1}: y^2=x^3+7 secp256k1:y2=x3+7","tags":[null]},{"title":"区块链与哈希","path":"/notes/fintech/blockchain/principles/bitcoin/blockchain-hashing.html","content":"Hashing Function 一个好的哈希函数 H(x)H(x)H(x) 需要满足如下几点： Property Explanation Efficiently Computable mmm-bit input takes O(m)O(m)O(m) to compute Preimage Resistance Given y=H(x)y=H(x)y=H(x), finding xxx is impossible. 2nd2^{\\text{nd}}2nd Preimage Resistance Given xxx, finding y≠xy e xy=x s.t. H(y)=H(x)H(y)=H(x)H(y)=H(x) is impossible. Collision Resistance Finding any pair x≠yx e yx=y s.t. H(x)=H(y)H(x)=H(y)H(x)=H(y) is impossible 同时，也分成 Keyed Hash Function 和 Keyless Hash Function. 在区块链中，我们主要关注 Collision Resistant Hash Function. Hiding Binding 原则 Is it Hiding? If you use Hash function, given commitment, is it possible to find data? Is it Binding? If you use Hash function, is it possible to deny your commitment later? Deep Dive: SHA-256 算法","tags":[null]},{"title":"Bitcoin Transactions: Block and Address","path":"/notes/fintech/blockchain/principles/bitcoin/mechanics-of-blocks-transactions-address.html","content":"Key Ownership 我们把 digital signature 的概念拓展到 identity and authtication: Public Key 是我们向外界展示的 identity 我们如何证明，这个 identity 是我们自己呢？ 我们可以通过 Private Key 对 digital record 进行签名，然后让外界拿着 Public Key 进行 verification 即可 Address 是如何生成的 Address Generation Public Key 哈希成 Address 的过程中加入了 checksum，目的是信息纠错，防止传输过程中数据损坏。 Asset 是如何 transfer 的 Sender 需要在交易之前，证明自己对资产的 ownership Receiving Sending Verification Bitcoin Transactions 区块链是 Transaction-based Ledger Account-based Ledger 就像银行一样，维护的是账户与余额 而 Transaction-based Ledger 维护一堆 transactions. Bitcoin Transaction 是一种数据结构，维护了： Proof of Balance, 指针，指向 sender’s unspent transactions Proof of Ownership, 对于每一个 unspent transaction 维护其 signature Transfer of Ownership, 记录 recipient’s bitcoin address 一个 Transaction 的 Verification 需要包含两个条件： 检查 Past transaction 是 valid 且 unspent 的 通过 signature 检查 sender 和 past transaction 的 recipient 是同一个人 Transaction 的简单实现 struct Transaction // Can be an array of unspent transactions HashPointer *unspent[]; // HashPointer 本质存的是 Transaction // signatures, corresponding to each transaction hash_t *signatures[]; address_t recipient; // the address of the recipient data_t metadata; // metadata for this new transaction.; UTXO 模型: Spending Transactions 真正在实践中，我们不直接把 receiver’s address 写入 Transaction 里，而是 UTXO (Unspent Transaction Output). UTXO 是一种数据结构，简而言之就是把数据 lock 到 receiver 的 Public Key 上. 每一个 UTXO 会维护： Data/Amount: 对于比特币来说，UTXO 会维护转账的金额 Bitcoin Locking Script 一种特殊的数据结构，用 receiver 的 Public Key 加密，只有 receiver 的 Private Key 可以 unlock。 Index: 帮助网络定位这笔 UTXO 那么与之相应的，如何花费一笔 Bitcoin 呢？sender 需要创建一个 Transaction: Hash Pointer 用于定位 Transaction Signature Script 用以证明自己有 Ownership 这个 Script 包含：sender 的 public key 和对 unspent transaction 的签名 这个签名是指：把 unspent transaction 的所有信息打包起来进行哈希，然后再用 sender 的 private key 进行签名。这样 receiver 就可以用 sender 的 public key 解密后，验证 sender 的 ownership 了。 交易完成后，上一个 transaction 的 UTXO 就失效了。 UTXO 是如何失效的？ 区块链上的节点会维护 UTXO set，用于维护 transaction 是否被 spent. 当 transaction 被 spent 后，其 UTXO 会从 UTXO set 中移除。 当其他 transaction 还想把这个 UTXO 当作输入时，会发现其不在节点的 UTXO set 里，从而拒绝该 transaction. 防止 double-spent. 所以，Transaction 和 UTXO 差不多长这样： struct UTXO ;struct Transaction ; Recording Transactions Construction Communication: send to any node Validation Propagation Block 会 record+track 所有 verify 过的 transaction Recording Transaction + HP coinbase TX Mining 记录进 global ledger","tags":[null]},{"title":"挖矿与 Proof-of-Work","path":"/notes/fintech/blockchain/principles/bitcoin/mining-PoW.html","content":"Node Types and Roles Node = 运行区块链网络协议的软件实例（通常是一台计算机 + Bitcoin/Ethereum 客户端） Extended Bitcoin Network P2P Protocols Flood; Random Walk Stratum Protocols Pool Mining Protocol Bitcoin Censensus: Mining Proof-of-Work What is Bitcoin Consensus? TX Validation: Full Node 验证 TX TX Aggregation: Mining Nodes 把 TX 记录进 Block PoW Block Validation: 网络里的每一个节点都验证 block 的合法性 Chain Selection","tags":[null]},{"title":"Consensus Protocols 共识协议","path":"/notes/fintech/blockchain/principles/DistributedSystem/consensus-protocols.html","content":"Consensus Protocols Motivations, Concepts and Principles graph LR A[Fault Tolerance] --> B[Availability] A --> C[Reliability] A --> D[Safety] A --> E[Maintainability] Term Content Availability Operating correctly at any given moment; Available to perform functions. Reliability System can run continuously without interruption or failure for long time. Safety When a system temporarily fails, no catastrophic event happens. Maintainability How easily a failed system can be repaired, automatically, if possible Goal: Robustness Fault 的特点 Transient Intermittent Permanent Type of Failure Server Behaviour Synchrony Assumptions 对于分布式系统而言，为什么我们需要不同的 Communication Models (aka. Synchrony Assumptions)？ 如何描述一种通信模型？ Communication uncertainty is captured by an adversary that controls the message delay in the network under the assumed communication model Keypoint: Communication model defines the limit of the power of such an adversary Synchrony Model Asynchrony Model Partial Synchrony Model Consensus Consensus Problem nnn 个节点，每个节点 iii 都可以从集合 VVV 中选出 viv_ivi​，最后需要通过 protocol 选出 v∗∈Vv^\\ast \\in Vv∗∈V Conditions of Consensus Agreement: No two honest nodes decide on different values at the end. 否则说明 Bad Node 污染了 Honest Nodes，证明 Protocol 不够 Fault-Tolerance 或者设计有缺陷. 没有一致性，系统的正确性就没意义（e.g. 用户 A 看到“转账成功”，用户 B 看到“转账失败”） Validity: If all honest nodes have input vvv, then vvv must be the decision value. 防止 Protocol 总是输出 trivial、无意义的输出 没有有效性，协议可能给出“伪造”的结果，与输入完全无关（即使大家都说 0，它输出 1）。这会破坏系统的公平性和合理性 Termination: Honest nodes must eventually decide on a value in VVV and halt. 没有终止性，系统可能一直等下去不决定（比如所有人卡死在等待别人消息），这就失去了“可用性” Paxos Raft PBFT and Byzantine General Problem Byzantine General Problem nnn 个人做 0/10/10/1 决定，有人选 000，有人选 111，人与人之间只能通过传递信息来决定。 所有人最终必须得出一个决定. 此外存在 xxx 人会 cast selective vote. 问 nnn 至少需要为多少才能保证最终可以得出统一结论？ 答案：n≥3x+1n\\ge 3x+1n≥3x+1 证明 CAP Theorem When designing distributed web services, there are three properties that are commonly desired: consistency, availability, and partition tolerance. It is impossible to achieve all three. Consistency: shared, replicated data appears as a single, up-to-date copy Availability: updates will always be eventually executed on the system Partition tolerance: the system will be tolerant to the partitioning of the process group due to failed network FLP Impossibility Any protocol PPP solving consensus in the asynchronous model, that is, resilient to even just one crash failure, must have an infinite execution time.","tags":[null]},{"title":"Solidity Basics","path":"/notes/fintech/blockchain/applications/solidity/solidity-basics.html","content":"Data Types Arrays uint[5] arr;// similar to std::arrayuint[] vec;// similar to std::vectoruint newLength = vec.push(); // similar to .push_back()vec.length; // similar to .size() Mappings / Dictionary mapping(K = V) map; // similar to std::mapK, Vdelete map[k]; // delete a specific key, similar to .erase()delete map; // .clear()// Contract Contract 的生态位 实际上我认为在设计代码架构的时候，还是应该将 Contract 视为 Class 进行处理. 不仅从逻辑上来说不叫相近，实现上也比较相近。 Important GLOBAL Variables 我们主要认识一下 msg 全局变量，msg 就是当前 contract 收到的消息，包含发送者、发送的金额等信息。 /* msg - Current message received by the contract */msg.sender; // address of the sendermsg.value; // amount of ETH sent (in `wei`). Should be marked as [payable]msg.data; // bytes, complete call datamsg.gas; // remaining gas/* tx - this transaction */tx.origin; // address of the sender of the transaction.tx.gasprice; // Gas price/* block - this block */block.timestamp;block.number; Function Modifiers Grammar to Define a Function // function [NAME]([ARGUMENTS])// [VISIBILITY] [PAYABLE?] returns ([RETURN TYPE] [RETURN NAME]) // function someFunction(uint amount) public payable returns (uint remaining) // do something here...... Function Visibility Explanation public All other functions/contracts (including internal or external) can call this function private Only this contract can call this function external Can only be called from externally. If an internal one want to call, have to use this keyword internal Can only be called from internally. payable: this function can receive send ETH.","tags":[null]},{"title":"分布式系统","path":"/notes/fintech/blockchain/principles/DistributedSystem/distributed-systems.html","content":"A distributed system is a collection of autonomous computing elements that appears to its users as a single coherent system. 分布式系统的特征 graph LR A[Node 1] -->|Communication| B[Node 2] B -->|Communication| A No assumptions on global clock for sync 互相连通（可以通信） 通过 open-group/close-group 策略管理节点 Collection of Autonomous Computing Elements 由各种节点构成 每个节点可以独立运行 act independently 拥有共同的目标 common goals Coherent Collective of Nodes appear as a single coherent system the collection of nodes operates as one entity for end users users may not even notice the existence of multiple nodes 分布式系统的 Design Goals Distribution Transparency Distributed systems aim at making the distribution of processes and resources (objects) transparent (invisible) to the end users and their applications. Transparency of … Meaning Access Hides how an object is accessed Location Hides where an object is located Migration Hides if an object is relocated/moved Replication Hides that an object is replicated Concurrency Hides concurrent access/sharing Failure Hides the failure/recovery of an object Resource Sharing 内涵：to make it easy for the users and their applications to access and share remote resources. Ease of collaboration and information exchange Collaborative work, meetings, contributions, etc. Peer-to-peer file-sharing networks like BitTorrent Backup services and data synchronization tools Be Open An open distributed system offers components that can easily be used by or can easily be integrated into other systems (vice versa in most systems). Be Scalable Scalability of Meaning Size Handling computational and storage issues with more users, nodes or resources Geography Handling communication issues with physically distant nodes and resources Administration Handling issues of managing systems spanning independent organizations System Architecture Placement of components and deciding on their mutual interactions lead to designing the overall software architecture in case of distributed systems. Centralized Simple Client-Server Multi-tiered De-centralized Structured P2P Unstructured P2P Hierarchical P2P P2P Communication 大多数的 P2P 网络都依赖 Multicast Communication","tags":[null]},{"title":"用 Solidity 编写 Blind Auction","path":"/notes/fintech/blockchain/applications/dev/examples/blind-auction-with-solidity.html","content":"什么是 Blind Auction?","tags":[null]}]